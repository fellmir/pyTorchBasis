{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradient methods in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding policy gradient methods](#understanding-policy-gradient-methods)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Defining the environment for training](#defining-the-environment-for-training)\n",
    "4. [Building the policy network](#building-the-policy-network)\n",
    "5. [Implementing the action selection policy](#implementing-the-action-selection-policy)\n",
    "6. [Computing the policy gradient](#computing-the-policy-gradient)\n",
    "7. [Implementing the REINFORCE algorithm](#implementing-the-reinforce-algorithm)\n",
    "8. [Training the policy gradient agent](#training-the-policy-gradient-agent)\n",
    "9. [Evaluating the policy gradient agent](#evaluating-the-policy-gradient-agent)\n",
    "10. [Experimenting with hyperparameters](#experimenting-with-hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding policy gradient methods\n",
    "\n",
    "Policy gradient methods are a family of reinforcement learning algorithms that focus on optimizing the policy directly. Instead of estimating value functions like in Q-learning, policy gradient methods learn a policy that maps states to actions by adjusting the parameters of a function, typically a neural network. These methods are especially effective in environments with continuous action spaces or in tasks where the policy needs to be stochastic, such as robotics and control systems.\n",
    "\n",
    "In reinforcement learning, the agent's goal is to maximize the expected cumulative reward by taking actions in an environment. The policy gradient methods achieve this by parameterizing the policy and adjusting those parameters to improve the expected reward. These methods rely on computing the gradient of the expected reward with respect to the policy's parameters and updating the parameters in the direction that maximizes the expected reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Key concepts in policy gradient methods**\n",
    "\n",
    "There are several key concepts that form the foundation of policy gradient methods:\n",
    "\n",
    "- **Policy**: A policy defines the probability of taking a specific action in a given state. In policy gradient methods, the policy is often stochastic, meaning it outputs a distribution over actions rather than a single deterministic action. This allows the agent to explore different actions in a given state.\n",
    "  \n",
    "- **Trajectory**: A trajectory is a sequence of states, actions, and rewards that the agent experiences over time as it interacts with the environment. It represents a full episode of experience from start to end.\n",
    "\n",
    "- **Return**: The return is the total accumulated reward the agent receives over a trajectory. It represents the agent's performance in that episode.\n",
    "\n",
    "- **Objective**: The objective of policy gradient methods is to maximize the expected return over all trajectories generated by the policy. The higher the return, the better the policy performs in the environment.\n",
    "\n",
    "### **The policy gradient theorem**\n",
    "\n",
    "The policy gradient theorem provides a way to compute how the parameters of the policy should be updated to maximize the expected return. The key insight is that the agent can adjust the probability of taking certain actions in certain states in a way that increases the expected reward. The method uses the gradient of the policy function to update the policy parameters in the direction that increases the likelihood of better outcomes.\n",
    "\n",
    "By doing this, the agent learns to take actions that lead to higher cumulative rewards over time, gradually improving its performance as it interacts with the environment.\n",
    "\n",
    "### **REINFORCE algorithm**\n",
    "\n",
    "One of the simplest policy gradient methods is the REINFORCE algorithm. REINFORCE updates the policy parameters based on the reward obtained from each action taken during an episode. The key idea is to increase the probability of actions that lead to higher rewards and decrease the probability of actions that lead to lower rewards.\n",
    "\n",
    "REINFORCE works by sampling trajectories from the environment, calculating the total return for each trajectory, and using that return to adjust the policy parameters. Actions that lead to higher returns are reinforced, while actions that result in lower returns are penalized. This simple mechanism allows the agent to learn a better policy over time, but it can suffer from high variance in the gradient estimates.\n",
    "\n",
    "### **Challenges with REINFORCE**\n",
    "\n",
    "While REINFORCE is conceptually simple, it has a few challenges:\n",
    "- **High variance**: The gradient estimates in REINFORCE can have high variance, which makes the learning process unstable and slower. The variance arises because the algorithm updates the policy based on entire episodes, meaning feedback is delayed and noisy.\n",
    "- **Delayed reward signal**: REINFORCE updates the policy only after an episode finishes, which can delay learning for earlier actions that influenced the final outcome. This makes it less efficient in complex environments with long trajectories.\n",
    "\n",
    "### **Reducing variance with baseline subtraction**\n",
    "\n",
    "To reduce the variance in policy gradient estimates, it’s common to introduce a **baseline**. The baseline helps stabilize learning by providing a reference point that allows the algorithm to better focus on actions that performed above or below expectations. One commonly used baseline is the value function, which estimates the expected reward for being in a particular state. By subtracting this baseline from the total return, the agent focuses on actions that are relatively better than the expected outcome.\n",
    "\n",
    "### **Actor-Critic methods**\n",
    "\n",
    "Actor-Critic methods combine value-based learning with policy gradient methods to improve learning efficiency. The **actor** is responsible for selecting actions based on the policy, while the **critic** estimates the value function, which helps guide the actor’s updates. The advantage of this approach is that the critic provides immediate feedback to the actor, allowing it to learn from individual time steps rather than waiting for an entire episode to finish.\n",
    "\n",
    "In Actor-Critic methods, the actor updates the policy using the gradient, while the critic evaluates the value of states to provide a more refined signal for the policy update. This helps reduce the variance of the gradient estimates and makes the learning process more stable.\n",
    "\n",
    "### **Advantage Actor-Critic (A2C) and Asynchronous Advantage Actor-Critic (A3C)**\n",
    "\n",
    "A2C is an improved version of the Actor-Critic method that uses the **advantage function**, which measures how much better or worse an action is compared to the average action for a given state. This helps focus the policy updates on actions that are better than average, leading to more efficient learning.\n",
    "\n",
    "A3C extends A2C by running multiple agents in parallel, each interacting with its own copy of the environment. These agents update a shared global policy, which improves the learning speed and helps the algorithm avoid getting stuck in local optima. The parallelization also makes A3C more scalable for large environments.\n",
    "\n",
    "### **Proximal Policy Optimization (PPO)**\n",
    "\n",
    "Proximal Policy Optimization (PPO) is one of the most popular and stable policy gradient methods. It addresses some of the issues with traditional policy gradient methods by preventing large updates to the policy. In PPO, the policy is constrained to change only by a small amount in each update, ensuring that the learning process remains stable. This makes PPO more robust and easier to tune compared to earlier policy gradient methods.\n",
    "\n",
    "PPO uses a clipped objective function that limits how much the policy can change at each step, which helps prevent destructive updates that could degrade the policy’s performance. This balance between exploration and exploitation leads to stable learning and has made PPO widely used in various reinforcement learning tasks.\n",
    "\n",
    "### **Applications of policy gradient methods**\n",
    "\n",
    "Policy gradient methods have broad applications, particularly in tasks that involve continuous actions or require a stochastic policy. Some key areas where policy gradient methods are used include:\n",
    "- **Robotics**: Policy gradient methods are commonly applied in robotic control tasks, where the agent needs to perform continuous actions like walking, grasping, or manipulation.\n",
    "- **Autonomous systems**: These methods are used in self-driving cars, drones, and other systems that require dynamic decision-making in complex environments.\n",
    "- **Game playing**: Policy gradient algorithms are used in games where the action space is large or continuous, such as Go, chess, and complex video games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Maths**\n",
    "\n",
    "#### **Objective in reinforcement learning**\n",
    "\n",
    "In policy gradient methods, the goal is to learn a policy $ \\pi_\\theta(a | s) $ that maximizes the expected cumulative reward over all trajectories an agent experiences while interacting with the environment. The objective function $ J(\\theta) $ is defined as the expected return across all possible trajectories generated by the policy $ \\pi_\\theta $:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} r_t \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\theta $ represents the parameters of the policy,\n",
    "- $ \\tau $ is a trajectory, which is a sequence of states, actions, and rewards,\n",
    "- $ r_t $ is the reward at time step $ t $,\n",
    "- $ T $ is the length of the trajectory.\n",
    "\n",
    "The agent's goal is to optimize the policy parameters $ \\theta $ to maximize the expected cumulative reward $ J(\\theta) $.\n",
    "\n",
    "#### **Policy gradient theorem**\n",
    "\n",
    "The policy gradient theorem provides a formula for computing the gradient of the objective function $ J(\\theta) $ with respect to the policy parameters $ \\theta $. This gradient is used to update the policy in the direction that increases the expected return.\n",
    "\n",
    "The gradient of the expected return can be expressed as:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) G_t \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\nabla_\\theta J(\\theta) $ is the gradient of the objective function with respect to the policy parameters $ \\theta $,\n",
    "- $ \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) $ is the gradient of the log-probability of the action $ a_t $ given state $ s_t $,\n",
    "- $ G_t $ is the return from time step $ t $, representing the total cumulative reward from that point onward.\n",
    "\n",
    "This equation shows that the policy gradient is proportional to the return $ G_t $ and the gradient of the log-likelihood of the action taken. This approach is often referred to as the **log-likelihood trick**, which allows us to compute gradients for stochastic policies.\n",
    "\n",
    "#### **Return and discounting**\n",
    "\n",
    "In many reinforcement learning tasks, future rewards are discounted by a factor $ \\gamma $ to prioritize immediate rewards over distant ones. The **discounted return** $ G_t $ at time step $ t $ is defined as:\n",
    "\n",
    "$$\n",
    "G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}\n",
    "$$\n",
    "\n",
    "Where $ \\gamma \\in [0, 1] $ is the discount factor that determines how much weight is placed on future rewards. A discount factor closer to 0 emphasizes short-term rewards, while a value closer to 1 places more importance on long-term rewards.\n",
    "\n",
    "The goal of the policy gradient method is to adjust the policy parameters such that the expected discounted return $ \\mathbb{E}[G_t] $ is maximized.\n",
    "\n",
    "#### **REINFORCE algorithm**\n",
    "\n",
    "The **REINFORCE algorithm** is a basic policy gradient method that uses the return $ G_t $ from each trajectory to update the policy parameters. The update rule for the parameters $ \\theta $ is:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) G_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\alpha $ is the learning rate,\n",
    "- $ \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) $ is the gradient of the log-probability of the action,\n",
    "- $ G_t $ is the cumulative return from time step $ t $.\n",
    "\n",
    "The REINFORCE algorithm updates the policy parameters based on the rewards obtained from each episode, reinforcing actions that lead to higher rewards and penalizing those that lead to lower rewards. However, REINFORCE tends to suffer from high variance, which makes learning slower and more unstable.\n",
    "\n",
    "#### **Baseline subtraction for variance reduction**\n",
    "\n",
    "To reduce the variance of the gradient estimates, policy gradient methods often use a **baseline**. The baseline does not change the expected value of the gradient but helps to reduce the variance in updates. The most commonly used baseline is the **value function** $ V(s_t) $, which estimates the expected return from state $ s_t $.\n",
    "\n",
    "The updated policy gradient with baseline subtraction is:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) (G_t - V(s_t)) \\right]\n",
    "$$\n",
    "\n",
    "By subtracting the baseline $ V(s_t) $, the algorithm focuses on actions that lead to returns better or worse than expected, reducing the variance in gradient estimates.\n",
    "\n",
    "#### **Actor-Critic methods**\n",
    "\n",
    "**Actor-Critic methods** combine the advantages of policy gradient methods (actor) and value-based methods (critic). The actor represents the policy, and the critic estimates the value function $ V(s_t) $, which is used to guide the actor’s policy updates. Instead of using the full return $ G_t $, the actor uses the **advantage function** $ A(s_t, a_t) $ to update the policy. The advantage function measures how much better or worse an action is compared to the expected outcome:\n",
    "\n",
    "$$\n",
    "A(s_t, a_t) = G_t - V(s_t)\n",
    "$$\n",
    "\n",
    "The actor is updated using the advantage, which reduces the variance of the updates while still improving the policy. The critic, on the other hand, is trained to minimize the error between the predicted value $ V(s_t) $ and the actual return $ G_t $.\n",
    "\n",
    "#### **Proximal Policy Optimization (PPO)**\n",
    "\n",
    "**Proximal Policy Optimization (PPO)** is an advanced policy gradient method that introduces a constraint on how much the policy can change in each update, ensuring more stable learning. PPO uses a clipped objective function to prevent large updates that could destabilize the policy. The PPO update rule prevents the policy from deviating too far from the previous policy:\n",
    "\n",
    "$$\n",
    "L^{\\text{PPO}}(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\min \\left( r_t(\\theta) A(s_t, a_t), \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A(s_t, a_t) \\right) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ r_t(\\theta) $ is the ratio of the new policy to the old policy,\n",
    "- $ A(s_t, a_t) $ is the advantage function,\n",
    "- $ \\epsilon $ is a small threshold that limits how much the policy is allowed to change.\n",
    "\n",
    "By constraining the updates, PPO ensures stable and reliable policy improvement, making it one of the most popular and widely used policy gradient methods in reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for implementing policy gradient methods in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules for building the policy network and interacting with the environment in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you configure the environment to utilize a GPU for training policy gradient agents in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the environment for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you load a reinforcement learning environment using OpenAI Gym for training a policy gradient agent?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you retrieve the state and action space of the Gym environment for defining the policy network input and output?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you reset the environment and retrieve the initial state for the training episodes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the policy network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you define the architecture of the policy network using `torch.nn.Module`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you implement the forward pass of the policy network to output action probabilities for a given state?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you initialize the weights of the policy network to ensure stable training and faster convergence?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the action selection policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you implement an action selection mechanism by sampling from the policy network’s output?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you store the log-probabilities of the selected actions to be used later in computing the policy gradient?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you modify the action selection process for continuous action spaces using different probability distributions ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the policy gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you compute the reward-to-go (discounted sum of future rewards) for each action in an episode?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you calculate the policy gradient using the log-probabilities of actions and the corresponding rewards?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you implement a method to compute the loss function for policy gradient updates using the log-probabilities and rewards?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the REINFORCE algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you collect experiences (state, action, reward) for each episode and store them for gradient computation?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you compute the policy gradient for an entire episode using the REINFORCE algorithm?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you apply backpropagation and perform gradient descent to update the weights of the policy network based on the computed gradients?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the policy gradient agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you implement the training loop for a policy gradient agent, including resetting the environment and selecting actions based on the policy?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you track and store the rewards for each episode to monitor the agent’s performance over time?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you update the policy network at the end of each episode using the policy gradient computed with the REINFORCE algorithm?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the policy gradient agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you evaluate the policy gradient agent by running it on the environment without exploration (i.e., using deterministic behavior)?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you visualize the cumulative reward over episodes to assess the performance of the trained policy gradient agent?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you compare the agent’s performance when using a policy gradient method to other reinforcement learning methods?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you experiment with different learning rates to observe their impact on the agent’s learning speed and convergence?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you adjust the discount factor (gamma) to analyze how it affects the agent’s long-term planning and reward optimization?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q27: How do you experiment with different policy network architectures, such as adding more layers or units, and analyze the effect on performance?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q28: How do you experiment with different batch sizes for updating the policy and observe their impact on training stability?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q29: How do you experiment with different methods of reward normalization to improve the stability of the policy gradient updates?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q30: How do you implement and evaluate an entropy bonus to encourage exploration during training and avoid premature convergence?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
