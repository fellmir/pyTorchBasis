{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep convolutional generative adversarial network (DCGAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding DCGAN](#understanding-dcgan)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Preparing the dataset](#preparing-the-dataset)\n",
    "4. [Building the Generator](#building-the-generator)\n",
    "5. [Building the Discriminator](#building-the-discriminator)\n",
    "6. [Initializing weights for the models](#initializing-weights-for-the-models)\n",
    "7. [Defining loss functions and optimizers](#defining-loss-functions-and-optimizers)\n",
    "8. [Training the DCGAN](#training-the-dcgan)\n",
    "9. [Visualizing generated samples](#visualizing-generated-samples)\n",
    "10. [Evaluating the model](#evaluating-the-model)\n",
    "11. [Experimenting with hyperparameters](#experimenting-with-hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding DCGAN\n",
    "\n",
    "Deep Convolutional Generative Adversarial Networks (DCGAN) are a type of GAN (Generative Adversarial Network) that incorporate convolutional layers, enabling the generation of more realistic images. DCGAN builds on the basic GAN architecture by using convolutional networks, which are particularly well-suited for handling image data, leading to significant improvements in image generation quality. The key innovation of DCGAN lies in the use of convolutional and transposed convolutional layers, allowing both the generator and discriminator networks to better capture spatial structures in images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generative adversarial networks (GANs): A brief overview**\n",
    "\n",
    "In a GAN, two neural networks—the generator and the discriminator—are trained simultaneously in a competitive setting:\n",
    "- **Generator (G)**: This network generates images from random noise, aiming to produce images that are indistinguishable from real ones.\n",
    "- **Discriminator (D)**: This network evaluates whether a given image is real or generated. It aims to correctly classify real images from the training set and generated images from the generator.\n",
    "\n",
    "GANs operate in a framework where the generator tries to fool the discriminator by producing increasingly realistic images, while the discriminator improves its ability to distinguish between real and generated images.\n",
    "\n",
    "### **DCGAN architecture**\n",
    "\n",
    "DCGAN extends the basic GAN by using deep convolutional neural networks (CNNs), which are highly effective for image processing. The main components of DCGAN include:\n",
    "- **Convolutional layers**: Both the generator and the discriminator in DCGAN use convolutional and transposed convolutional layers to learn spatial hierarchies in images. These layers allow the networks to capture complex patterns, such as textures and shapes.\n",
    "- **Batch normalization**: DCGAN uses batch normalization to stabilize training. Batch normalization normalizes the inputs to each layer, helping to maintain stable gradient flow and reducing the likelihood of issues such as vanishing or exploding gradients.\n",
    "- **Leaky ReLU and ReLU**: The discriminator uses the **Leaky ReLU** activation function, which ensures that neurons don’t stop learning, even when they output small or negative values. The generator, on the other hand, uses **ReLU** in most layers to ensure non-linearity and generate more complex patterns.\n",
    "- **Tanh activation**: In the generator’s output layer, the **tanh** activation function is used to map the generated images to the appropriate pixel range.\n",
    "\n",
    "### **The generator network**\n",
    "\n",
    "The generator network is responsible for converting random noise into realistic images. It takes a random noise vector as input and transforms it through a series of transposed convolutional layers. These layers progressively upsample the noise, increasing the resolution of the output until it reaches the desired image size.\n",
    "\n",
    "Key features of the generator:\n",
    "- **Input**: The generator starts with a noise vector, which serves as the seed for generating an image.\n",
    "- **Transposed convolutional layers**: These layers allow the generator to upsample the input and increase the spatial dimensions of the image. The upsampling process is critical for transforming the low-dimensional noise into a high-dimensional image.\n",
    "- **Activation functions**: The generator uses ReLU activation functions in its hidden layers, which introduce non-linearity and allow the network to learn more complex patterns.\n",
    "\n",
    "The ultimate goal of the generator is to produce images that are indistinguishable from real images, making it increasingly difficult for the discriminator to correctly identify them as fake.\n",
    "\n",
    "### **The discriminator network**\n",
    "\n",
    "The discriminator’s job is to classify whether an input image is real or generated by the generator. It is a convolutional neural network that takes an image as input and outputs a probability indicating whether the image is real or fake.\n",
    "\n",
    "Key features of the discriminator:\n",
    "- **Input**: The discriminator receives an image as input, either from the training set (real image) or from the generator (generated image).\n",
    "- **Convolutional layers**: The discriminator uses convolutional layers with strides to downsample the image and extract features at multiple scales. This allows it to learn useful representations of the image, such as edges, textures, and shapes.\n",
    "- **Leaky ReLU activation**: The use of Leaky ReLU ensures that the network remains capable of learning even when certain neurons output negative values.\n",
    "- **Output**: The final layer of the discriminator outputs a probability that represents how likely the input image is real.\n",
    "\n",
    "The discriminator's objective is to correctly distinguish between real and generated images, providing feedback to the generator to improve its image synthesis capabilities.\n",
    "\n",
    "### **Training DCGAN: The adversarial process**\n",
    "\n",
    "DCGAN training involves a back-and-forth process between the generator and discriminator. The generator tries to improve by producing better images that can fool the discriminator, while the discriminator learns to become more accurate at detecting fakes. This competition drives both networks to improve.\n",
    "\n",
    "The generator is updated based on how well it can fool the discriminator, while the discriminator is updated based on its ability to differentiate real images from generated ones. Over time, as the generator improves, the discriminator must adapt to keep up, leading to more realistic image generation.\n",
    "\n",
    "### **Challenges in training DCGANs**\n",
    "\n",
    "Training DCGANs comes with several challenges:\n",
    "- **Mode collapse**: The generator may start producing only a limited variety of outputs, regardless of the input noise, leading to a lack of diversity in the generated images. This is known as mode collapse.\n",
    "- **Instability**: The adversarial training process can sometimes be unstable, with the generator and discriminator oscillating in performance rather than converging.\n",
    "- **Vanishing gradients**: If the discriminator becomes too powerful, the generator may struggle to receive useful feedback, leading to slow or halted learning.\n",
    "\n",
    "### **Applications of DCGANs**\n",
    "\n",
    "DCGANs have been applied to a wide range of tasks, particularly in image generation. Some key applications include:\n",
    "- **Image generation**: DCGANs can generate high-quality images in domains such as fashion, art, and facial generation.\n",
    "- **Data augmentation**: They can be used to generate synthetic images to augment training datasets, particularly when labeled data is scarce.\n",
    "- **Image-to-image translation**: DCGANs can be a starting point for tasks like translating sketches into realistic photos or transforming one type of image into another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Maths**\n",
    "\n",
    "#### **Objective of GANs**\n",
    "\n",
    "At the heart of GANs, including DCGANs, is a minimax optimization problem where the generator and discriminator networks are trained simultaneously with opposing objectives. The generator $ G $ aims to create data that mimics the real data distribution, while the discriminator $ D $ seeks to distinguish between real and fake data. This adversarial setup can be expressed through the following objective:\n",
    "\n",
    "$$\n",
    "\\min_G \\max_D \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)} [\\log (1 - D(G(z)))]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ D(x) $ is the discriminator's estimate of the probability that real data $ x $ is from the real data distribution.\n",
    "- $ G(z) $ represents the generator's attempt to produce data from random noise $ z $, where $ z $ is sampled from a prior distribution $ p_z(z) $.\n",
    "- $ p_{\\text{data}}(x) $ is the distribution of real data.\n",
    "\n",
    "The generator tries to minimize the likelihood that the discriminator correctly identifies generated data as fake, while the discriminator tries to maximize its ability to distinguish real from fake data.\n",
    "\n",
    "#### **Generator network: Mathematical formulation**\n",
    "\n",
    "The generator $ G $ is responsible for mapping a random noise vector $ z $ from a latent space to an output that resembles the real data distribution. Mathematically, the generator can be represented as a function $ G: z \\rightarrow x $, where $ z \\sim p_z(z) $ and $ x $ is the generated data.\n",
    "\n",
    "The generator consists of transposed convolutional layers that upsample the input noise. Each layer applies the following transformation:\n",
    "\n",
    "$$\n",
    "h_{l+1} = f(W_l^T h_l + b_l)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ h_l $ is the activation at layer $ l $,\n",
    "- $ W_l $ is the weight matrix of the transposed convolutional layer,\n",
    "- $ b_l $ is the bias vector for the layer,\n",
    "- $ f $ is the activation function (such as ReLU in the hidden layers and tanh in the output layer).\n",
    "\n",
    "The generator's final goal is to produce data $ G(z) $ that maximizes the likelihood that the discriminator labels it as real, i.e., maximizing $ D(G(z)) $.\n",
    "\n",
    "#### **Discriminator network: Mathematical formulation**\n",
    "\n",
    "The discriminator $ D $ is a convolutional neural network that outputs the probability that an input image is real. It takes either a real image $ x $ or a generated image $ G(z) $ and applies a series of downsampling convolutions to extract features. The discriminator is trained to maximize its ability to correctly classify real images as real and generated images as fake.\n",
    "\n",
    "The discriminator applies the following transformation at each layer:\n",
    "\n",
    "$$\n",
    "h_{l+1} = f(W_l h_l + b_l)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ h_l $ is the activation at layer $ l $,\n",
    "- $ W_l $ is the weight matrix of the convolutional layer,\n",
    "- $ b_l $ is the bias vector for the layer,\n",
    "- $ f $ is the activation function (Leaky ReLU in most layers and sigmoid in the output layer).\n",
    "\n",
    "The discriminator is trained to maximize $ \\log D(x) $ for real data and $ \\log(1 - D(G(z))) $ for generated data, where $ D(x) $ represents the probability that $ x $ is real.\n",
    "\n",
    "#### **Minimax game and loss functions**\n",
    "\n",
    "The training process for DCGAN involves solving a minimax optimization problem. The generator and discriminator are optimized alternately:\n",
    "- The **discriminator's loss** is calculated by comparing its predictions on both real and fake images:\n",
    "  \n",
    "$$\n",
    "L_D = -\\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] - \\mathbb{E}_{z \\sim p_z(z)}[\\log (1 - D(G(z)))]\n",
    "$$\n",
    "\n",
    "- The **generator's loss** is computed based on the discriminator's ability to classify the generated images as real:\n",
    "  \n",
    "$$\n",
    "L_G = -\\mathbb{E}_{z \\sim p_z(z)}[\\log D(G(z))]\n",
    "$$\n",
    "\n",
    "The generator is trained to minimize this loss, while the discriminator is trained to maximize the probability of correctly distinguishing between real and fake images.\n",
    "\n",
    "#### **Batch normalization**\n",
    "\n",
    "Batch normalization is applied in both the generator and discriminator to stabilize training. The mathematical operation for batch normalization at layer $ l $ is:\n",
    "\n",
    "$$\n",
    "\\hat{h}_l = \\frac{h_l - \\mu}{\\sigma + \\epsilon}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ h_l $ is the activation of layer $ l $,\n",
    "- $ \\mu $ is the mean of the activations in the batch,\n",
    "- $ \\sigma $ is the standard deviation of the activations in the batch,\n",
    "- $ \\epsilon $ is a small constant to prevent division by zero.\n",
    "\n",
    "Batch normalization ensures that the activations at each layer have a consistent distribution, which helps prevent vanishing or exploding gradients during training.\n",
    "\n",
    "#### **Transposed convolutions**\n",
    "\n",
    "In the generator, **transposed convolutions** (also known as deconvolutions) are used to upsample the noise vector and increase the resolution of the generated images. Mathematically, a transposed convolution is the reverse of a standard convolution. It can be expressed as:\n",
    "\n",
    "$$\n",
    "y_{i,j} = \\sum_k W_{k} x_{i+k, j+k}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ y_{i,j} $ is the output pixel at position $ (i, j) $,\n",
    "- $ W_k $ is the filter matrix,\n",
    "- $ x $ is the input to the transposed convolution.\n",
    "\n",
    "This operation increases the spatial resolution of the input while applying learned filters, allowing the generator to produce high-resolution images from low-dimensional noise.\n",
    "\n",
    "#### **Training stability and the role of gradients**\n",
    "\n",
    "The adversarial nature of GANs often leads to instability in training. One reason is that the gradients passed to the generator during training can become too small or too large, making learning difficult. This problem can be mitigated using techniques such as:\n",
    "- **Batch normalization**: Normalizes activations and stabilizes gradient flow.\n",
    "- **Leaky ReLU**: Prevents the “dying ReLU” problem, where neurons output zero gradients, by allowing a small gradient for negative inputs.\n",
    "\n",
    "Additionally, when the discriminator becomes too powerful, the gradients flowing back to the generator become small, making it hard for the generator to improve. To address this, a common strategy is to balance the training updates between the generator and the discriminator, ensuring neither network dominates the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for implementing DCGAN in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules for building the generator, discriminator, and handling data in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you configure your environment to utilize GPU for training the DCGAN in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you load a dataset (e.g., CelebA or CIFAR-10) using `torchvision.datasets` in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you apply transformations like resizing, normalization, and converting images to tensors using `torchvision.transforms`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you create a DataLoader in PyTorch to load the dataset in batches for training?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you define the architecture of the generator model using `torch.nn.Module`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you implement transposed convolutional layers in the generator to upsample random noise vectors into images?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you implement the forward pass in the generator model to generate images from latent space (random noise)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Discriminator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you define the architecture of the discriminator model using `torch.nn.Module`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you implement convolutional layers in the discriminator to downsample images and predict whether they are real or fake?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you implement the forward pass in the discriminator model to classify input images as real or fake?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing weights for the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you define a custom weight initialization function for the generator and discriminator models?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you apply the custom weight initialization to the generator and discriminator models in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining loss functions and optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you define the loss function for the discriminator using binary cross-entropy loss (BCE loss)?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you define the loss function for the generator using binary cross-entropy loss (BCE loss)?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you set up the Adam optimizer for both the generator and discriminator models in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the DCGAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you implement the training loop for the DCGAN, alternating between training the discriminator and generator?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you compute the loss for the discriminator using both real and fake images during each training iteration?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you compute the loss for the generator based on how well it fools the discriminator into classifying fake images as real?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you update the weights of the generator and discriminator after computing the loss during training?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing generated samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you generate images from the trained generator model at different stages of training to monitor progress?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you visualize generated images alongside real images to compare the quality of the generator’s output?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you save generated images during training to evaluate the progression of the DCGAN's performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you evaluate the quality of the images generated by the DCGAN after a certain number of epochs?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you save the trained generator and discriminator models for later use or evaluation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q27: How do you experiment with different latent vector sizes in the generator and observe their effect on the quality of generated images?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q28: How do you adjust the learning rates for the generator and discriminator to stabilize training?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q29: How do you experiment with different architectures for the generator (e.g., adding more layers or adjusting filter sizes) and observe the effect on image quality?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q30: How do you experiment with different batch sizes and observe their effect on the training stability and quality of generated images?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
