{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex models with custom modules in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding custom modules](#understanding-custom-modules)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Building custom modules from custom layers](#building-custom-modules-from-custom-layers)\n",
    "4. [Creating reusable blocks of layers](#creating-reusable-blocks-of-layers)\n",
    "5. [Combining custom modules into a complex model](#combining-custom-modules-into-a-complex-model)\n",
    "6. [Implementing custom residual blocks](#implementing-custom-residual-blocks)\n",
    "7. [Building an encoder-decoder architecture with custom modules](#building-an-encoder-decoder-architecture-with-custom-modules)\n",
    "8. [Training and evaluating the complex model](#training-and-evaluating-the-complex-model)\n",
    "9. [Experimenting with different model configurations](#experimenting-with-different-model-configurations)\n",
    "10. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding custom modules\n",
    "\n",
    "In PyTorch, building complex models often requires creating modular and reusable components. Custom modules provide the flexibility to design specialized architectures that go beyond the standard layers provided by PyTorch. Complex models with custom modules allow for breaking down large, sophisticated networks into manageable and reusable parts, facilitating easier debugging, extension, and experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What are custom modules in PyTorch?**\n",
    "\n",
    "Custom modules in PyTorch are classes that inherit from `torch.nn.Module`, the core building block for all neural networks in PyTorch. These custom modules encapsulate the logic for specific parts of the model, making them reusable, modular, and easy to integrate into more extensive networks.\n",
    "\n",
    "Complex models often consist of several layers or submodules, each performing a specific function or sequence of operations. By designing custom modules, each component can be defined separately, reused across different parts of the model, and even fine-tuned or modified independently.\n",
    "\n",
    "### **Why use custom modules in complex models?**\n",
    "\n",
    "Using custom modules is crucial for several reasons, particularly when building more sophisticated architectures:\n",
    "- **Modularity**: Custom modules break down complex models into smaller, manageable components that are easier to debug and understand.\n",
    "- **Reusability**: Once a custom module is defined, it can be reused across different parts of the model or even in entirely different models. This encourages cleaner and more maintainable code.\n",
    "- **Flexibility**: Custom modules can incorporate non-standard layers, complex operations, or even integrate external components, providing flexibility when designing novel architectures.\n",
    "- **Readability and scalability**: When models grow large, maintaining clean and readable code becomes challenging. Custom modules help organize the code, making it easier to scale up without sacrificing clarity.\n",
    "\n",
    "### **The structure of custom modules**\n",
    "\n",
    "Custom modules are typically defined by subclassing `torch.nn.Module` and consist of two key parts:\n",
    "- **Initialization (`__init__`)**: This is where the components (such as layers or other submodules) are defined. During initialization, the module sets up its internal state, including the parameters, sub-layers, and constants that it will use.\n",
    "- **Forward pass (`forward`)**: This method defines the logic for how input data is processed. It takes the input and applies the necessary transformations using the components defined in the `__init__` method. The forward pass contains the actual operations that map inputs to outputs.\n",
    "\n",
    "### **Building complex models with custom modules**\n",
    "\n",
    "Complex models are often hierarchical, meaning they are composed of multiple custom modules, each handling a specific part of the network. These modules can range from simple linear layers to more advanced components like convolutional layers, attention mechanisms, or recurrent modules. By combining these modules, a large model is built in a more structured and manageable way.\n",
    "\n",
    "#### **Submodules within a custom module**\n",
    "\n",
    "A custom module can itself contain other custom modules (referred to as submodules). This modular approach allows for a layered architecture, where each submodule performs a specific function. The submodules can be combined sequentially or in parallel, depending on the model's requirements.\n",
    "\n",
    "For example, a complex model might consist of:\n",
    "- An encoder module that processes the input data.\n",
    "- A decoder module that generates the output based on the encoded representation.\n",
    "- Intermediate layers or attention modules that process the data between the encoder and decoder.\n",
    "\n",
    "#### **Sequential models**\n",
    "\n",
    "PyTorch provides the `torch.nn.Sequential` class to streamline the process of stacking layers sequentially. This class can be useful when a custom module involves applying multiple layers in a fixed order. However, `nn.Sequential` is limited in that it doesn't support more flexible data flow, such as branching or using different submodules conditionally. For more sophisticated models, custom modules should be defined explicitly to handle non-linear data flows.\n",
    "\n",
    "#### **Parameter sharing and memory efficiency**\n",
    "\n",
    "Custom modules allow for parameter sharing, where the same layer or set of parameters is used across multiple parts of the model. This can be useful in architectures like recurrent models or attention-based models where certain transformations are reused multiple times.\n",
    "\n",
    "By reusing layers or weights, parameter sharing not only reduces the number of learned parameters, but also helps improve memory efficiency during training and inference.\n",
    "\n",
    "### **Examples of complex models with custom modules**\n",
    "\n",
    "#### **Example 1: Encoder-decoder architecture**\n",
    "\n",
    "An encoder-decoder architecture, commonly used in tasks like machine translation or sequence-to-sequence learning, can be implemented using custom modules for both the encoder and decoder.\n",
    "\n",
    "- **Encoder module**: The encoder processes the input sequence and produces a context or latent representation. It could consist of several stacked layers such as LSTM or GRU units.\n",
    "- **Decoder module**: The decoder takes the encoded representation and generates the output sequence, often using another LSTM or GRU unit to decode the input.\n",
    "- **Custom attention module**: Between the encoder and decoder, an attention mechanism may be included to help the model focus on different parts of the input sequence while generating each token in the output sequence.\n",
    "\n",
    "Each of these components can be implemented as a separate custom module, allowing for flexibility in how they are combined and trained.\n",
    "\n",
    "#### **Example 2: Residual networks (ResNet)**\n",
    "\n",
    "Residual networks introduce skip connections, which allow gradients to flow more easily during backpropagation by bypassing one or more layers. This architecture can be implemented using custom modules to define the building blocks of the residual connections.\n",
    "\n",
    "- A **Residual block** could be implemented as a custom module that adds the input to the output of a series of convolutional layers, making the learning of identity mappings easier.\n",
    "- These blocks can then be stacked to form deep residual networks, with each block being a reusable component of the architecture.\n",
    "\n",
    "### **Designing modular networks for experimentation**\n",
    "\n",
    "Custom modules make it easier to experiment with different architectures by allowing components to be swapped out or modified without changing the entire model. For example, if a certain layer in the model is underperforming, it can be replaced with a more advanced custom module without having to redesign the entire network.\n",
    "\n",
    "This modularity also allows for rapid prototyping, where different configurations of a model can be tested quickly by assembling them from pre-defined custom modules. Additionally, complex models with custom modules can integrate external modules or specialized components (such as those built in C++ or CUDA), expanding the model’s capabilities.\n",
    "\n",
    "### **Training and backpropagation in custom modules**\n",
    "\n",
    "When using custom modules in complex models, PyTorch handles backpropagation automatically. As long as the custom modules use differentiable operations (such as matrix multiplication or activation functions), PyTorch tracks the gradients through the computational graph. During training, the optimizer updates the parameters of each custom module in the same way it does for standard PyTorch layers.\n",
    "\n",
    "Since custom modules integrate seamlessly with PyTorch’s autograd system, there is no need to manually compute gradients. This ensures that even the most complex models, composed of multiple submodules and custom operations, can be trained efficiently with minimal effort.\n",
    "\n",
    "### **Advantages of using custom modules in complex models**\n",
    "\n",
    "Custom modules offer several advantages when building complex models:\n",
    "- **Code organization**: Custom modules improve code readability and maintainability by breaking down large models into manageable components.\n",
    "- **Reusability**: Modules can be reused in different models or shared across multiple parts of the same model, reducing redundancy in the codebase.\n",
    "- **Flexibility**: Custom modules allow for the integration of novel architectures, external components, or unique operations not covered by standard PyTorch layers.\n",
    "- **Debugging**: Breaking a large model into smaller custom modules makes it easier to debug and isolate issues, especially when experimenting with new architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Maths**\n",
    "\n",
    "#### **Modular representation of complex models**\n",
    "\n",
    "In neural networks, a model can be mathematically described as a function $ f $ that maps an input $ x $ to an output $ y $ through a sequence of transformations, represented as:\n",
    "\n",
    "$$\n",
    "y = f(x) = f_n(f_{n-1}(...f_1(x)))\n",
    "$$\n",
    "\n",
    "Where each $ f_i $ represents a layer or operation in the network. In the case of complex models with custom modules, each function $ f_i $ can be a custom module that encapsulates a set of transformations, submodules, or learnable parameters.\n",
    "\n",
    "Custom modules, like standard layers, often define the transformation of an input tensor through a series of operations involving matrix multiplications, non-linear functions, and other operations.\n",
    "\n",
    "#### **Forward pass through custom modules**\n",
    "\n",
    "Consider a custom module that represents a linear transformation followed by an activation function. The forward pass for such a custom module can be described mathematically as:\n",
    "\n",
    "$$\n",
    "y = f(Wx + b)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W $ is the weight matrix,\n",
    "- $ x $ is the input tensor,\n",
    "- $ b $ is the bias vector,\n",
    "- $ f $ is a non-linear activation function (such as ReLU, sigmoid, or tanh),\n",
    "- $ y $ is the output.\n",
    "\n",
    "The custom module applies the linear transformation $ Wx + b $ followed by the activation function, which introduces non-linearity into the model.\n",
    "\n",
    "#### **Parameter initialization in complex models**\n",
    "\n",
    "When using custom modules, the parameters (such as weights and biases) are initialized according to specific initialization schemes to ensure that the model starts from a stable point. For example, a common initialization method is **Xavier initialization**, where the weights $ W $ are sampled from a uniform distribution:\n",
    "\n",
    "$$\n",
    "W \\sim U\\left(-\\frac{\\sqrt{6}}{\\sqrt{n_{\\text{in}} + n_{\\text{out}}}}, \\frac{\\sqrt{6}}{\\sqrt{n_{\\text{in}} + n_{\\text{out}}}}\\right)\n",
    "$$\n",
    "\n",
    "Where $ n_{\\text{in}} $ is the number of input units, and $ n_{\\text{out}} $ is the number of output units for that layer.\n",
    "\n",
    "Alternatively, **He initialization** is used when the activation function is ReLU, and the weights are initialized from a normal distribution:\n",
    "\n",
    "$$\n",
    "W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\text{in}}}\\right)\n",
    "$$\n",
    "\n",
    "These initialization schemes ensure that the gradients flow properly during backpropagation and prevent issues like vanishing or exploding gradients.\n",
    "\n",
    "#### **Sequential and parallel operations in complex models**\n",
    "\n",
    "In complex models, operations may be applied sequentially or in parallel, depending on the architecture. Mathematically, sequential operations can be represented as:\n",
    "\n",
    "$$\n",
    "y = f_n(f_{n-1}(...f_1(x)))\n",
    "$$\n",
    "\n",
    "This describes a series of transformations applied one after another, where the output of each layer becomes the input to the next.\n",
    "\n",
    "Parallel operations involve splitting the input $ x $ into multiple branches, applying different transformations in parallel, and then combining the results. For example, if there are two parallel transformations $ f_1 $ and $ f_2 $, the output can be described as:\n",
    "\n",
    "$$\n",
    "y = g(f_1(x), f_2(x))\n",
    "$$\n",
    "\n",
    "Where $ g $ is a function that combines the results, such as element-wise addition or concatenation.\n",
    "\n",
    "#### **Residual connections in complex models**\n",
    "\n",
    "In residual networks (ResNet), skip or residual connections are introduced to help gradients flow more easily during backpropagation. The mathematical representation of a residual block is:\n",
    "\n",
    "$$\n",
    "y = f(x) + x\n",
    "$$\n",
    "\n",
    "Here, $ f(x) $ is a series of transformations (e.g., convolution, batch normalization, and ReLU activation), and the input $ x $ is added directly to the output, forming a shortcut connection. This simple operation helps mitigate the problem of vanishing gradients in deep networks by allowing the model to learn identity mappings.\n",
    "\n",
    "The gradients in the residual block are computed using the chain rule, and the residual connection ensures that the gradient is not diminished when backpropagated through many layers.\n",
    "\n",
    "#### **Attention mechanisms in complex models**\n",
    "\n",
    "In more advanced architectures, such as transformer-based models, custom modules often implement attention mechanisms. The **scaled dot-product attention** is a common component in these models, and it can be described mathematically as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ Q $ is the query matrix,\n",
    "- $ K $ is the key matrix,\n",
    "- $ V $ is the value matrix,\n",
    "- $ d_k $ is the dimensionality of the key vectors.\n",
    "\n",
    "The attention mechanism computes the dot product of the query and key matrices, scales it by $ \\frac{1}{\\sqrt{d_k}} $, and applies a softmax function to generate attention weights. These weights are then multiplied by the value matrix $ V $ to produce the output. Custom modules that implement attention mechanisms allow the model to focus on specific parts of the input data dynamically.\n",
    "\n",
    "#### **Backpropagation and gradient flow in complex models**\n",
    "\n",
    "When training complex models with custom modules, PyTorch automatically handles backpropagation by computing gradients through the computational graph. The gradients of the loss function with respect to each parameter are computed using the chain rule.\n",
    "\n",
    "For a custom module with a transformation $ y = f(Wx + b) $, the gradients with respect to the parameters are:\n",
    "- Gradient with respect to the weight matrix $ W $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\delta \\cdot x^T\n",
    "$$\n",
    "\n",
    "Where $ \\delta $ is the gradient of the loss with respect to the output $ y $, and $ x^T $ is the transpose of the input.\n",
    "\n",
    "- Gradient with respect to the bias $ b $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\delta\n",
    "$$\n",
    "\n",
    "- Gradient with respect to the input $ x $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = W^T \\cdot \\delta\n",
    "$$\n",
    "\n",
    "These gradients are then used by the optimizer to update the parameters during training.\n",
    "\n",
    "#### **Parameter sharing in custom modules**\n",
    "\n",
    "Parameter sharing in custom modules allows the same set of parameters to be used across multiple layers or operations. This is useful in models where the same transformation is applied repeatedly. Mathematically, parameter sharing can be expressed as:\n",
    "\n",
    "$$\n",
    "y_t = f(Wx_t + b)\n",
    "$$\n",
    "\n",
    "Where the same weight matrix $ W $ and bias $ b $ are applied to different inputs $ x_t $ across multiple time steps or layers.\n",
    "\n",
    "Parameter sharing reduces the number of learned parameters and ensures consistent transformations across the model, which is particularly useful in architectures like recurrent neural networks (RNNs) or attention-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for building and training complex models with custom modules in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules for defining custom modules and training models in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you configure your environment to use a GPU for training complex models, and how do you fallback to CPU in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building custom modules from custom layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you define a custom module by combining multiple custom layers using `torch.nn.Module`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you implement the forward pass for a custom module that integrates both custom and built-in layers in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you create a custom module that applies a linear transformation followed by a custom activation layer?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you apply a custom module to a simple input tensor to verify its behavior in a standalone setting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating reusable blocks of layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you define a reusable block of layers, such as a convolutional block, by combining multiple layers in a custom module?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you implement a custom convolutional block that includes convolution, batch normalization, and activation layers?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you stack multiple instances of a custom block in a larger neural network architecture?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining custom modules into a complex model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you combine custom modules into a more complex model, such as a multi-layer CNN or a deep feedforward network?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you implement a forward pass that uses multiple custom modules to process data in a hierarchical fashion?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you apply dropout between custom modules to prevent overfitting in a complex model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing custom residual blocks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you implement a custom residual block that includes a skip connection for bypassing the main layer stack?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you integrate a residual block into a deeper network by stacking multiple residual blocks?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you apply layer normalization or batch normalization inside a custom residual block?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an encoder-decoder architecture with custom modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you define a custom encoder module that compresses input data into a lower-dimensional representation?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you build a decoder module that reconstructs the original data from the encoder’s representation?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you combine the custom encoder and decoder modules to create a full encoder-decoder architecture in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you apply the encoder-decoder architecture to a task such as autoencoding or image segmentation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluating the complex model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you set up a training loop to train a complex model built from custom modules on a dataset like CIFAR-10?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you define the loss function (e.g., CrossEntropyLoss) and optimizer (e.g., Adam) for training the complex model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you monitor training loss and accuracy during the training process to ensure the model is learning effectively?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you implement early stopping to prevent overfitting when training a complex model with custom modules?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you evaluate the complex model on a validation or test set to assess its performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with different model configurations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you modify the number of layers in the custom modules and observe the impact on model performance?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q27: How do you experiment with different activation functions inside custom modules and measure their effect on training speed and accuracy?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q28: How do you experiment with varying the number of filters or hidden units in the custom convolutional or linear layers?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q29: How do you perform an ablation study to determine the contribution of individual custom modules to the overall performance of the complex model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q30: How do you test different initialization techniques (e.g., Xavier, Kaiming) for weights in custom modules and observe their impact on model convergence?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
