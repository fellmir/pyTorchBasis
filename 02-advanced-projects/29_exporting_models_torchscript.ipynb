{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting models using TorchScript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding TorchScript and model exporting](#understanding-torchscript-and-model-exporting)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Building a simple PyTorch model](#building-a-simple-pytorch-model)\n",
    "4. [Tracing a model with TorchScript](#tracing-a-model-with-torchscript)\n",
    "5. [Scripting a model with TorchScript](#scripting-a-model-with-torchscript)\n",
    "6. [Saving and loading TorchScript models](#saving-and-loading-torchscript-models)\n",
    "7. [Running TorchScript models in C++](#running-torchscript-models-in-c)\n",
    "8. [Comparing performance: TorchScript vs. native PyTorch](#comparing-performance-torchscript-vs-native-pytorch)\n",
    "9. [Experimenting with optimizations](#experimenting-with-optimizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding TorchScript and model exporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Key concepts**\n",
    "TorchScript is a tool in PyTorch that allows models to be serialized and exported for use in production environments. By converting PyTorch models into an intermediate representation, TorchScript enables deployment in environments without a Python runtime, such as mobile devices or edge systems. It supports the same functionality as PyTorch while providing flexibility for optimized inference.\n",
    "\n",
    "Key features of TorchScript include:\n",
    "- **Tracing**: Converts a model into TorchScript by recording operations during a forward pass.\n",
    "- **Scripting**: Directly converts a model into TorchScript by analyzing its code, including control flows like loops and conditionals.\n",
    "- **Serialization**: Saves the model as a `.pt` file, enabling portability and reuse.\n",
    "- **Integration**: TorchScript models can run in C++ environments using the PyTorch C++ API, making them ideal for production deployment.\n",
    "\n",
    "TorchScript combines the dynamic nature of PyTorch with the static benefits required for efficient inference in production.\n",
    "\n",
    "### **Applications**\n",
    "Exporting models using TorchScript is essential for:\n",
    "- **Mobile deployment**: Running models on Android and iOS devices with PyTorch Mobile.\n",
    "- **Edge computing**: Deploying models on low-power devices for real-time applications.\n",
    "- **Cross-platform compatibility**: Using TorchScript models in C++ applications without a Python dependency.\n",
    "- **Optimized inference**: Improving inference speed and memory efficiency for production systems.\n",
    "\n",
    "### **Advantages**\n",
    "- **Portability**: Enables seamless deployment across various platforms and environments.\n",
    "- **Performance optimization**: Static graphs allow for optimizations that improve inference speed and reduce memory usage.\n",
    "- **Flexibility**: Supports dynamic models with scripting while enabling production-ready deployment.\n",
    "- **Ease of integration**: Simplifies using PyTorch models in non-Python environments.\n",
    "\n",
    "### **Challenges**\n",
    "- **Debugging**: Errors in TorchScript conversion can be challenging to trace and resolve.\n",
    "- **Limited Python support**: Some Python constructs and third-party libraries are not supported in TorchScript.\n",
    "- **Model compatibility**: Custom layers or operations may require additional adaptation for TorchScript compatibility.\n",
    "- **Static limitations**: Dynamic PyTorch functionalities may need to be rewritten or adjusted for TorchScript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries, such as PyTorch, for exporting models using TorchScript?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required PyTorch modules for exporting and working with TorchScript models?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you configure the environment to use GPU acceleration with TorchScript?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # use GPU if available\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple PyTorch model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you define a simple neural network in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)  # fully connected layer 1\n",
    "        self.fc2 = nn.Linear(128, 64)     # fully connected layer 2\n",
    "        self.fc3 = nn.Linear(64, 10)      # output layer for 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you implement the forward pass for the PyTorch model?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)            # flatten the input\n",
    "        x = F.relu(self.fc1(x))          # apply ReLU after fc1\n",
    "        x = F.relu(self.fc2(x))          # apply ReLU after fc2\n",
    "        x = self.fc3(x)                  # output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you train a simple PyTorch model on a small dataset or a synthetic dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:01<00:00, 8.87MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 305kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.65MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 454kB/s]\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                     # convert image to tensor\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # normalize with MNIST mean and std\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset  = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 0.2762\n",
      "Epoch 2 - Training Loss: 0.1155\n",
      "Epoch 3 - Training Loss: 0.0797\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)                         # forward pass\n",
    "        loss = criterion(outputs, labels)               # compute loss\n",
    "        optimizer.zero_grad()                           # reset gradients\n",
    "        loss.backward()                                 # backpropagation\n",
    "        optimizer.step()                                # update weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1} - Training Loss: {total_loss / len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing a model with TorchScript\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you use `torch.jit.trace` to trace a PyTorch model and convert it into TorchScript?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # set model to evaluation mode\n",
    "example_input = torch.randn(1, 1, 28, 28).to(device)  # dummy input for tracing\n",
    "traced_model = torch.jit.trace(model, example_input)  # perform tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you feed example inputs into the model during tracing to capture its computation graph?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self.1 : __torch__.SimpleNet,\n",
      "      %x : Float(1, 1, 28, 28, strides=[784, 784, 28, 1], requires_grad=0, device=cuda:0)):\n",
      "  %fc3 : __torch__.torch.nn.modules.linear.___torch_mangle_1.Linear = prim::GetAttr[name=\"fc3\"](%self.1)\n",
      "  %fc2 : __torch__.torch.nn.modules.linear.___torch_mangle_0.Linear = prim::GetAttr[name=\"fc2\"](%self.1)\n",
      "  %fc1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"fc1\"](%self.1)\n",
      "  %19 : int = prim::Constant[value=-1]() # C:\\Users\\Fellipe\\AppData\\Local\\Temp\\ipykernel_26344\\2950993803.py:9:0\n",
      "  %20 : int = prim::Constant[value=784]() # C:\\Users\\Fellipe\\AppData\\Local\\Temp\\ipykernel_26344\\2950993803.py:9:0\n",
      "  %21 : int[] = prim::ListConstruct(%19, %20)\n",
      "  %input.1 : Float(1, 784, strides=[784, 1], requires_grad=0, device=cuda:0) = aten::view(%x, %21) # C:\\Users\\Fellipe\\AppData\\Local\\Temp\\ipykernel_26344\\2950993803.py:9:0\n",
      "  %43 : Tensor = prim::CallMethod[name=\"forward\"](%fc1, %input.1)\n",
      "  %input.5 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cuda:0) = aten::relu(%43) # c:\\Users\\Fellipe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:1704:0\n",
      "  %44 : Tensor = prim::CallMethod[name=\"forward\"](%fc2, %input.5)\n",
      "  %input : Float(1, 64, strides=[64, 1], requires_grad=1, device=cuda:0) = aten::relu(%44) # c:\\Users\\Fellipe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:1704:0\n",
      "  %45 : Tensor = prim::CallMethod[name=\"forward\"](%fc3, %input)\n",
      "  return (%45)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(traced_model.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you run inference with the traced TorchScript model to verify that it works correctly?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = traced_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced Model Accuracy: 96.95%\n"
     ]
    }
   ],
   "source": [
    "print(f'Traced Model Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripting a model with TorchScript\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you use `torch.jit.script` to script a PyTorch model and convert it into TorchScript?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_model = torch.jit.script(model)  # convert model using scripting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you handle control flow in your PyTorch model when using scripting?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControlFlowNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ControlFlowNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        for _ in range(2):               # example of loop\n",
    "            x = F.relu(self.fc2(x))\n",
    "        if x.sum() > 0:                  # example of conditional\n",
    "            x = self.fc3(x)\n",
    "        else:\n",
    "            x = torch.zeros_like(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_model = ControlFlowNet().to(device)\n",
    "scripted_cf_model = torch.jit.script(cf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you compare the scripted model’s behavior to the original PyTorch model to ensure consistency?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControlFlowNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ControlFlowNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2a = nn.Linear(128, 64)\n",
    "        self.fc2b = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2a(x))\n",
    "        x = F.relu(self.fc2b(x))\n",
    "        if x.sum() > 0:\n",
    "            x = self.fc3(x)\n",
    "        else:\n",
    "            x = torch.zeros_like(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_model = ControlFlowNet().to(device)\n",
    "scripted_cf_model = torch.jit.script(cf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=ControlFlowNet\n",
       "  (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "  (fc2a): RecursiveScriptModule(original_name=Linear)\n",
       "  (fc2b): RecursiveScriptModule(original_name=Linear)\n",
       "  (fc3): RecursiveScriptModule(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_model.eval()\n",
    "scripted_cf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sample = torch.randn(1, 1, 28, 28).to(device)\n",
    "with torch.no_grad():\n",
    "    original_output = cf_model(input_sample)\n",
    "    scripted_output = scripted_cf_model(input_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.allclose(original_output, scripted_output, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading TorchScript models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you save a traced or scripted TorchScript model using `model.save()`?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model.save('traced_mnist_model.pt')   # save traced model\n",
    "scripted_cf_model.save('scripted_cf_model.pt')  # save scripted model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you load a saved TorchScript model using `torch.jit.load()` for inference?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_traced = torch.jit.load('traced_mnist_model.pt').to(device)\n",
    "loaded_scripted = torch.jit.load('scripted_cf_model.pt').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you verify that the saved and loaded TorchScript model produces the same results as the original model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_traced.eval()\n",
    "with torch.no_grad():\n",
    "    sample = torch.randn(1, 1, 28, 28).to(device)\n",
    "    original_out = traced_model(sample)\n",
    "    loaded_out = loaded_traced(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.allclose(original_out, loaded_out, atol=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running TorchScript models in C++\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you export a TorchScript model to run it in a C++ environment?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see Q13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you set up a simple C++ project using LibTorch to load and run the TorchScript model?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the following as a main.cpp file:\n",
    "\n",
    "# #include <torch/script.h>  // One-stop header.\n",
    "# #include <iostream>\n",
    "# #include <memory>\n",
    "\n",
    "# int main() {\n",
    "#     torch::jit::script::Module module;\n",
    "#     try {\n",
    "#         // Load the TorchScript model\n",
    "#         module = torch::jit::load(\"traced_mnist_model.pt\");\n",
    "\n",
    "#         // Prepare example input\n",
    "#         torch::Tensor input = torch::randn({1, 1, 28, 28});\n",
    "\n",
    "#         // Run inference\n",
    "#         std::vector<torch::jit::IValue> inputs;\n",
    "#         inputs.push_back(input);\n",
    "#         at::Tensor output = module.forward(inputs).toTensor();\n",
    "\n",
    "#         std::cout << \"Output Tensor: \" << output << std::endl;\n",
    "#     }\n",
    "#     catch (const c10::Error& e) {\n",
    "#         std::cerr << \"Error loading the model.\\n\";\n",
    "#         return -1;\n",
    "#     }\n",
    "\n",
    "#     return 0;\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you pass input data to the TorchScript model in C++ for inference?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile and run the main.cpp code. e.g.,\n",
    "\n",
    "# c++ -std=c++17 main.cpp -I/path/to/libtorch/include -I/path/to/libtorch/include/torch/csrc/api/include \\\n",
    "#    -L/path/to/libtorch/lib -ltorch -lc10 -Wl,-rpath=/path/to/libtorch/lib -o run_model\n",
    "\n",
    "# ./run_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you verify the outputs of the TorchScript model in C++ and compare them to the Python version?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can manually log output values in C++ and cross-check with Python predictions, as included in main.cpp: std::cout << \"Output Tensor: \" << output << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing performance: TorchScript vs. native PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you compare the inference speed of the TorchScript model to the original PyTorch model on the same dataset?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_inference(model, data_loader, label):\n",
    "    model.eval()\n",
    "    total_time = 0\n",
    "    with torch.no_grad():\n",
    "        for images, _ in data_loader:\n",
    "            images = images.to(device)\n",
    "            start = time.time()\n",
    "            _ = model(images)\n",
    "            end = time.time()\n",
    "            total_time += (end - start)\n",
    "    print(f'{label} total inference time: {total_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original PyTorch total inference time: 0.0020 seconds\n",
      "Traced TorchScript total inference time: 0.0017 seconds\n"
     ]
    }
   ],
   "source": [
    "benchmark_inference(model, test_loader, 'Original PyTorch')\n",
    "benchmark_inference(traced_model, test_loader, 'Traced TorchScript')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you measure memory usage during inference for both the TorchScript model and the native PyTorch model?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_memory(label):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    _ = torch.randn(1, 1, 28, 28).to(device)  # warm-up\n",
    "    torch.cuda.synchronize()\n",
    "    before = torch.cuda.memory_allocated(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(torch.randn(64, 1, 28, 28).to(device))  # replace `model` as needed\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    after = torch.cuda.max_memory_allocated(device)\n",
    "    print(f'{label} peak memory usage: {(after - before)/1e6:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original PyTorch peak memory usage: 1.29 MB\n",
      "Traced TorchScript peak memory usage: 1.29 MB\n"
     ]
    }
   ],
   "source": [
    "if device.type == 'cuda':\n",
    "    model.eval()\n",
    "    report_memory('Original PyTorch')\n",
    "\n",
    "    traced_model.eval()\n",
    "    model = traced_model  # reuse function without redefining\n",
    "    report_memory('Traced TorchScript')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you benchmark the performance of both models (TorchScript and PyTorch) in terms of latency and throughput?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latency_throughput(model, batch_size=64, runs=100):\n",
    "    model.eval()\n",
    "    input_sample = torch.randn(batch_size, 1, 28, 28).to(device)\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(runs):\n",
    "            _ = model(input_sample)\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    total_time = end - start\n",
    "    latency = total_time / runs\n",
    "    throughput = batch_size * runs / total_time\n",
    "    return latency, throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original PyTorch — Latency: 1.04 ms, Throughput: 61312.81 samples/s\n",
      "Traced TorchScript — Latency: 0.84 ms, Throughput: 76078.74 samples/s\n"
     ]
    }
   ],
   "source": [
    "if device.type == 'cuda':\n",
    "    l1, t1 = latency_throughput(model)\n",
    "    print(f'Original PyTorch — Latency: {l1*1000:.2f} ms, Throughput: {t1:.2f} samples/s')\n",
    "\n",
    "    l2, t2 = latency_throughput(traced_model)\n",
    "    print(f'Traced TorchScript — Latency: {l2*1000:.2f} ms, Throughput: {t2:.2f} samples/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with optimizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you reduce the model’s precision to optimize performance when exporting with TorchScript?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fellipe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\jit\\_trace.py:685: UserWarning: The input to trace is already a ScriptModule, tracing it is a no-op. Returning the object as is.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "traced_fp16 = torch.jit.trace(model.half(), torch.randn(1, 1, 28, 28).half().to(device))\n",
    "traced_fp16.save('traced_mnist_model_fp16.pt')  # save reduced-precision model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you apply other optimizations, such as pruning or quantization, to improve the efficiency of the TorchScript model?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=128, bias=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils import prune\n",
    "\n",
    "model = SimpleNet().to(device)  # make sure model is fully constructed first\n",
    "model.eval()\n",
    "\n",
    "prune.random_unstructured(model.fc1, name='weight', amount=0.3)  # apply pruning\n",
    "prune.remove(model.fc1, 'weight')  # remove reparam hooks and finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_pruned = torch.jit.trace(model, torch.randn(1, 1, 28, 28).to(device))\n",
    "traced_pruned.save('traced_mnist_model_pruned.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you experiment with different TorchScript backends to analyze performance changes?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backend_inference(model, backend):\n",
    "    model = model.to(backend)\n",
    "    model.eval()\n",
    "\n",
    "    input_dtype = next(model.parameters()).dtype\n",
    "    input_tensor = torch.randn(64, 1, 28, 28).to(backend).to(input_dtype)  # match dtype\n",
    "\n",
    "    if backend.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = model(input_tensor)\n",
    "    if backend.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    print(f'Backend: {backend}  |  Inference time: {(end - start)*1000:.2f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend: cpu  |  Inference time: 0.26 ms\n",
      "Backend: cuda  |  Inference time: 139.42 ms\n"
     ]
    }
   ],
   "source": [
    "backend_inference(traced_model, torch.device('cpu'))\n",
    "if torch.cuda.is_available():\n",
    "    backend_inference(traced_model, torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you combine TorchScript with other optimization techniques to enhance inference speed?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "\n",
    "fused_model = torch.nn.Sequential(\n",
    "    nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(28*28, 128),\n",
    "        nn.ReLU()\n",
    "    ),\n",
    "    nn.Sequential(\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU()\n",
    "    ),\n",
    "    nn.Linear(64, 10)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_model.eval()\n",
    "example_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "traced_fused = torch.jit.trace(fused_model, example_input)\n",
    "traced_fused.save('traced_fused_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "shutil.rmtree('data', ignore_errors=True)\n",
    "\n",
    "for pt_file in Path('.').glob('*.pt'):\n",
    "    pt_file.unlink()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
