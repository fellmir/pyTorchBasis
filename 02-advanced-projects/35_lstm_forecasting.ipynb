{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long short-term memory (LSTM) forecasting in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding LSTM for time-series forecasting](#understanding-lstm-for-time-series-forecasting)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Loading and visualizing time-series data](#loading-and-visualizing-time-series-data)\n",
    "4. [Data preprocessing for LSTM forecasting](#data-preprocessing-for-lstm-forecasting)\n",
    "5. [Building the LSTM model](#building-the-lstm-model)\n",
    "6. [Training the LSTM model](#training-the-lstm-model)\n",
    "7. [Evaluating the LSTM model](#evaluating-the-lstm-model)\n",
    "8. [Tuning hyperparameters and experimenting with different LSTM architectures](#tuning-hyperparameters-and-experimenting-with-different-lstm-architectures)\n",
    "9. [Making predictions with the trained LSTM model](#making-predictions-with-the-trained-lstm-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding LSTM for time-series forecasting\n",
    "\n",
    "Long short-term memory (LSTM) networks are a specialized type of recurrent neural network (RNN) designed to handle sequential data, making them well-suited for time-series forecasting tasks. LSTMs are particularly effective in capturing both short-term and long-term dependencies within sequences, addressing some of the limitations faced by traditional RNNs, especially the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Why use LSTMs for time-series forecasting?**\n",
    "\n",
    "LSTMs are widely used for time-series forecasting because they excel at remembering information over long sequences. Unlike standard RNNs, which struggle to maintain relevant information over extended time steps, LSTMs are designed with a memory mechanism that allows them to selectively remember or forget information as needed. This makes them especially useful for tasks involving time-series data, where both recent and past observations can influence future values.\n",
    "\n",
    "Key reasons LSTMs are effective for time-series forecasting:\n",
    "- **Memory retention**: LSTMs use memory cells that can retain information for long periods, enabling them to capture long-range dependencies in the data.\n",
    "- **Adaptive forgetting**: LSTMs have mechanisms for forgetting irrelevant information, which helps prevent the network from becoming overwhelmed by irrelevant past data points.\n",
    "- **Handling irregular time dependencies**: Time-series data often have irregular dependencies, and LSTMs can adaptively model the importance of each time step.\n",
    "\n",
    "### **The architecture of LSTM networks**\n",
    "\n",
    "LSTMs are composed of repeating units, each containing several internal mechanisms that help manage the flow of information. These units work together to determine which parts of the input sequence are important and should be remembered or forgotten. In forecasting tasks, LSTMs take the input sequence (past time steps) and predict future values based on the learned patterns in the data.\n",
    "\n",
    "The core components of an LSTM unit include:\n",
    "- **Forget gate**: This gate controls how much of the past information should be forgotten or discarded. It helps the model decide which information is no longer relevant for predicting future values.\n",
    "- **Input gate**: This gate determines how much new information from the current input should be stored in the memory cell. It allows the model to incorporate new observations while maintaining relevant information from the past.\n",
    "- **Output gate**: This gate regulates the information passed from the memory cell to the output, deciding what parts of the memory to use for the current prediction.\n",
    "\n",
    "These gates work in tandem to ensure that the LSTM retains useful information from the past and discards unnecessary details, making the network highly effective for time-series forecasting.\n",
    "\n",
    "### **Capturing long-term dependencies**\n",
    "\n",
    "One of the key strengths of LSTMs is their ability to capture long-term dependencies in time-series data. In many forecasting tasks, long-term trends or patterns are critical for making accurate predictions. Traditional RNNs, however, struggle with long sequences because they tend to forget older information. LSTMs address this issue by maintaining a memory cell that can selectively remember information over extended periods.\n",
    "\n",
    "This makes LSTMs particularly well-suited for time-series forecasting tasks where:\n",
    "- There are long-term trends or patterns that extend over many time steps.\n",
    "- The sequence contains complex dependencies between past and future values.\n",
    "- Both recent observations and events from the distant past need to be considered.\n",
    "\n",
    "### **Multi-step forecasting with LSTMs**\n",
    "\n",
    "LSTM models can be used for **multi-step forecasting**, where the goal is to predict multiple future values instead of just the next one. This is common in real-world forecasting tasks where longer time horizons are needed. In this setup, the LSTM model is trained to predict several time steps into the future, either through iterative predictions (one step at a time) or directly predicting multiple steps in a single forward pass.\n",
    "\n",
    "- **Iterative forecasting**: The model predicts one time step ahead, then uses that prediction as input for the next step, continuing this process until the required number of future steps is predicted.\n",
    "- **Direct forecasting**: The model is trained to predict all future time steps at once, reducing potential errors that accumulate when iterating predictions step by step.\n",
    "\n",
    "### **LSTM variations for time-series forecasting**\n",
    "\n",
    "LSTMs can be adapted in different ways to improve performance or handle specific challenges in time-series forecasting. Some of the common variations include:\n",
    "- **Bidirectional LSTMs**: In bidirectional LSTMs, two LSTM layers are used: one processes the sequence from the past to the future, while the other processes it from the future to the past. This can improve performance by allowing the model to understand both past and future context simultaneously.\n",
    "- **Stacked LSTMs**: In stacked LSTMs, multiple layers of LSTMs are used to capture more complex patterns in the data. The output of one LSTM layer becomes the input to the next, allowing the model to learn hierarchical representations of the time-series data.\n",
    "- **Sequence-to-sequence LSTMs**: These LSTMs are used in forecasting tasks where the input sequence and the output sequence have different lengths (e.g., machine translation or forecasting multiple future time steps).\n",
    "\n",
    "### **Handling seasonality and trends with LSTMs**\n",
    "\n",
    "Time-series data often contains seasonality (repeating patterns) and trends (long-term increases or decreases). LSTMs can model these components, but itâ€™s common to enhance the performance of LSTMs by incorporating additional features that explicitly capture these patterns. For example, the input to the LSTM can be augmented with time-based features like:\n",
    "- **Day of the week**\n",
    "- **Month of the year**\n",
    "- **Holiday indicators**\n",
    "\n",
    "These additional features help the LSTM model better recognize and account for seasonal effects or long-term trends in the data, improving the accuracy of the forecasts.\n",
    "\n",
    "### **Applications of LSTM forecasting**\n",
    "\n",
    "LSTMs are used in various time-series forecasting applications, including:\n",
    "- **Financial forecasting**: LSTMs can predict stock prices, market trends, and other financial time-series, capturing both short-term fluctuations and long-term patterns.\n",
    "- **Weather forecasting**: By learning from past weather patterns, LSTMs can be used to forecast temperature, precipitation, and other weather-related variables.\n",
    "- **Energy demand prediction**: LSTMs can model energy consumption patterns to predict future demand, helping utilities manage resources more effectively.\n",
    "- **Sales forecasting**: Businesses use LSTM models to predict future sales based on historical data, identifying seasonal trends and long-term growth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Maths**\n",
    "\n",
    "#### **LSTM architecture**\n",
    "\n",
    "The key idea behind LSTM networks is the use of **memory cells** and **gates** to control the flow of information. Each LSTM unit has three gates:\n",
    "1. **Forget gate**: Decides how much information from the previous cell state should be discarded.\n",
    "2. **Input gate**: Controls how much new information from the current input should be stored in the cell state.\n",
    "3. **Output gate**: Determines how much of the current cell state should be passed on as output.\n",
    "\n",
    "These gates work together to update the cell state and produce the output for each time step.\n",
    "\n",
    "#### **Cell state**\n",
    "\n",
    "The cell state $ c_t $ is the core component of the LSTM, which acts as a memory that runs through the sequence, allowing the network to store long-term information. The cell state is updated at each time step based on the forget and input gates, as well as the current input.\n",
    "\n",
    "The update for the cell state is:\n",
    "\n",
    "$$\n",
    "c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ c_t $ is the cell state at time step $ t $,\n",
    "- $ f_t $ is the forget gate,\n",
    "- $ i_t $ is the input gate,\n",
    "- $ \\tilde{c}_t $ is the candidate cell state (new information),\n",
    "- $ \\odot $ denotes the element-wise multiplication (Hadamard product).\n",
    "\n",
    "The cell state is a linear combination of the previous cell state $ c_{t-1} $ (controlled by the forget gate) and the candidate cell state $ \\tilde{c}_t $ (controlled by the input gate).\n",
    "\n",
    "#### **Forget gate**\n",
    "\n",
    "The forget gate $ f_t $ controls how much of the previous cell state $ c_{t-1} $ is retained. It is computed as:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma(W_f [h_{t-1}, x_t] + b_f)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W_f $ is the weight matrix for the forget gate,\n",
    "- $ h_{t-1} $ is the hidden state from the previous time step,\n",
    "- $ x_t $ is the current input at time step $ t $,\n",
    "- $ b_f $ is the bias term for the forget gate,\n",
    "- $ \\sigma $ is the sigmoid activation function.\n",
    "\n",
    "The forget gate outputs values between 0 and 1, which are used to scale the previous cell state, determining how much information to retain or forget.\n",
    "\n",
    "#### **Input gate**\n",
    "\n",
    "The input gate $ i_t $ controls how much new information from the current input should be added to the cell state. It is computed as:\n",
    "\n",
    "$$\n",
    "i_t = \\sigma(W_i [h_{t-1}, x_t] + b_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W_i $ is the weight matrix for the input gate,\n",
    "- $ h_{t-1} $ is the hidden state from the previous time step,\n",
    "- $ x_t $ is the current input at time step $ t $,\n",
    "- $ b_i $ is the bias term for the input gate.\n",
    "\n",
    "The input gate works alongside the candidate cell state $ \\tilde{c}_t $ to determine how much new information should be added to the cell state.\n",
    "\n",
    "#### **Candidate cell state**\n",
    "\n",
    "The candidate cell state $ \\tilde{c}_t $ is the new information that the input gate considers adding to the cell state. It is computed using the following equation:\n",
    "\n",
    "$$\n",
    "\\tilde{c}_t = \\tanh(W_c [h_{t-1}, x_t] + b_c)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W_c $ is the weight matrix for the candidate cell state,\n",
    "- $ b_c $ is the bias term for the candidate cell state,\n",
    "- $ \\tanh $ is the hyperbolic tangent activation function.\n",
    "\n",
    "The candidate cell state represents the new information that could potentially be added to the cell state, but the input gate determines how much of it is actually added.\n",
    "\n",
    "#### **Output gate**\n",
    "\n",
    "The output gate $ o_t $ determines how much of the current cell state should be output as the hidden state for the next time step. It is computed as:\n",
    "\n",
    "$$\n",
    "o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W_o $ is the weight matrix for the output gate,\n",
    "- $ b_o $ is the bias term for the output gate.\n",
    "\n",
    "The output gate controls the flow of information from the cell state to the hidden state $ h_t $.\n",
    "\n",
    "#### **Hidden state**\n",
    "\n",
    "The hidden state $ h_t $ is the output of the LSTM at each time step and is computed based on the current cell state and the output gate:\n",
    "\n",
    "$$\n",
    "h_t = o_t \\odot \\tanh(c_t)\n",
    "$$\n",
    "\n",
    "The hidden state incorporates the cell state, but the output gate controls how much of the cell state contributes to the hidden state.\n",
    "\n",
    "#### **Putting it all together**\n",
    "\n",
    "The full update equations for an LSTM at time step $ t $ are as follows:\n",
    "\n",
    "1. Compute the forget gate:\n",
    "$$\n",
    "f_t = \\sigma(W_f [h_{t-1}, x_t] + b_f)\n",
    "$$\n",
    "\n",
    "2. Compute the input gate:\n",
    "$$\n",
    "i_t = \\sigma(W_i [h_{t-1}, x_t] + b_i)\n",
    "$$\n",
    "\n",
    "3. Compute the candidate cell state:\n",
    "$$\n",
    "\\tilde{c}_t = \\tanh(W_c [h_{t-1}, x_t] + b_c)\n",
    "$$\n",
    "\n",
    "4. Update the cell state:\n",
    "$$\n",
    "c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
    "$$\n",
    "\n",
    "5. Compute the output gate:\n",
    "$$\n",
    "o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o)\n",
    "$$\n",
    "\n",
    "6. Compute the hidden state:\n",
    "$$\n",
    "h_t = o_t \\odot \\tanh(c_t)\n",
    "$$\n",
    "\n",
    "These equations describe how an LSTM processes input at each time step, deciding what to remember, what to forget, and how to use the information to generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries such as PyTorch, `pandas`, and `matplotlib` for building and training an LSTM model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required PyTorch modules for building and training an LSTM model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you set up GPU support in PyTorch to accelerate LSTM training?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and visualizing time-series data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you load a time-series dataset using `pandas` in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you visualize the time-series data to identify trends and seasonality using `matplotlib` or `seaborn`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you split the time-series dataset into training and test sets for model evaluation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing for LSTM forecasting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you scale or normalize time-series data to improve model performance?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you create sliding windows of input sequences and corresponding target values from the time-series data?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you reshape time-series data into the required format for LSTM models?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you create a PyTorch `DataLoader` to batch the preprocessed time-series data for LSTM training?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the LSTM model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you define an LSTM model in PyTorch using `torch.nn.LSTM` for time-series forecasting?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you implement the forward pass of the LSTM model, where the input is a sequence of time-series data and the output is a predicted value?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you add fully connected layers after the LSTM to transform the hidden states into predictions?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you include dropout in the LSTM model for regularization?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LSTM model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you define the loss function and optimizer for training the LSTM model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you implement the training loop, including the forward pass, loss calculation, and backpropagation for the LSTM model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you log and track the training loss over epochs to monitor the modelâ€™s performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the LSTM model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you evaluate the LSTM model on the test set by calculating metrics such as Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE)?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you visualize the predicted vs. actual values for the test set to assess the modelâ€™s forecasting accuracy?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you compare the LSTM modelâ€™s performance to baseline models such as moving average or naive forecasting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters and experimenting with different LSTM architectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you experiment with different LSTM architectures, such as increasing the number of hidden units or adding more LSTM layers?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you tune hyperparameters to improve LSTM model performance?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you experiment with adding more fully connected layers after the LSTM to improve forecasting accuracy?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you test different optimizers to observe their impact on the LSTM training performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with the trained LSTM model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you use the trained LSTM model to make future predictions on unseen time-series data?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you implement multi-step forecasting, where the LSTM model predicts several future time steps based on the previous sequence?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q27: How do you visualize the LSTM modelâ€™s future predictions alongside the actual future values to evaluate its accuracy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
