{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer models in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Understanding transformer models](#understanding-transformer-models)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Defining the input data](#defining-the-input-data)\n",
    "4. [Implementing positional encoding](#implementing-positional-encoding)\n",
    "5. [Building the scaled dot-product attention mechanism](#building-the-scaled-dot-product-attention-mechanism)\n",
    "6. [Implementing multi-head attention](#implementing-multi-head-attention)\n",
    "7. [Building the feed-forward network](#building-the-feed-forward-network)\n",
    "8. [Constructing the transformer encoder](#constructing-the-transformer-encoder)\n",
    "9. [Training the transformer model](#training-the-transformer-model)\n",
    "10. [Evaluating the transformer model](#evaluating-the-transformer-model)\n",
    "11. [Experimenting with different transformer configurations](#experimenting-with-different-transformer-configurations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Key concepts**\n",
    "Transformer models are a class of deep learning architectures that leverage self-attention mechanisms to process sequential data. They excel in tasks like natural language processing (NLP), computer vision, and beyond, by capturing long-range dependencies efficiently. Unlike recurrent or convolutional networks, Transformers process entire sequences in parallel, enabling faster computation and improved scalability.\n",
    "\n",
    "Key features of Transformers include:\n",
    "- **Self-attention mechanism**: Dynamically focuses on relevant parts of the input sequence for each token, capturing global context.\n",
    "- **Multi-head attention**: Uses multiple attention heads in parallel to learn diverse relationships within the data.\n",
    "- **Positional encoding**: Provides order information to the sequence, as Transformers process tokens without inherent order.\n",
    "- **Feedforward layers**: Independently apply transformations to each token after self-attention, adding depth and expressiveness.\n",
    "- **Layer normalization and residual connections**: Improve stability and efficiency during training.\n",
    "\n",
    "In PyTorch, the `torch.nn.Transformer` module offers a flexible framework for building and training Transformer-based models.\n",
    "\n",
    "### **Applications**\n",
    "Transformer models are widely applied in tasks requiring sequence understanding and context modeling:\n",
    "- **Natural language processing (NLP)**: Machine translation, text summarization, sentiment analysis, and question answering.\n",
    "- **Computer vision**: Tasks like image classification, object detection, and image segmentation with architectures such as Vision Transformers (ViT).\n",
    "- **Speech processing**: Speech-to-text, text-to-speech, and audio feature extraction tasks.\n",
    "- **Time-series forecasting**: Modeling dependencies in long-term temporal data for predictive analytics.\n",
    "\n",
    "### **Advantages**\n",
    "- **Parallel processing**: Processes sequences as a whole, significantly speeding up training compared to recurrent models.\n",
    "- **Scalability**: Handles large datasets and model sizes effectively, achieving state-of-the-art performance.\n",
    "- **Versatility**: Adapts to a wide range of tasks and data modalities, from text to vision.\n",
    "- **Global context**: Captures long-range dependencies without the need for sequential processing.\n",
    "\n",
    "### **Challenges**\n",
    "- **Computational intensity**: Requires substantial memory and compute resources, especially for long sequences.\n",
    "- **Data dependency**: Performs best when trained on large, high-quality datasets.\n",
    "- **Interpretability**: The complexity of attention mechanisms can make model decisions less transparent.\n",
    "- **Training difficulty**: Hyperparameter tuning and model design can be intricate and resource-intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for building and training transformer models in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# !pip install matplotlib\n",
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required PyTorch modules to construct attention mechanisms and build transformer models?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you configure the environment to use GPU support for training transformer models in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the input data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you define input sequences to feed into the transformer model?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = torch.tensor([\n",
    "    [4, 7, 2, 9, 0],\n",
    "    [5, 6, 1, 0, 0]\n",
    "], dtype=torch.long).to(device)  # move to gpu if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you preprocess the input data and convert it into embeddings for the transformer model?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10\n",
    "embedding_dim = 32\n",
    "\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_inputs = embedding_layer(input_sequences)  # shape: (batch_size, seq_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you pad input sequences to ensure consistent lengths before feeding them into the transformer model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# example tokenized input sequences (varying lengths)\n",
    "sequences = [\n",
    "    torch.tensor([4, 7, 2, 9], dtype=torch.long),\n",
    "    torch.tensor([5, 6, 1], dtype=torch.long)\n",
    "]\n",
    "\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0).to(device)  # pad with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing positional encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you implement sinusoidal positional encoding in PyTorch to represent the order of tokens in sequences?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # positional encoding table\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # shape: (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # decay rates\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices\n",
    "\n",
    "        self.pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)].to(x.device)  # add positional encoding up to sequence length\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you add positional encodings to the input embeddings for the transformer model?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoder = PositionalEncoding(d_model=embedding_dim).to(device)\n",
    "\n",
    "encoded_inputs = pos_encoder(embedded_inputs)  # shape: (batch_size, seq_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you verify that the positional encoding has been correctly added to the input data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded inputs (first token): tensor([ 0.9781, -0.1581, -0.6428, -0.2876,  0.3417,  1.7808, -1.3230, -1.2824,\n",
      "         0.8206, -0.8285, -0.3401,  0.5750, -0.9450, -0.6592, -0.4777,  0.1327,\n",
      "         1.1768, -0.1969,  0.2404, -0.3387, -0.4212,  0.1310, -0.3772,  1.2905,\n",
      "        -1.2184, -1.3354, -0.8109, -1.4791,  0.6834,  0.3014, -0.1476, -0.6980],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Positional encoded (first token): tensor([ 0.9781,  0.8419, -0.6428,  0.7124,  0.3417,  2.7808, -1.3230, -0.2824,\n",
      "         0.8206,  0.1715, -0.3401,  1.5750, -0.9450,  0.3408, -0.4777,  1.1327,\n",
      "         1.1768,  0.8031,  0.2404,  0.6613, -0.4212,  1.1310, -0.3772,  2.2905,\n",
      "        -1.2184, -0.3354, -0.8109, -0.4791,  0.6834,  1.3014, -0.1476,  0.3020],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Difference (added position info): tensor([0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "        1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000,\n",
      "        0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "        1.0000, 0.0000, 1.0000, 0.0000, 1.0000], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedded inputs (first token):\", embedded_inputs[0, 0])  # before adding positional encoding\n",
    "print(\"Positional encoded (first token):\", encoded_inputs[0, 0])  # after adding positional encoding\n",
    "\n",
    "difference = encoded_inputs[0, 0] - embedded_inputs[0, 0]\n",
    "print(\"Difference (added position info):\", difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the scaled dot-product attention mechanism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you implement the scaled dot-product attention mechanism in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)  # scaled dot-product\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))  # apply mask\n",
    "\n",
    "    attn_weights = F.softmax(scores, dim=-1)  # normalize\n",
    "    output = torch.matmul(attn_weights, value)  # weighted sum\n",
    "\n",
    "    return output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you compute attention scores by calculating the dot product of query and key matrices?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_k = 32\n",
    "\n",
    "query = torch.rand(batch_size, seq_len, d_k).to(device)\n",
    "key = torch.rand(batch_size, seq_len, d_k).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw attention scores shape: torch.Size([2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# compute raw attention scores (no scaling yet)\n",
    "raw_scores = torch.matmul(query, key.transpose(-2, -1))  # shape: (batch_size, seq_len, seq_len)\n",
    "print(\"Raw attention scores shape:\", raw_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you apply softmax to the attention scores and multiply them by the value matrix to get the final output?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output shape: torch.Size([2, 5, 32])\n"
     ]
    }
   ],
   "source": [
    "value = torch.rand(batch_size, seq_len, d_k).to(device)\n",
    "\n",
    "scaled_scores = raw_scores / math.sqrt(d_k)  # scale by sqrt(d_k)\n",
    "attn_weights = F.softmax(scaled_scores, dim=-1)  # apply softmax\n",
    "attn_output = torch.matmul(attn_weights, value)  # attention output\n",
    "\n",
    "print(\"Attention output shape:\", attn_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing multi-head attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you implement multi-head attention by splitting input sequences into multiple attention heads in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0  # ensure divisibility\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # linear projections\n",
    "        q = self.q_linear(x)  # shape: (batch_size, seq_len, embed_dim)\n",
    "        k = self.k_linear(x)\n",
    "        v = self.v_linear(x)\n",
    "\n",
    "        # reshape for multi-head attention\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        return q, k, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you apply the scaled dot-product attention mechanism separately for each head in the multi-head attention?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    d_k = q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, v)  # shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "    return output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you concatenate the outputs of the multiple attention heads and apply a linear projection?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        q = self.q_linear(x)\n",
    "        k = self.k_linear(x)\n",
    "        v = self.v_linear(x)\n",
    "\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        # concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "\n",
    "        # final linear projection\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the feed-forward network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you implement the position-wise feed-forward network using `torch.nn.Linear` layers?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))  # two-layer transformation with ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you apply activation functions after the linear layers in the feed-forward network?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "# already applied ReLU in Q16 within the forward method of PositionwiseFeedForward\n",
    "# for illustration:\n",
    "x = torch.rand(2, 5, 64).to(device)  # dummy input: (batch_size, seq_len, embed_dim)\n",
    "ffn = PositionwiseFeedForward(embed_dim=64, ff_dim=256).to(device)\n",
    "\n",
    "output = ffn(x)  # applies linear1 -> ReLU -> linear2\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you add dropout and layer normalization to the feed-forward network for regularization?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.layer_norm(x + residual)  # apply residual and normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the transformer encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you combine multi-head attention and the feed-forward network to construct a transformer encoder layer?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.feed_forward = PositionwiseFeedForward(embed_dim, ff_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output, _ = self.self_attn(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))  # residual connection after attention\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))  # residual connection after feed-forward\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you implement residual connections and layer normalization around the attention and feed-forward layers in the transformer encoder?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already applied in TransformerEncoderLayer forward method from Q19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you stack multiple transformer encoder layers to create a deep transformer model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the transformer model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you define the loss function for a sequence-based task in the transformer model?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore padding index during loss computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you set up the optimizer to update the transformer model’s parameters during training?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerEncoder(num_layers=4, embed_dim=64, num_heads=4, ff_dim=256).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you implement the training loop, including forward pass, loss calculation, and backpropagation for the transformer model?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputProjection(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)  # project to vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(num_embeddings=10, embedding_dim=64).to(device)\n",
    "pos_encoder = PositionalEncoding(d_model=64).to(device)\n",
    "projection_layer = OutputProjection(embed_dim=64, vocab_size=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, projection_layer, optimizer, criterion, inputs, targets, mask=None):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    x = embedding_layer(inputs)  # embed token indices\n",
    "    x = pos_encoder(x)  # add positional encoding\n",
    "\n",
    "    encoded = model(x, mask)  # pass through encoder\n",
    "    logits = projection_layer(encoded)  # project to vocab size\n",
    "\n",
    "    loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))  # flatten for loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you track and log the training loss and accuracy over multiple epochs when training the transformer model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss: 2.6124  Accuracy: 0.0000\n",
      "Epoch 2  Loss: 2.2403  Accuracy: 0.2000\n",
      "Epoch 3  Loss: 2.0370  Accuracy: 0.2000\n",
      "Epoch 4  Loss: 2.1122  Accuracy: 0.2000\n",
      "Epoch 5  Loss: 1.9199  Accuracy: 0.2000\n",
      "Epoch 6  Loss: 1.9140  Accuracy: 0.2000\n",
      "Epoch 7  Loss: 1.7676  Accuracy: 0.4000\n",
      "Epoch 8  Loss: 1.6318  Accuracy: 0.4000\n",
      "Epoch 9  Loss: 1.5428  Accuracy: 0.6000\n",
      "Epoch 10  Loss: 1.4302  Accuracy: 0.6000\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# sample input data\n",
    "x_batch = torch.tensor([\n",
    "    [4, 7, 2, 9, 0],\n",
    "    [5, 6, 1, 0, 0]\n",
    "], dtype=torch.long)\n",
    "\n",
    "y_batch = torch.tensor([\n",
    "    [7, 2, 9, 0, 0],\n",
    "    [6, 1, 0, 0, 0]\n",
    "], dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(x_batch, y_batch)\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for inputs, targets in data_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        loss, logits = train_step(model, projection_layer, optimizer, criterion, inputs, targets)\n",
    "\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        mask = targets != 0\n",
    "        correct = (predictions[mask] == targets[mask]).sum().item()\n",
    "        total = mask.sum().item()\n",
    "\n",
    "        total_correct += correct\n",
    "        total_tokens += total\n",
    "        total_loss += loss\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = total_correct / total_tokens\n",
    "\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch}  Loss: {avg_loss:.4f}  Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the transformer model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you evaluate the transformer model on a validation or test dataset to calculate performance metrics?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "total_tokens = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for inputs, targets in data_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        x = embedding_layer(inputs)  # embed tokens\n",
    "        x = pos_encoder(x)  # add positional encoding\n",
    "        encoded = model(x)  # forward pass through transformer\n",
    "        logits = projection_layer(encoded)  # project to vocab size\n",
    "\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))  # compute loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        predictions = logits.argmax(dim=-1)  # predict tokens\n",
    "        mask = targets != 0  # exclude padding\n",
    "        correct = (predictions[mask] == targets[mask]).sum().item()  # count correct\n",
    "        total = mask.sum().item()  # count total valid tokens\n",
    "\n",
    "        total_correct += correct\n",
    "        total_tokens += total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Evaluation — Loss: 1.3342  Accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "avg_loss = total_loss / len(data_loader)\n",
    "accuracy = total_correct / total_tokens\n",
    "\n",
    "print(f\"Transformer Evaluation — Loss: {avg_loss:.4f}  Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q27: How do you compute metrics such as accuracy or perplexity to evaluate the transformer’s performance?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(loss):\n",
    "    return math.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Evaluation — Perplexity: 3.80\n"
     ]
    }
   ],
   "source": [
    "perplexity = compute_perplexity(avg_loss)\n",
    "\n",
    "print(f\"Transformer Evaluation — Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q28: How do you compare the transformer model's performance to other baseline models, such as LSTMs or RNNs?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBaseline(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        output, _ = self.rnn(x)\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNNBaseline(vocab_size=10, embed_dim=64, hidden_dim=64).to(device)\n",
    "rnn_optimizer = torch.optim.Adam(rnn_model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Epoch 1  Loss: 2.3661  Accuracy: 0.0000  Perplexity: 10.66\n",
      "RNN Epoch 2  Loss: 2.3424  Accuracy: 0.0000  Perplexity: 10.41\n",
      "RNN Epoch 3  Loss: 2.3188  Accuracy: 0.0000  Perplexity: 10.16\n",
      "RNN Epoch 4  Loss: 2.2953  Accuracy: 0.0000  Perplexity: 9.93\n",
      "RNN Epoch 5  Loss: 2.2719  Accuracy: 0.2000  Perplexity: 9.70\n",
      "RNN Epoch 6  Loss: 2.2486  Accuracy: 0.2000  Perplexity: 9.47\n",
      "RNN Epoch 7  Loss: 2.2254  Accuracy: 0.2000  Perplexity: 9.26\n",
      "RNN Epoch 8  Loss: 2.2023  Accuracy: 0.2000  Perplexity: 9.05\n",
      "RNN Epoch 9  Loss: 2.1793  Accuracy: 0.2000  Perplexity: 8.84\n",
      "RNN Epoch 10  Loss: 2.1563  Accuracy: 0.2000  Perplexity: 8.64\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    rnn_model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for inputs, targets in data_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        rnn_optimizer.zero_grad()\n",
    "        logits = rnn_model(inputs)\n",
    "\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        loss.backward()\n",
    "        rnn_optimizer.step()\n",
    "\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        mask = targets != 0\n",
    "        correct = (predictions[mask] == targets[mask]).sum().item()\n",
    "        total = mask.sum().item()\n",
    "\n",
    "        total_correct += correct\n",
    "        total_tokens += total\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss_rnn = total_loss / len(data_loader)\n",
    "    accuracy_rnn = total_correct / total_tokens\n",
    "    perplexity_rnn = compute_perplexity(avg_loss_rnn)\n",
    "\n",
    "    print(f\"RNN Epoch {epoch}  Loss: {avg_loss_rnn:.4f}  Accuracy: {accuracy_rnn:.4f}  Perplexity: {perplexity_rnn:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBaseline(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        output, _ = self.lstm(x)\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTMBaseline(vocab_size=10, embed_dim=64, hidden_dim=64).to(device)\n",
    "lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=3e-4)\n",
    "lstm_criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Epoch 1  Loss: 2.2869  Accuracy: 0.2000  Perplexity: 9.84\n",
      "LSTM Epoch 2  Loss: 2.2723  Accuracy: 0.2000  Perplexity: 9.70\n",
      "LSTM Epoch 3  Loss: 2.2578  Accuracy: 0.2000  Perplexity: 9.56\n",
      "LSTM Epoch 4  Loss: 2.2432  Accuracy: 0.2000  Perplexity: 9.42\n",
      "LSTM Epoch 5  Loss: 2.2288  Accuracy: 0.2000  Perplexity: 9.29\n",
      "LSTM Epoch 6  Loss: 2.2143  Accuracy: 0.2000  Perplexity: 9.15\n",
      "LSTM Epoch 7  Loss: 2.1998  Accuracy: 0.2000  Perplexity: 9.02\n",
      "LSTM Epoch 8  Loss: 2.1854  Accuracy: 0.2000  Perplexity: 8.89\n",
      "LSTM Epoch 9  Loss: 2.1709  Accuracy: 0.2000  Perplexity: 8.77\n",
      "LSTM Epoch 10  Loss: 2.1564  Accuracy: 0.4000  Perplexity: 8.64\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    lstm_model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for inputs, targets in data_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        lstm_optimizer.zero_grad()\n",
    "        logits = lstm_model(inputs)\n",
    "\n",
    "        loss = lstm_criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        loss.backward()\n",
    "        lstm_optimizer.step()\n",
    "\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        mask = targets != 0\n",
    "        correct = (predictions[mask] == targets[mask]).sum().item()\n",
    "        total = mask.sum().item()\n",
    "\n",
    "        total_correct += correct\n",
    "        total_tokens += total\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss_lstm = total_loss / len(data_loader)\n",
    "    accuracy_lstm = total_correct / total_tokens\n",
    "    perplexity_lstm = compute_perplexity(avg_loss_lstm)\n",
    "\n",
    "    print(f\"LSTM Epoch {epoch}  Loss: {avg_loss_lstm:.4f}  Accuracy: {accuracy_lstm:.4f}  Perplexity: {perplexity_lstm:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with different transformer configurations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q29: How do you experiment with different numbers of layers and attention heads in the transformer model to observe their effect on performance?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small = TransformerEncoder(num_layers=2, embed_dim=64, num_heads=2, ff_dim=256).to(device)\n",
    "model_large = TransformerEncoder(num_layers=6, embed_dim=64, num_heads=8, ff_dim=256).to(device)\n",
    "\n",
    "projection_layer_small = OutputProjection(embed_dim=64, vocab_size=10).to(device)\n",
    "projection_layer_large = OutputProjection(embed_dim=64, vocab_size=10).to(device)\n",
    "\n",
    "optimizer_small = torch.optim.Adam(model_small.parameters(), lr=3e-4)\n",
    "optimizer_large = torch.optim.Adam(model_large.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small Transformer Loss: 2.8785\n",
      "Large Transformer Loss: 2.7999\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = next(iter(data_loader))\n",
    "inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "x_small = pos_encoder(embedding_layer(inputs))\n",
    "x_large = pos_encoder(embedding_layer(inputs))\n",
    "\n",
    "logits_small = projection_layer_small(model_small(x_small))\n",
    "logits_large = projection_layer_large(model_large(x_large))\n",
    "\n",
    "loss_small = criterion(logits_small.view(-1, logits_small.size(-1)), targets.view(-1))\n",
    "loss_large = criterion(logits_large.view(-1, logits_large.size(-1)), targets.view(-1))\n",
    "\n",
    "print(f\"Small Transformer Loss: {loss_small.item():.4f}\")\n",
    "print(f\"Large Transformer Loss: {loss_large.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q30: How do you adjust the hidden dimension size of the transformer and analyze its impact on training time and accuracy?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model_dim32 = TransformerEncoder(num_layers=4, embed_dim=32, num_heads=4, ff_dim=128).to(device)\n",
    "model_dim128 = TransformerEncoder(num_layers=4, embed_dim=128, num_heads=8, ff_dim=512).to(device)\n",
    "\n",
    "embed32 = nn.Embedding(10, 32).to(device)\n",
    "embed128 = nn.Embedding(10, 128).to(device)\n",
    "proj32 = OutputProjection(32, 10).to(device)\n",
    "proj128 = OutputProjection(128, 10).to(device)\n",
    "pos_encoder_32 = PositionalEncoding(d_model=32).to(device)\n",
    "pos_encoder_128 = PositionalEncoding(d_model=128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim 32 — Loss: 2.4019  Time: 0.0061s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "x = pos_encoder_32(embed32(inputs))\n",
    "logits = proj32(model_dim32(x))\n",
    "loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "end = time.time()\n",
    "print(f\"Dim 32 — Loss: {loss.item():.4f}  Time: {end - start:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim 128 — Loss: 2.7081  Time: 0.0061s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "x = pos_encoder_128(embed128(inputs))\n",
    "logits = proj128(model_dim128(x))\n",
    "loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "end = time.time()\n",
    "print(f\"Dim 128 — Loss: {loss.item():.4f}  Time: {end - start:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q31: How do you experiment with different learning rates and dropout rates to optimize the transformer’s generalization and performance?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR=1e-4, Dropout=0.3 — Loss: 2.3417\n",
      "LR=1e-2, Dropout=0.0 — Loss: 2.2674\n"
     ]
    }
   ],
   "source": [
    "model_lowlr_highdrop = TransformerEncoder(num_layers=4, embed_dim=64, num_heads=4, ff_dim=256, dropout=0.3).to(device)\n",
    "model_highlr_nodrop = TransformerEncoder(num_layers=4, embed_dim=64, num_heads=4, ff_dim=256, dropout=0.0).to(device)\n",
    "\n",
    "proj_lowlr_highdrop = OutputProjection(64, 10).to(device)\n",
    "proj_highlr_nodrop = OutputProjection(64, 10).to(device)\n",
    "\n",
    "optimizer_lowlr_highdrop = torch.optim.Adam(model_lowlr_highdrop.parameters(), lr=1e-4)\n",
    "optimizer_highlr_nodrop = torch.optim.Adam(model_highlr_nodrop.parameters(), lr=1e-2)\n",
    "\n",
    "inputs, targets = next(iter(data_loader))\n",
    "inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "x_low = pos_encoder(embedding_layer(inputs))\n",
    "x_high = pos_encoder(embedding_layer(inputs))\n",
    "\n",
    "logits_low = proj_lowlr_highdrop(model_lowlr_highdrop(x_low))\n",
    "logits_high = proj_highlr_nodrop(model_highlr_nodrop(x_high))\n",
    "\n",
    "loss_low = criterion(logits_low.view(-1, logits_low.size(-1)), targets.view(-1))\n",
    "loss_high = criterion(logits_high.view(-1, logits_high.size(-1)), targets.view(-1))\n",
    "\n",
    "print(f\"LR=1e-4, Dropout=0.3 — Loss: {loss_low.item():.4f}\")\n",
    "print(f\"LR=1e-2, Dropout=0.0 — Loss: {loss_high.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q32: How do you analyze how the transformer model performs on different tasks by varying the input data and sequence lengths?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short Sequence — Loss: 1.2265\n",
      "Long Sequence  — Loss: 2.4804\n"
     ]
    }
   ],
   "source": [
    "short_seq = torch.tensor([[4, 7, 2]], dtype=torch.long).to(device)  # short sequence (length 3)\n",
    "long_seq = torch.tensor([[5, 6, 1, 2, 3, 7, 9, 4, 0, 0]], dtype=torch.long).to(device)  # long sequence (length 10)\n",
    "\n",
    "target_short = torch.tensor([[7, 2, 0]], dtype=torch.long).to(device)\n",
    "target_long = torch.tensor([[6, 1, 2, 3, 7, 9, 4, 0, 0, 0]], dtype=torch.long).to(device)\n",
    "\n",
    "embed_short = embedding_layer(short_seq)\n",
    "embed_long = embedding_layer(long_seq)\n",
    "\n",
    "pos_short = pos_encoder(embed_short)\n",
    "pos_long = pos_encoder(embed_long)\n",
    "\n",
    "encoded_short = model(pos_short)\n",
    "encoded_long = model(pos_long)\n",
    "\n",
    "logits_short = projection_layer(encoded_short)\n",
    "logits_long = projection_layer(encoded_long)\n",
    "\n",
    "loss_short = criterion(logits_short.view(-1, logits_short.size(-1)), target_short.view(-1))\n",
    "loss_long = criterion(logits_long.view(-1, logits_long.size(-1)), target_long.view(-1))\n",
    "\n",
    "print(f\"Short Sequence — Loss: {loss_short.item():.4f}\")\n",
    "print(f\"Long Sequence  — Loss: {loss_long.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
