{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom layers in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding custom layers](#understanding-custom-layers)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Creating a basic custom layer](#creating-a-basic-custom-layer)\n",
    "4. [Implementing parameterized custom layers](#implementing-parameterized-custom-layers)\n",
    "5. [Applying custom layers in simple networks](#applying-custom-layers-in-simple-networks)\n",
    "6. [Building a custom activation layer](#building-a-custom-activation-layer)\n",
    "7. [Building a custom normalization layer](#building-a-custom-normalization-layer)\n",
    "8. [Testing and validating custom layers](#testing-and-validating-custom-layers)\n",
    "9. [Experimenting with custom layers](#experimenting-with-custom-layers)\n",
    "10. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding custom layers\n",
    "\n",
    "In PyTorch, building custom layers allows for greater flexibility in neural network design, enabling you to create specialized architectures tailored to specific tasks or to experiment with novel operations beyond the standard layers like convolutions or fully connected layers. By defining your own layers, you can introduce unique transformations, apply custom logic, or create entirely new types of neural networks that may not be available through existing modules.\n",
    "\n",
    "Custom layers in PyTorch are created by subclassing `torch.nn.Module`, which is the base class for all neural network layers in PyTorch. This class provides the core functionalities needed to define the layer’s parameters and forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Why create custom layers?**\n",
    "\n",
    "Creating custom layers offers several advantages, particularly when dealing with complex architectures or specific requirements:\n",
    "- **Specialized transformations**: Custom layers allow you to define operations that are not available as standard PyTorch layers. This is useful for research purposes or when implementing novel architectures.\n",
    "- **Flexibility**: You can create layers that behave differently during training and inference, or layers that change their behavior based on input data.\n",
    "- **Experimentation**: Custom layers enable experimentation with new layer designs, allowing you to test alternative approaches to improving model performance.\n",
    "\n",
    "### **Basic structure of a custom layer**\n",
    "\n",
    "To create a custom layer in PyTorch, you need to subclass `nn.Module` and define two key components:\n",
    "- **Initialization (`__init__`)**: This is where you define the parameters or learnable weights of the layer.\n",
    "- **Forward pass (`forward`)**: This defines how the input is transformed by the layer. This function performs the computations necessary to map the input to the output.\n",
    "\n",
    "#### **Initialization (`__init__`)**\n",
    "\n",
    "In the `__init__` method, you define any parameters, constants, or layers that your custom layer will need. These parameters will be registered as part of the layer, and PyTorch will automatically handle them during backpropagation, including calculating gradients and updating the weights.\n",
    "\n",
    "#### **Forward pass (`forward`)**\n",
    "\n",
    "The `forward` method defines how the layer processes its input. This is where you implement the logic of the layer, such as matrix multiplications, nonlinear activations, or any other operations that the layer should perform.\n",
    "\n",
    "### **Creating custom layers using PyTorch operations**\n",
    "\n",
    "Custom layers often involve standard PyTorch operations such as matrix multiplication, element-wise functions, or predefined layers like `nn.Linear`. You can mix these operations in the `forward` method to create complex transformations.\n",
    "\n",
    "For instance, if you're designing a layer that applies a sequence of transformations, you could:\n",
    "1. Apply a linear transformation (like a fully connected layer).\n",
    "2. Apply a non-linearity (like ReLU or sigmoid).\n",
    "3. Perform a custom operation, such as normalizing the output or adding a bias in a non-standard way.\n",
    "\n",
    "### **Examples of custom layers**\n",
    "\n",
    "#### **Custom linear layer with weight normalization**\n",
    "\n",
    "A custom linear layer may involve learning a weight matrix and applying weight normalization, which ensures that the weights of the layer have a unit norm, helping to improve training stability and performance.\n",
    "\n",
    "1. **Initialization**: In the `__init__` method, you would define the weight matrix and bias for the linear layer, as well as any normalization parameters.\n",
    "2. **Forward pass**: In the `forward` method, you would apply the linear transformation followed by weight normalization and any activation functions you wish to use.\n",
    "\n",
    "#### **Custom layer with attention mechanism**\n",
    "\n",
    "Attention mechanisms are common in natural language processing and other tasks where the model needs to focus on certain parts of the input. A custom layer implementing attention could involve:\n",
    "1. Calculating attention scores (via dot product or another method).\n",
    "2. Normalizing these scores (using softmax).\n",
    "3. Applying the attention weights to the input data.\n",
    "\n",
    "This type of custom layer might involve both learned parameters (for the attention calculation) and operations like softmax for normalizing the scores.\n",
    "\n",
    "### **Using custom layers in neural networks**\n",
    "\n",
    "Once you have defined a custom layer, you can use it just like any other PyTorch layer. This involves:\n",
    "- Instantiating the custom layer as part of your model's initialization.\n",
    "- Calling the custom layer in the model’s `forward` method, allowing it to be part of the overall computation graph.\n",
    "- PyTorch automatically handles gradient calculation and weight updates for custom layers, so there is no need to manually define backpropagation.\n",
    "\n",
    "### **Parameter management in custom layers**\n",
    "\n",
    "When you define parameters in a custom layer (such as weight matrices or biases), PyTorch registers these as part of the layer. PyTorch’s automatic differentiation engine will track all operations on these parameters, and during training, it will compute gradients and update the parameters based on the optimizer used.\n",
    "\n",
    "By defining parameters using `torch.nn.Parameter`, you can create learnable parameters that PyTorch optimizes during training. Any tensors defined as `Parameter` will be considered part of the layer’s parameters and can be accessed via the `parameters()` method of the model.\n",
    "\n",
    "### **Common use cases for custom layers**\n",
    "\n",
    "- **Novel architectures**: Custom layers are often used in research or experimental models where new types of layers need to be introduced.\n",
    "- **Specialized data transformations**: In some cases, the data needs to undergo specific transformations that are not covered by standard layers (e.g., for tasks in scientific computing or when processing time-series data).\n",
    "- **Custom regularization**: Custom layers can include regularization techniques such as dropout or weight normalization that are applied in a unique way, or that differ from standard approaches.\n",
    "\n",
    "### **Advantages of creating custom layers**\n",
    "\n",
    "- **Flexibility**: Allows for the design of any layer type, which can be important when building innovative architectures or fine-tuning a model for a specific task.\n",
    "- **Modularity**: Custom layers can be reused across different models, making it easier to organize code and experiment with different configurations.\n",
    "- **Optimized learning**: By customizing layers, you can optimize the learning process, either by introducing novel regularization techniques or by designing layers that are more efficient for specific data types.\n",
    "\n",
    "### **Challenges of custom layers**\n",
    "\n",
    "While custom layers offer flexibility, they also present some challenges:\n",
    "- **Complexity**: Custom layers can add complexity to your model, making it harder to debug and optimize.\n",
    "- **Performance**: Depending on the operations used, custom layers may introduce computational overhead, especially if they involve non-standard operations or require significant memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Maths**\n",
    "\n",
    "#### **Understanding layers as functions**\n",
    "\n",
    "In deep learning, layers can be mathematically understood as functions that map input vectors (or tensors) to output vectors. Each layer applies some transformation to the input, often involving learned parameters like weights and biases. In PyTorch, custom layers are functions that define how inputs are processed during the forward pass and how parameters are updated during backpropagation.\n",
    "\n",
    "A typical layer, such as a fully connected layer, can be described mathematically as a linear transformation followed by a non-linear activation function:\n",
    "\n",
    "$$\n",
    "y = f(Wx + b)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W $ is the weight matrix (learned parameters),\n",
    "- $ x $ is the input vector,\n",
    "- $ b $ is the bias vector (also learned parameters),\n",
    "- $ f $ is a non-linear activation function (e.g., ReLU, sigmoid),\n",
    "- $ y $ is the output vector.\n",
    "\n",
    "The goal during training is to learn the optimal values of $ W $ and $ b $ such that the model minimizes the loss function over the dataset.\n",
    "\n",
    "#### **Parameter initialization**\n",
    "\n",
    "In a custom layer, parameters like weights and biases are initialized either randomly or with specific values. Common initialization methods include:\n",
    "- **Xavier (Glorot) initialization**: This method sets the weights based on the size of the input and output layers. For a layer with input dimension $ n_{\\text{in}} $ and output dimension $ n_{\\text{out}} $, the weights are initialized as:\n",
    "\n",
    "  $$\n",
    "  W \\sim U\\left(-\\frac{\\sqrt{6}}{\\sqrt{n_{\\text{in}} + n_{\\text{out}}}}, \\frac{\\sqrt{6}}{\\sqrt{n_{\\text{in}} + n_{\\text{out}}}}\\right)\n",
    "  $$\n",
    "\n",
    "  Where $ U(a, b) $ denotes a uniform distribution between $ a $ and $ b $.\n",
    "\n",
    "- **He initialization**: Similar to Xavier initialization, but more suitable for layers using ReLU activation functions. The weights are initialized as:\n",
    "\n",
    "  $$\n",
    "  W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\text{in}}}\\right)\n",
    "  $$\n",
    "\n",
    "  Where $ \\mathcal{N}(0, \\sigma^2) $ represents a normal distribution with mean 0 and variance $ \\sigma^2 $.\n",
    "\n",
    "#### **Forward pass: Mathematical formulation**\n",
    "\n",
    "The forward pass of a custom layer computes the output based on the input, learned parameters, and the transformation the layer performs. For instance, in a custom linear layer, the forward pass computes:\n",
    "\n",
    "$$\n",
    "y = Wx + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W \\in \\mathbb{R}^{n_{\\text{out}} \\times n_{\\text{in}}} $ is the weight matrix,\n",
    "- $ x \\in \\mathbb{R}^{n_{\\text{in}}} $ is the input,\n",
    "- $ b \\in \\mathbb{R}^{n_{\\text{out}}} $ is the bias vector,\n",
    "- $ y \\in \\mathbb{R}^{n_{\\text{out}}} $ is the output vector.\n",
    "\n",
    "If the custom layer involves non-linearities, an activation function $ f $ is applied after the linear transformation:\n",
    "\n",
    "$$\n",
    "y = f(Wx + b)\n",
    "$$\n",
    "\n",
    "Activation functions such as ReLU or sigmoid modify the output in a non-linear way, enabling the model to learn complex patterns in the data.\n",
    "\n",
    "#### **Gradient flow and backpropagation**\n",
    "\n",
    "During training, the custom layer must support backpropagation, the process of updating weights based on the gradients of the loss function with respect to each parameter. PyTorch automatically computes these gradients using its autograd system, provided the operations defined in the forward pass are differentiable.\n",
    "\n",
    "For the custom linear layer, the gradients with respect to the parameters $ W $ and $ b $ are:\n",
    "\n",
    "- **Gradient of the loss with respect to the weights** $ W $:\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial W} = \\delta \\cdot x^T\n",
    "   $$\n",
    "   Where:\n",
    "   - $ L $ is the loss function,\n",
    "   - $ \\delta = \\frac{\\partial L}{\\partial y} $ is the gradient of the loss with respect to the output,\n",
    "   - $ x^T $ is the transpose of the input vector.\n",
    "\n",
    "- **Gradient of the loss with respect to the bias** $ b $:\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial b} = \\delta\n",
    "   $$\n",
    "\n",
    "- **Gradient of the loss with respect to the input** $ x $:\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial x} = W^T \\cdot \\delta\n",
    "   $$\n",
    "\n",
    "These gradients are used to update the layer's parameters during the optimization process (e.g., using stochastic gradient descent).\n",
    "\n",
    "#### **Custom layer example: Weight normalization**\n",
    "\n",
    "In weight normalization, a custom layer learns to normalize its weights during training. The normalized weights $ \\hat{W} $ are computed as:\n",
    "\n",
    "$$\n",
    "\\hat{W} = \\frac{g}{\\| W \\|} W\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W $ is the unnormalized weight matrix,\n",
    "- $ g $ is a learnable scaling factor,\n",
    "- $ \\| W \\| $ is the Euclidean norm of the weight matrix.\n",
    "\n",
    "The forward pass using weight normalization is:\n",
    "\n",
    "$$\n",
    "y = \\hat{W}x + b\n",
    "$$\n",
    "\n",
    "In this case, the gradients are computed for both $ W $ and $ g $, and the normalization step ensures that the weights have unit norm, helping stabilize training.\n",
    "\n",
    "#### **Non-linearity in custom layers**\n",
    "\n",
    "Custom layers can also involve complex non-linear operations. For example, in layers that involve non-linear activation functions like the sigmoid or hyperbolic tangent (tanh), the mathematical expressions are:\n",
    "\n",
    "- **Sigmoid function**:\n",
    "\n",
    "  $$\n",
    "  f(x) = \\frac{1}{1 + e^{-x}}\n",
    "  $$\n",
    "\n",
    "  The sigmoid function squashes the input values to the range (0, 1), making it suitable for probabilistic interpretations.\n",
    "\n",
    "- **Tanh function**:\n",
    "\n",
    "  $$\n",
    "  f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "  $$\n",
    "\n",
    "  The tanh function maps inputs to the range (-1, 1), offering zero-centered outputs that help gradient flow during training.\n",
    "\n",
    "#### **Parameter regularization**\n",
    "\n",
    "In custom layers, regularization techniques such as **L2 regularization** can be applied to control overfitting by penalizing large weights. L2 regularization adds a penalty term to the loss function, proportional to the squared magnitude of the weights:\n",
    "\n",
    "$$\n",
    "L_{\\text{reg}} = L + \\lambda \\| W \\|^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ L $ is the original loss,\n",
    "- $ \\lambda $ is the regularization strength,\n",
    "- $ \\| W \\|^2 $ is the L2 norm of the weights.\n",
    "\n",
    "During backpropagation, the regularization term contributes to the gradient update, encouraging smaller weights and improving generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for building and training custom layers in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules for creating custom layers and handling model training in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you configure your environment to leverage a GPU for training custom layers, and how do you fallback to CPU in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a basic custom layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you define a simple custom layer by subclassing `torch.nn.Module`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you implement the forward pass for a basic custom layer in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you instantiate and apply a basic custom layer to an input tensor in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing parameterized custom layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you create trainable parameters like weights and biases in a custom layer using `nn.Parameter`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you implement a parameterized custom layer that applies a learned linear transformation to input data?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you initialize custom layer parameters (e.g., using Xavier or Kaiming initialization) in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you apply a parameterized custom layer in a small neural network for a regression task?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying custom layers in simple networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you use a custom layer alongside PyTorch's built-in layers in a feedforward neural network?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you define a small neural network that combines custom layers with standard layers (e.g., `nn.Linear`, `nn.ReLU`)?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you train a simple neural network with custom layers using a standard dataset like MNIST?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a custom activation layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you create a custom activation function by subclassing `torch.nn.Module`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you implement a variant of ReLU as a custom activation layer in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you apply your custom activation layer in a small neural network, and how does it compare with built-in activation functions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a custom normalization layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you implement a custom batch normalization layer using `nn.Module`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you define a custom layer normalization operation and apply it to a neural network?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you test the performance of a network using custom normalization layers compared to standard ones like `BatchNorm2d`?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and validating custom layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you perform unit tests for a custom layer to ensure the output dimensions and gradients are correct?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you inspect the gradients of a custom layer during backpropagation to verify proper gradient flow?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you evaluate the performance of a custom layer on a simple classification or regression task using a validation dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with custom layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you modify the architecture of a custom layer (e.g., adding more parameters or changing the activation function) and observe the effect on performance?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you test different initialization techniques for the weights in your custom layers, and how do they affect the model’s convergence?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you experiment with adding multiple custom layers in a network and measure their impact on model accuracy or loss?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
