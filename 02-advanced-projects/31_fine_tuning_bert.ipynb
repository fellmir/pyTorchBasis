{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding BERT and transfer learning](#understanding-bert-and-transfer-learning)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Loading the pre-trained BERT model](#loading-the-pre-trained-bert-model)\n",
    "4. [Preparing the dataset](#preparing-the-dataset)\n",
    "5. [Tokenizing input data for BERT](#tokenizing-input-data-for-bert)\n",
    "6. [Modifying BERT for fine-tuning](#modifying-bert-for-fine-tuning)\n",
    "7. [Training the fine-tuned BERT model](#training-the-fine-tuned-bert-model)\n",
    "8. [Evaluating the fine-tuned BERT model](#evaluating-the-fine-tuned-bert-model)\n",
    "9. [Experimenting with different fine-tuning strategies](#experimenting-with-different-fine-tuning-strategies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding BERT and transfer learning\n",
    "\n",
    "**BERT (Bidirectional Encoder Representations from Transformers)** is a pre-trained language model built on the Transformer architecture that has dramatically advanced the field of natural language processing (NLP). It uses **transfer learning**, where the knowledge gained during the model's pre-training on one task is transferred to another task through fine-tuning. Fine-tuning BERT in PyTorch involves adapting this pre-trained model to specific downstream tasks (e.g., sentiment analysis, text classification, or named entity recognition) by training it further on task-specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is Transfer Learning?**\n",
    "\n",
    "**Transfer learning** is a machine learning technique where a model trained on one task is reused or adapted for a different, but related, task. In the context of NLP and BERT, transfer learning takes place in two stages:\n",
    "1. **Pre-training**: The model is trained on a large, generic corpus (such as Wikipedia or BookCorpus) to learn general language representations. In BERT’s case, it learns bidirectional representations by predicting masked words and understanding sentence relationships.\n",
    "2. **Fine-tuning**: The pre-trained model is then further trained (or fine-tuned) on a smaller, task-specific dataset. This process allows BERT to adjust its general understanding of language to the specific requirements of the downstream task, like question answering or text classification.\n",
    "\n",
    "Transfer learning is effective because the model doesn’t have to learn from scratch for each new task, significantly reducing training time and improving performance, especially when only limited data is available for the target task.\n",
    "\n",
    "### **Why use BERT for transfer learning?**\n",
    "\n",
    "BERT’s ability to understand language contextually in a **bidirectional** manner makes it an excellent candidate for transfer learning. Traditional models like recurrent neural networks (RNNs) and long short-term memory (LSTM) networks process language in one direction (either left-to-right or right-to-left), which limits their ability to understand the context of words in a broader sense. BERT, on the other hand, captures context from both directions simultaneously, which allows for a deeper understanding of the relationships between words in a sentence.\n",
    "\n",
    "In addition to its **bidirectional training** approach, BERT’s pre-training on massive corpora makes it highly effective at transfer learning for a wide range of NLP tasks.\n",
    "\n",
    "### **Fine-tuning BERT: The process**\n",
    "\n",
    "Fine-tuning BERT is the process of adapting the pre-trained model to specific downstream tasks by making small updates to the model’s weights. Here’s how fine-tuning works:\n",
    "\n",
    "#### **Loading a pre-trained BERT model**\n",
    "The first step in fine-tuning is to load the pre-trained BERT model. This model has already been trained on general language understanding tasks like masked language modeling (MLM) and next sentence prediction (NSP). The learned weights from this pre-training phase are transferred to the downstream task.\n",
    "\n",
    "#### **Task-specific architecture**\n",
    "For most tasks, a simple architecture is added on top of BERT’s base layers. For example, in classification tasks like sentiment analysis, a fully connected (dense) layer is typically added on top of the **[CLS]** token (a special token that represents the entire input sequence) to predict the class label.\n",
    "\n",
    "The output of the BERT model for the **[CLS]** token provides a rich representation of the entire input sequence. This output is passed through a linear layer to produce logits (raw predictions), which are then used to compute the task-specific loss.\n",
    "\n",
    "#### **Fine-tuning with task-specific data**\n",
    "Once the architecture is set up, BERT is fine-tuned using labeled data specific to the task. The entire model, including the pre-trained layers, is updated during this process. This step allows BERT to retain the general language understanding gained during pre-training while adapting to the nuances of the task.\n",
    "\n",
    "Fine-tuning is relatively fast and efficient compared to training a model from scratch because the majority of language patterns have already been learned during pre-training. Fine-tuning typically only requires a small dataset for the target task.\n",
    "\n",
    "#### **Optimizing hyperparameters**\n",
    "When fine-tuning BERT, several key hyperparameters are typically adjusted:\n",
    "- **Learning rate**: Since the pre-trained BERT model already has strong general language representations, fine-tuning is done with a lower learning rate to prevent drastic changes in the model’s weights. A typical learning rate is between $2 \\times 10^{-5}$ and $5 \\times 10^{-5}$.\n",
    "- **Batch size**: Fine-tuning BERT often requires smaller batch sizes (e.g., 16 or 32) due to memory constraints, as BERT is a large model with many parameters.\n",
    "- **Epochs**: Fine-tuning generally requires fewer epochs (typically 2-4) because the model is already pre-trained and only needs to adjust to the task-specific data.\n",
    "\n",
    "### **Benefits of Fine-tuning BERT with Transfer Learning**\n",
    "\n",
    "#### **Reduced training time**\n",
    "One of the biggest advantages of transfer learning with BERT is that it significantly reduces training time. Since BERT has already learned general language representations, fine-tuning is much faster compared to training a model from scratch. This is especially beneficial for tasks with limited labeled data, where training from scratch may not be feasible.\n",
    "\n",
    "#### **Improved performance**\n",
    "Fine-tuning BERT has consistently demonstrated state-of-the-art performance on a wide range of NLP tasks, including sentiment analysis, question answering, named entity recognition, and text classification. By transferring the knowledge gained during pre-training, BERT achieves better results compared to models trained solely on task-specific data.\n",
    "\n",
    "#### **Generalization across tasks**\n",
    "Since BERT is pre-trained on a wide variety of text, it generalizes well to many different NLP tasks. The fine-tuning process allows the model to adapt its knowledge to new tasks without losing the benefits of its pre-training.\n",
    "\n",
    "### **Applications of fine-tuning BERT**\n",
    "\n",
    "Fine-tuning BERT can be applied to numerous NLP tasks, including:\n",
    "- **Text classification**: Fine-tuning BERT for tasks like sentiment analysis, spam detection, or topic classification allows the model to predict labels for text based on context.\n",
    "- **Named entity recognition (NER)**: BERT can be fine-tuned to identify and classify named entities such as people, locations, and organizations within a text.\n",
    "- **Question answering**: BERT has been highly successful in question-answering systems, where the model is fine-tuned to find and output the answer to a question given a passage of text.\n",
    "- **Sentence similarity**: BERT can be fine-tuned to determine how similar two sentences are, which is useful in tasks like paraphrase detection or semantic search.\n",
    "- **Text summarization**: Fine-tuning BERT for summarization tasks helps the model generate concise summaries of long documents.\n",
    "\n",
    "### **Challenges in Fine-tuning BERT**\n",
    "\n",
    "While fine-tuning BERT is highly effective, there are several challenges to be mindful of:\n",
    "- **Overfitting**: Since fine-tuning is done on task-specific datasets, there is a risk of overfitting if the dataset is too small. Regularization techniques like dropout and early stopping can help mitigate this risk.\n",
    "- **Computational resources**: BERT is a large model, and fine-tuning requires significant computational power, especially for larger versions like BERT-large. Access to GPUs or TPUs is often necessary to fine-tune BERT efficiently.\n",
    "- **Catastrophic forgetting**: While fine-tuning adapts BERT to new tasks, there is a risk that the model will \"forget\" the general language knowledge it gained during pre-training. Careful adjustment of the learning rate and training schedule can help prevent this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for fine-tuning BERT?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules from the `transformers` library to load BERT and handle tokenization?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you configure the environment to use GPU for fine-tuning BERT in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the pre-trained BERT model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you load a pre-trained BERT model using Hugging Face’s `transformers` library?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you load the corresponding tokenizer for BERT to handle input data preprocessing?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you inspect the structure of the pre-trained BERT model to understand its layers and outputs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you load a text classification dataset using Hugging Face’s `datasets` library?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you split the dataset into training, validation, and test sets for fine-tuning BERT?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you preprocess the dataset before passing it to the tokenizer?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing input data for BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you use `BertTokenizer` to tokenize input text and convert it into token IDs for BERT?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you handle padding and truncation to ensure all input sequences are the same length before feeding them into BERT?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you create attention masks to distinguish between padded and real tokens in the input sequences?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you create a PyTorch `DataLoader` to batch the tokenized input data for efficient training?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying BERT for fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you add a classification layer on top of the pre-trained BERT model for text classification tasks?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you freeze the BERT base layers and train only the added classification head to avoid overfitting early on?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you unfreeze the BERT base layers after initial training to fine-tune the entire model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the fine-tuned BERT model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you define the loss function for training BERT on a text classification task?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you set up the AdamW optimizer with weight decay to update the model’s parameters during training?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you implement the training loop, including the forward pass through BERT, loss calculation, and backpropagation?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you apply gradient clipping to prevent exploding gradients during the fine-tuning of BERT?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you track and log the training loss and accuracy over multiple epochs while fine-tuning BERT?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the fine-tuned BERT model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you evaluate the fine-tuned BERT model on a validation or test set to calculate performance metrics?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you compute additional evaluation metrics for the fine-tuned BERT model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you implement a function to perform inference using the fine-tuned BERT model on new text data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with different fine-tuning strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you experiment with freezing and unfreezing different layers of BERT during fine-tuning to observe their impact on performance?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you experiment with different learning rates for the classification head and the BERT base model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q27: How do you experiment with different batch sizes and observe their impact on training stability and performance?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q28: How do you fine-tune BERT with a smaller dataset and apply regularization techniques to prevent overfitting?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q29: How do you implement early stopping based on validation performance to prevent overfitting during fine-tuning?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q30: How do you fine-tune BERT on a specific task and analyze the results?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
