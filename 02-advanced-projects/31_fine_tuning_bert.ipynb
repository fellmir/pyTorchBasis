{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding BERT and transfer learning](#understanding-bert-and-transfer-learning)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Loading the pre-trained BERT model](#loading-the-pre-trained-bert-model)\n",
    "4. [Preparing the dataset](#preparing-the-dataset)\n",
    "5. [Tokenizing input data for BERT](#tokenizing-input-data-for-bert)\n",
    "6. [Modifying BERT for fine-tuning](#modifying-bert-for-fine-tuning)\n",
    "7. [Training the fine-tuned BERT model](#training-the-fine-tuned-bert-model)\n",
    "8. [Evaluating the fine-tuned BERT model](#evaluating-the-fine-tuned-bert-model)\n",
    "9. [Experimenting with different fine-tuning strategies](#experimenting-with-different-fine-tuning-strategies)\n",
    "10. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding BERT and transfer learning\n",
    "\n",
    "**BERT (Bidirectional Encoder Representations from Transformers)** is a pre-trained language model built on the Transformer architecture that has dramatically advanced the field of natural language processing (NLP). It uses **transfer learning**, where the knowledge gained during the model's pre-training on one task is transferred to another task through fine-tuning. Fine-tuning BERT in PyTorch involves adapting this pre-trained model to specific downstream tasks (e.g., sentiment analysis, text classification, or named entity recognition) by training it further on task-specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is Transfer Learning?**\n",
    "\n",
    "**Transfer learning** is a machine learning technique where a model trained on one task is reused or adapted for a different, but related, task. In the context of NLP and BERT, transfer learning takes place in two stages:\n",
    "1. **Pre-training**: The model is trained on a large, generic corpus (such as Wikipedia or BookCorpus) to learn general language representations. In BERT’s case, it learns bidirectional representations by predicting masked words and understanding sentence relationships.\n",
    "2. **Fine-tuning**: The pre-trained model is then further trained (or fine-tuned) on a smaller, task-specific dataset. This process allows BERT to adjust its general understanding of language to the specific requirements of the downstream task, like question answering or text classification.\n",
    "\n",
    "Transfer learning is effective because the model doesn’t have to learn from scratch for each new task, significantly reducing training time and improving performance, especially when only limited data is available for the target task.\n",
    "\n",
    "### **Why use BERT for transfer learning?**\n",
    "\n",
    "BERT’s ability to understand language contextually in a **bidirectional** manner makes it an excellent candidate for transfer learning. Traditional models like recurrent neural networks (RNNs) and long short-term memory (LSTM) networks process language in one direction (either left-to-right or right-to-left), which limits their ability to understand the context of words in a broader sense. BERT, on the other hand, captures context from both directions simultaneously, which allows for a deeper understanding of the relationships between words in a sentence.\n",
    "\n",
    "In addition to its **bidirectional training** approach, BERT’s pre-training on massive corpora makes it highly effective at transfer learning for a wide range of NLP tasks.\n",
    "\n",
    "### **Fine-tuning BERT: The process**\n",
    "\n",
    "Fine-tuning BERT is the process of adapting the pre-trained model to specific downstream tasks by making small updates to the model’s weights. Here’s how fine-tuning works:\n",
    "\n",
    "#### **Loading a pre-trained BERT model**\n",
    "The first step in fine-tuning is to load the pre-trained BERT model. This model has already been trained on general language understanding tasks like masked language modeling (MLM) and next sentence prediction (NSP). The learned weights from this pre-training phase are transferred to the downstream task.\n",
    "\n",
    "#### **Task-specific architecture**\n",
    "For most tasks, a simple architecture is added on top of BERT’s base layers. For example, in classification tasks like sentiment analysis, a fully connected (dense) layer is typically added on top of the **[CLS]** token (a special token that represents the entire input sequence) to predict the class label.\n",
    "\n",
    "The output of the BERT model for the **[CLS]** token provides a rich representation of the entire input sequence. This output is passed through a linear layer to produce logits (raw predictions), which are then used to compute the task-specific loss.\n",
    "\n",
    "#### **Fine-tuning with task-specific data**\n",
    "Once the architecture is set up, BERT is fine-tuned using labeled data specific to the task. The entire model, including the pre-trained layers, is updated during this process. This step allows BERT to retain the general language understanding gained during pre-training while adapting to the nuances of the task.\n",
    "\n",
    "Fine-tuning is relatively fast and efficient compared to training a model from scratch because the majority of language patterns have already been learned during pre-training. Fine-tuning typically only requires a small dataset for the target task.\n",
    "\n",
    "#### **Optimizing hyperparameters**\n",
    "When fine-tuning BERT, several key hyperparameters are typically adjusted:\n",
    "- **Learning rate**: Since the pre-trained BERT model already has strong general language representations, fine-tuning is done with a lower learning rate to prevent drastic changes in the model’s weights. A typical learning rate is between $2 \\times 10^{-5}$ and $5 \\times 10^{-5}$.\n",
    "- **Batch size**: Fine-tuning BERT often requires smaller batch sizes (e.g., 16 or 32) due to memory constraints, as BERT is a large model with many parameters.\n",
    "- **Epochs**: Fine-tuning generally requires fewer epochs (typically 2-4) because the model is already pre-trained and only needs to adjust to the task-specific data.\n",
    "\n",
    "### **Benefits of Fine-tuning BERT with Transfer Learning**\n",
    "\n",
    "#### **Reduced training time**\n",
    "One of the biggest advantages of transfer learning with BERT is that it significantly reduces training time. Since BERT has already learned general language representations, fine-tuning is much faster compared to training a model from scratch. This is especially beneficial for tasks with limited labeled data, where training from scratch may not be feasible.\n",
    "\n",
    "#### **Improved performance**\n",
    "Fine-tuning BERT has consistently demonstrated state-of-the-art performance on a wide range of NLP tasks, including sentiment analysis, question answering, named entity recognition, and text classification. By transferring the knowledge gained during pre-training, BERT achieves better results compared to models trained solely on task-specific data.\n",
    "\n",
    "#### **Generalization across tasks**\n",
    "Since BERT is pre-trained on a wide variety of text, it generalizes well to many different NLP tasks. The fine-tuning process allows the model to adapt its knowledge to new tasks without losing the benefits of its pre-training.\n",
    "\n",
    "### **Applications of fine-tuning BERT**\n",
    "\n",
    "Fine-tuning BERT can be applied to numerous NLP tasks, including:\n",
    "- **Text classification**: Fine-tuning BERT for tasks like sentiment analysis, spam detection, or topic classification allows the model to predict labels for text based on context.\n",
    "- **Named entity recognition (NER)**: BERT can be fine-tuned to identify and classify named entities such as people, locations, and organizations within a text.\n",
    "- **Question answering**: BERT has been highly successful in question-answering systems, where the model is fine-tuned to find and output the answer to a question given a passage of text.\n",
    "- **Sentence similarity**: BERT can be fine-tuned to determine how similar two sentences are, which is useful in tasks like paraphrase detection or semantic search.\n",
    "- **Text summarization**: Fine-tuning BERT for summarization tasks helps the model generate concise summaries of long documents.\n",
    "\n",
    "### **Challenges in Fine-tuning BERT**\n",
    "\n",
    "While fine-tuning BERT is highly effective, there are several challenges to be mindful of:\n",
    "- **Overfitting**: Since fine-tuning is done on task-specific datasets, there is a risk of overfitting if the dataset is too small. Regularization techniques like dropout and early stopping can help mitigate this risk.\n",
    "- **Computational resources**: BERT is a large model, and fine-tuning requires significant computational power, especially for larger versions like BERT-large. Access to GPUs or TPUs is often necessary to fine-tune BERT efficiently.\n",
    "- **Catastrophic forgetting**: While fine-tuning adapts BERT to new tasks, there is a risk that the model will \"forget\" the general language knowledge it gained during pre-training. Careful adjustment of the learning rate and training schedule can help prevent this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Maths**\n",
    "\n",
    "#### **BERT’s core architecture**\n",
    "\n",
    "BERT is built upon the **Transformer** model, specifically utilizing the **encoder** part of the Transformer architecture. The primary operations of BERT involve processing sequences of tokens using **self-attention** and feedforward layers.\n",
    "\n",
    "Given an input sequence of tokens $ X = \\{x_1, x_2, \\dots, x_n\\} $, where $ n $ is the number of tokens, BERT first converts each token into an embedding vector. These embeddings are then fed into multiple layers of the Transformer encoder.\n",
    "\n",
    "Let’s break down the key components:\n",
    "\n",
    "#### **Input embeddings**\n",
    "\n",
    "Each token in the input sequence is converted into an embedding vector:\n",
    "$$\n",
    "E = \\{e_1, e_2, \\dots, e_n\\}, \\quad e_i \\in \\mathbb{R}^d\n",
    "$$\n",
    "where $ e_i $ is the embedding of token $ x_i $, and $ d $ is the dimensionality of the embeddings. The input embeddings are the sum of the token embeddings, positional embeddings, and segment embeddings.\n",
    "\n",
    "$$\n",
    "e_i = \\text{TokenEmb}(x_i) + \\text{PosEmb}(i) + \\text{SegEmb}(s_i)\n",
    "$$\n",
    "Here, $ \\text{TokenEmb}(x_i) $ is the token embedding, $ \\text{PosEmb}(i) $ is the positional embedding (providing information about the token’s position in the sequence), and $ \\text{SegEmb}(s_i) $ is the segment embedding, distinguishing between different segments (e.g., in next sentence prediction).\n",
    "\n",
    "#### **Self-attention mechanism**\n",
    "\n",
    "Self-attention enables BERT to compute a representation for each token based on the entire input sequence. For each token, a **query** $ q_i $, **key** $ k_i $, and **value** $ v_i $ vector are computed through learned weight matrices:\n",
    "\n",
    "$$\n",
    "q_i = W_Q e_i, \\quad k_i = W_K e_i, \\quad v_i = W_V e_i\n",
    "$$\n",
    "where $ W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d_k} $ are the learned weight matrices that project the input embedding $ e_i $ into the query, key, and value spaces, each with dimensionality $ d_k $.\n",
    "\n",
    "The attention score between token $ i $ and token $ j $ is computed using the dot product between their query and key vectors, scaled by $ \\sqrt{d_k} $:\n",
    "\n",
    "$$\n",
    "\\text{score}(q_i, k_j) = \\frac{q_i \\cdot k_j}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "These attention scores are normalized using a **softmax** function to compute the attention weights:\n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{\\exp(\\text{score}(q_i, k_j))}{\\sum_{k=1}^n \\exp(\\text{score}(q_i, k_k))}\n",
    "$$\n",
    "\n",
    "Finally, the output for each token is computed as a weighted sum of the value vectors from all tokens:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(q_i, K, V) = \\sum_{j=1}^{n} \\alpha_{ij} v_j\n",
    "$$\n",
    "\n",
    "This operation allows each token to attend to all other tokens in the sequence, enabling BERT to capture long-range dependencies in the input.\n",
    "\n",
    "#### **Multi-head attention**\n",
    "\n",
    "BERT uses **multi-head attention**, where multiple attention mechanisms (or heads) operate in parallel to capture different aspects of the relationships between tokens. Each attention head computes its own query, key, and value vectors, and the results are concatenated:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W_O\n",
    "$$\n",
    "where $ W_O \\in \\mathbb{R}^{h \\cdot d_k \\times d} $ is a learned output projection matrix, and $ h $ is the number of attention heads. This enables the model to learn multiple ways of attending to the input tokens.\n",
    "\n",
    "#### **Feedforward network**\n",
    "\n",
    "After the multi-head attention mechanism, the output is passed through a feedforward neural network (FFN) that applies non-linear transformations to each token’s representation independently:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\max(0, x W_1 + b_1) W_2 + b_2\n",
    "$$\n",
    "where $ W_1 \\in \\mathbb{R}^{d \\times d_{\\text{ff}}} $, $ W_2 \\in \\mathbb{R}^{d_{\\text{ff}} \\times d} $, and $ b_1, b_2 $ are the learned weight matrices and biases. The dimensionality of the hidden layer, $ d_{\\text{ff}} $, is typically larger than the input/output dimensionality $ d $.\n",
    "\n",
    "#### **Pre-training tasks**\n",
    "\n",
    "During pre-training, BERT is optimized using two key objectives:\n",
    "\n",
    "- **Masked Language Modeling (MLM)**: In this task, random tokens in the input sequence are masked, and the model is trained to predict the masked tokens. For each token $ x_i $, BERT is trained to maximize the probability of the correct token:\n",
    "\n",
    "$$\n",
    "p(x_i \\mid X_{\\text{masked}}) = \\frac{\\exp(W_{\\text{vocab}} e_i)}{\\sum_{w \\in \\text{Vocab}} \\exp(W_{\\text{vocab}} e_w)}\n",
    "$$\n",
    "where $ W_{\\text{vocab}} $ maps the hidden representation of the masked token back to the vocabulary space.\n",
    "\n",
    "- **Next Sentence Prediction (NSP)**: BERT is also trained to predict whether two sentences follow each other in the text. Given sentence A and sentence B, BERT maximizes the probability that sentence B follows sentence A. This task helps BERT learn relationships between sentences.\n",
    "\n",
    "#### **Fine-tuning BERT**\n",
    "\n",
    "When fine-tuning BERT on a downstream task, such as text classification or question answering, the pre-trained BERT model is used as the base, and a task-specific layer (e.g., a classifier) is added on top. The entire model, including the pre-trained layers, is fine-tuned using a task-specific objective.\n",
    "\n",
    "For classification tasks, a softmax classifier is typically added on top of the **[CLS]** token’s final hidden state. The predicted probability of each class is given by:\n",
    "\n",
    "$$\n",
    "p(c \\mid X) = \\frac{\\exp(W_c h_{\\text{[CLS]}})}{\\sum_{c'} \\exp(W_{c'} h_{\\text{[CLS]}})}\n",
    "$$\n",
    "\n",
    "Here, $ h_{\\text{[CLS]}} $ is the final hidden state of the **[CLS]** token, and $ W_c $ are the learned weights for the classification layer. The model is fine-tuned to minimize the cross-entropy loss between the predicted and true class labels.\n",
    "\n",
    "#### **Optimization during fine-tuning**\n",
    "\n",
    "During fine-tuning, the loss function is optimized using gradient-based methods. The parameters of the entire model are updated using gradients computed through backpropagation. The learning rate is typically smaller during fine-tuning to prevent large updates to the pre-trained weights, which could degrade the model’s general language understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries like PyTorch, Hugging Face `transformers`, and `datasets` for fine-tuning BERT?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules from the `transformers` library to load BERT and handle tokenization?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you configure the environment to use GPU for fine-tuning BERT in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the pre-trained BERT model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you load a pre-trained BERT model (e.g., `bert-base-uncased`) using Hugging Face’s `transformers` library?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you load the corresponding tokenizer for BERT to handle input data preprocessing?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you inspect the structure of the pre-trained BERT model to understand its layers and outputs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you load a text classification dataset (e.g., IMDb or SST-2) using Hugging Face’s `datasets` library?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you split the dataset into training, validation, and test sets for fine-tuning BERT?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you preprocess the dataset (e.g., lowercasing, removing special characters) before passing it to the tokenizer?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing input data for BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you use `BertTokenizer` to tokenize input text and convert it into token IDs for BERT?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you handle padding and truncation to ensure all input sequences are the same length before feeding them into BERT?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you create attention masks to distinguish between padded and real tokens in the input sequences?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you create a PyTorch `DataLoader` to batch the tokenized input data for efficient training?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying BERT for fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you add a classification layer on top of the pre-trained BERT model for text classification tasks?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you freeze the BERT base layers and train only the added classification head to avoid overfitting early on?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you unfreeze the BERT base layers after initial training to fine-tune the entire model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the fine-tuned BERT model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you define the loss function (e.g., CrossEntropyLoss) for training BERT on a text classification task?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you set up the AdamW optimizer with weight decay to update the model’s parameters during training?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you implement the training loop, including the forward pass through BERT, loss calculation, and backpropagation?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you apply gradient clipping to prevent exploding gradients during the fine-tuning of BERT?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you track and log the training loss and accuracy over multiple epochs while fine-tuning BERT?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the fine-tuned BERT model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you evaluate the fine-tuned BERT model on a validation or test set to calculate performance metrics like accuracy?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you compute additional evaluation metrics like F1 score, precision, and recall for the fine-tuned BERT model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you implement a function to perform inference using the fine-tuned BERT model on new text data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with different fine-tuning strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you experiment with freezing and unfreezing different layers of BERT during fine-tuning to observe their impact on performance?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you experiment with different learning rates for the classification head and the BERT base model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q27: How do you experiment with different batch sizes and observe their impact on training stability and performance?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q28: How do you fine-tune BERT with a smaller dataset and apply regularization techniques like dropout or weight decay to prevent overfitting?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q29: How do you implement early stopping based on validation performance to prevent overfitting during fine-tuning?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q30: How do you fine-tune BERT on a specific task like sentiment analysis or named entity recognition (NER) and analyze the results?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
