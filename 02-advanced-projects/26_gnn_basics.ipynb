{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph neural network (GNN) basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding graph neural networks (GNNs)](#understanding-graph-neural-networks-gnns)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Defining graph data](#defining-graph-data)\n",
    "4. [Building a basic message-passing mechanism](#building-a-basic-message-passing-mechanism)\n",
    "5. [Implementing a simple graph convolution layer](#implementing-a-simple-graph-convolution-layer)\n",
    "6. [Building a basic GNN model](#building-a-basic-gnn-model)\n",
    "7. [Training the GNN on a node classification task](#training-the-gnn-on-a-node-classification-task)\n",
    "8. [Evaluating the GNN model](#evaluating-the-gnn-model)\n",
    "9. [Experimenting with different configurations](#experimenting-with-different-configurations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding graph neural networks (GNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Key concepts**\n",
    "Graph Neural Networks (GNNs) are a class of neural networks designed to process data represented as graphs, where nodes represent entities and edges represent relationships. Unlike traditional neural networks, which operate on structured data like grids or sequences, GNNs can handle non-Euclidean data, making them suitable for tasks involving irregular, interconnected data structures.\n",
    "\n",
    "Key elements of GNNs include:\n",
    "- **Node Features**: Represent attributes or characteristics of individual nodes.\n",
    "- **Edge Features**: Capture the relationships or interactions between nodes.\n",
    "- **Message Passing**: Nodes exchange information with their neighbors to update their representations iteratively.\n",
    "- **Graph Representation**: The model learns embeddings for nodes, edges, or the entire graph, depending on the task.\n",
    "\n",
    "Popular GNN architectures include Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and GraphSAGE, each specializing in different aspects of graph learning.\n",
    "\n",
    "### **Applications**\n",
    "GNNs have a wide range of applications across various domains:\n",
    "- **Social networks**: Analyzing user interactions for recommendations, influence detection, or community detection.\n",
    "- **Molecular biology**: Predicting molecular properties or drug interactions based on chemical structure graphs.\n",
    "- **Knowledge graphs**: Enhancing link prediction, node classification, and graph completion tasks.\n",
    "- **Recommendation systems**: Personalizing content or product suggestions by modeling user-item interaction graphs.\n",
    "- **Traffic networks**: Analyzing and predicting traffic flow in road or transportation networks.\n",
    "\n",
    "### **Advantages**\n",
    "- **Flexible data handling**: Processes graph-structured data of varying sizes and connectivity.\n",
    "- **Relational reasoning**: Captures relationships and dependencies between entities effectively.\n",
    "- **Generalization**: Learns embeddings that generalize well to unseen nodes or subgraphs.\n",
    "- **Scalability**: Supports various graph sizes through efficient message-passing mechanisms.\n",
    "\n",
    "### **Challenges**\n",
    "- **Scalability for large graphs**: Training on massive graphs requires substantial computational resources and optimization techniques.\n",
    "- **Over-smoothing**: Node representations may become indistinguishable with excessive message-passing layers.\n",
    "- **Irregular data**: Processing graphs with heterogeneous structures or dynamic topologies can be complex.\n",
    "- **Data dependency**: Performance depends heavily on the quality and completeness of the graph structure and features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for building a GNN in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install torch-geometric\n",
    "# !pip install numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules for constructing a GNN and handling graph data in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you configure the environment to use GPU for training the GNN model in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining graph data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you represent a graph using an adjacency matrix in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = torch.tensor([\n",
    "    [0, 1, 1, 0],\n",
    "    [1, 0, 1, 1],\n",
    "    [1, 1, 0, 1],\n",
    "    [0, 1, 1, 0]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you define node features for each node in the graph as input to the GNN?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [1, 1],\n",
    "    [0, 0]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you convert graph edges into an edge list to represent the connections between nodes?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([\n",
    "    [0, 0, 1, 1, 1, 2, 2, 3, 3],\n",
    "    [1, 2, 0, 2, 3, 0, 3, 1, 2]\n",
    "], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a basic message-passing mechanism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you implement a basic message-passing mechanism between neighboring nodes in a graph?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_passing(x, edge_index):\n",
    "    source_nodes = edge_index[0]  # sender nodes\n",
    "    messages = x[source_nodes]  # fetch features of source nodes\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you aggregate messages from neighboring nodes using operations in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_messages(messages, edge_index, num_nodes):\n",
    "    target_nodes = edge_index[1]  # receiver nodes\n",
    "    aggregated = torch.zeros(num_nodes, messages.size(1)).to(messages.device)\n",
    "    aggregated.index_add_(0, target_nodes, messages)  # sum messages for each target\n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you implement node updates by combining aggregated messages with the node's own features?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_nodes(x, aggregated):\n",
    "    return x + aggregated  # residual connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a simple graph convolution layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you define a simple graph convolution layer using `torch.nn.Module` in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_channels, out_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you implement the forward pass of the graph convolution layer to compute new node embeddings?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        messages = message_passing(x, edge_index)  # fetch messages from neighbors\n",
    "        aggregated = aggregate_messages(messages, edge_index, x.size(0))  # sum by target\n",
    "        updated = update_nodes(x, aggregated)  # residual update\n",
    "        return self.linear(updated)  # linear transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you apply a non-linearity, such as ReLU, after computing the graph convolution to update node features?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        messages = message_passing(x, edge_index)  # fetch messages from neighbors\n",
    "        aggregated = aggregate_messages(messages, edge_index, x.size(0))  # sum by target\n",
    "        updated = update_nodes(x, aggregated)  # residual update\n",
    "        out = self.linear(updated)  # linear transformation\n",
    "        return F.relu(out)  # apply relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a basic GNN model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you stack multiple graph convolution layers to build a simple GNN model in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, out_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you define the forward pass of the GNN model to process node features through multiple graph convolution layers?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)  # first graph convolution\n",
    "        x = self.conv2(x, edge_index)  # second graph convolution\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you implement dropout and batch normalization in the GNN model to improve generalization?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_channels, hidden_channels)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, out_channels)\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)  # first graph convolution\n",
    "        x = self.bn1(x)  # batch norm after first layer\n",
    "        x = F.dropout(x, p=self.dropout_p, training=self.training)  # dropout\n",
    "        x = self.conv2(x, edge_index)  # second graph convolution\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the GNN on a node classification task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you define the loss function for training the GNN model on a node classification task?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you set up the optimizer to update the GNN model parameters during training?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleGNN(in_channels=2, hidden_channels=8, out_channels=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you implement the training loop for the GNN, including the forward pass, loss computation, and backpropagation?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphData:\n",
    "    def __init__(self, x, edge_index):\n",
    "        self.x = x\n",
    "        self.edge_index = edge_index\n",
    "\n",
    "x_input = torch.tensor([[1, 0], [0, 1], [1, 1], [0, 0]], dtype=torch.float32).to(device)\n",
    "edge_index_input = torch.tensor([[0, 0, 1, 1, 1, 2, 2, 3, 3],\n",
    "                                 [1, 2, 0, 2, 3, 0, 3, 1, 2]], dtype=torch.long).to(device)\n",
    "labels = torch.tensor([0, 1, 0, 1], dtype=torch.long).to(device)\n",
    "\n",
    "data = GraphData(x_input, edge_index_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, labels, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)  # forward pass\n",
    "    loss = criterion(out, labels)  # compute loss\n",
    "    loss.backward()  # backprop\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you track and log the accuracy and loss over training epochs to monitor the GNN modelâ€™s performance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        preds = out.argmax(dim=1)\n",
    "        correct = preds.eq(labels).sum().item()\n",
    "        acc = correct / labels.size(0)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00  Loss: 0.8202  Accuracy: 0.5000\n",
      "Epoch 100  Loss: 0.6931  Accuracy: 1.0000\n",
      "Epoch 200  Loss: 0.6443  Accuracy: 1.0000\n",
      "Epoch 300  Loss: 0.5450  Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "losses, accs = [], []\n",
    "for epoch in range(301):\n",
    "    loss = train(model, data, labels, optimizer, criterion)\n",
    "    acc = evaluate(model, data, labels)\n",
    "    losses.append(loss)\n",
    "    accs.append(acc)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:02d}  Loss: {loss:.4f}  Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the GNN model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you evaluate the GNN model on a validation or test dataset and calculate its accuracy for node classification?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.tensor([[1, 0], [0, 1], [1, 1], [0, 0]], dtype=torch.float32).to(device)\n",
    "test_edge_index = torch.tensor([[0, 1, 2, 3, 0, 1],\n",
    "                                [1, 2, 3, 0, 2, 3]], dtype=torch.long).to(device)\n",
    "test_labels = torch.tensor([0, 1, 0, 1], dtype=torch.long).to(device)\n",
    "test_data = GraphData(test_x, test_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "test_acc = evaluate(model, test_data, test_labels)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you implement a function to perform inference using the trained GNN model on new graph data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x.to(device), data.edge_index.to(device))\n",
    "        preds = out.argmax(dim=1)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "predictions = infer(model, test_data)\n",
    "print(f\"Predicted labels: {predictions.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with different configurations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you experiment with different numbers of graph convolution layers and observe the effect on model performance?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GraphConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeeperGNN(2, 8, 2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 3-layer GNN: 0.5000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(201):\n",
    "    loss = train(model, data, labels, optimizer, criterion)\n",
    "acc = evaluate(model, data, labels)\n",
    "print(f\"Accuracy with 3-layer GNN: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you adjust the hidden dimension size in the GNN layers to analyze its impact on training time and accuracy?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden size: 4  Accuracy: 0.5000\n",
      "Hidden size: 8  Accuracy: 0.5000\n",
      "Hidden size: 16  Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "hidden_sizes = [4, 8, 16]\n",
    "for h in hidden_sizes:\n",
    "    model = SimpleGNN(2, h, 2).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    for epoch in range(201):\n",
    "        train(model, data, labels, optimizer, criterion)\n",
    "    acc = evaluate(model, data, labels)\n",
    "    print(f\"Hidden size: {h}  Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you experiment with different aggregation functions in the message-passing mechanism?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_mean(messages, edge_index, num_nodes):\n",
    "    target_nodes = edge_index[1]\n",
    "    aggregated = torch.zeros(num_nodes, messages.size(1)).to(messages.device)\n",
    "    counts = torch.bincount(target_nodes, minlength=num_nodes).clamp(min=1).unsqueeze(1).float()\n",
    "    aggregated.index_add_(0, target_nodes, messages)\n",
    "    return aggregated / counts\n",
    "\n",
    "def aggregate_max(messages, edge_index, num_nodes):\n",
    "    target_nodes = edge_index[1]\n",
    "    aggregated = torch.full((num_nodes, messages.size(1)), float('-inf')).to(messages.device)\n",
    "    for i in range(messages.size(0)):\n",
    "        idx = target_nodes[i].item()\n",
    "        aggregated[idx] = torch.max(aggregated[idx].clone(), messages[i])  # avoid in-place ops on graph\n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvMean(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        messages = message_passing(x, edge_index)\n",
    "        aggregated = aggregate_mean(messages, edge_index, x.size(0))\n",
    "        updated = update_nodes(x, aggregated)\n",
    "        out = self.linear(updated)\n",
    "        return F.relu(out)\n",
    "\n",
    "class GraphConvMax(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        messages = message_passing(x, edge_index)\n",
    "        aggregated = aggregate_max(messages, edge_index, x.size(0))\n",
    "        updated = update_nodes(x, aggregated)\n",
    "        out = self.linear(updated)\n",
    "        return F.relu(out)\n",
    "\n",
    "class GNNWithAggregation(nn.Module):\n",
    "    def __init__(self, conv_layer_class, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv_layer_class(in_channels, hidden_channels)\n",
    "        self.conv2 = conv_layer_class(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sum aggregation (baseline)\n",
      "Sum agg accuracy: 0.5000\n",
      "Testing mean aggregation\n",
      "Mean agg accuracy: 1.0000\n",
      "Testing max aggregation\n",
      "Max agg accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing sum aggregation (baseline)\")\n",
    "model = SimpleGNN(2, 8, 2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "for epoch in range(20):\n",
    "    train(model, data, labels, optimizer, criterion)\n",
    "acc = evaluate(model, data, labels)\n",
    "print(f\"Sum agg accuracy: {acc:.4f}\")\n",
    "\n",
    "print(\"Testing mean aggregation\")\n",
    "model = GNNWithAggregation(GraphConvMean, 2, 8, 2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "for epoch in range(20):\n",
    "    train(model, data, labels, optimizer, criterion)\n",
    "acc = evaluate(model, data, labels)\n",
    "print(f\"Mean agg accuracy: {acc:.4f}\")\n",
    "\n",
    "print(\"Testing max aggregation\")\n",
    "model = GNNWithAggregation(GraphConvMax, 2, 8, 2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "for epoch in range(20):\n",
    "    train(model, data, labels, optimizer, criterion)\n",
    "acc = evaluate(model, data, labels)\n",
    "print(f\"Max agg accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you tune learning rates and dropout rates to improve the generalization of the GNN model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuningGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout_p):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_channels, hidden_channels)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, out_channels)\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.dropout(x, p=self.dropout_p, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout: 0.2  LR: 0.001  Accuracy: 0.5000\n",
      "Dropout: 0.2  LR: 0.010  Accuracy: 1.0000\n",
      "Dropout: 0.2  LR: 0.100  Accuracy: 0.5000\n",
      "Dropout: 0.5  LR: 0.001  Accuracy: 0.5000\n",
      "Dropout: 0.5  LR: 0.010  Accuracy: 0.5000\n",
      "Dropout: 0.5  LR: 0.100  Accuracy: 1.0000\n",
      "Dropout: 0.8  LR: 0.001  Accuracy: 0.5000\n",
      "Dropout: 0.8  LR: 0.010  Accuracy: 0.5000\n",
      "Dropout: 0.8  LR: 0.100  Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "dropouts = [0.2, 0.5, 0.8]\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "\n",
    "for dr in dropouts:\n",
    "    for lr in learning_rates:\n",
    "        model = TuningGNN(2, 8, 2, dropout_p=dr).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        for epoch in range(20):\n",
    "            train(model, data, labels, optimizer, criterion)\n",
    "        acc = evaluate(model, data, labels)\n",
    "        print(f\"Dropout: {dr:.1f}  LR: {lr:.3f}  Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
