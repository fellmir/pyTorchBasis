{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StyleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding StyleGAN](#understanding-stylegan)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Preparing the dataset](#preparing-the-dataset)\n",
    "4. [Building the mapping network](#building-the-mapping-network)\n",
    "5. [Building the Generator with style-based latent space](#building-the-generator-with-style-based-latent-space)\n",
    "6. [Building the Discriminator](#building-the-discriminator)\n",
    "7. [Implementing AdaIN for style control](#implementing-adain-for-style-control)\n",
    "8. [Initializing weights for the models](#initializing-weights-for-the-models)\n",
    "9. [Defining loss functions and optimizers](#defining-loss-functions-and-optimizers)\n",
    "10. [Training the StyleGAN](#training-the-stylegan)\n",
    "11. [Visualizing generated images and styles](#visualizing-generated-images-and-styles)\n",
    "12. [Experimenting with style mixing and interpolation](#experimenting-with-style-mixing-and-interpolation)\n",
    "13. [Evaluating the model](#evaluating-the-model)\n",
    "14. [Experimenting with hyperparameters](#experimenting-with-hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding StyleGAN\n",
    "\n",
    "StyleGAN is an advanced generative adversarial network (GAN) architecture known for producing high-quality and highly detailed images with precise control over the style and content of generated outputs. It builds on the standard GAN framework but introduces several key innovations that allow for greater flexibility and control over the generated images, particularly in the field of face generation. The architecture behind StyleGAN makes it one of the most popular models for generating realistic, detailed images with intricate structure and textures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview of GANs**\n",
    "\n",
    "As a quick recap, a typical GAN consists of two neural networks:\n",
    "- **Generator**: A network that generates synthetic images from random noise.\n",
    "- **Discriminator**: A network that evaluates the generated images, determining whether they are real (from the training set) or fake (generated).\n",
    "\n",
    "The two networks are trained simultaneously in an adversarial process where the generator tries to fool the discriminator, and the discriminator tries to accurately identify fake images. The goal is for the generator to produce increasingly realistic images that are indistinguishable from real ones.\n",
    "\n",
    "### **Key innovations of StyleGAN**\n",
    "\n",
    "While StyleGAN retains the core adversarial process of GANs, it introduces significant architectural innovations that set it apart from traditional GANs:\n",
    "\n",
    "#### **Style-based generator architecture**\n",
    "\n",
    "The generator in StyleGAN departs from the traditional input of random noise vectors directly to the network. Instead, it introduces a **style-based architecture** where the input noise vector is first transformed through a series of fully connected layers, forming a **latent code**. This latent code then influences the generation process at multiple levels through **adaptive instance normalization (AdaIN)** layers, which allow the generator to control the style of the image at various spatial resolutions.\n",
    "\n",
    "This separation between the input noise and the generation process provides greater flexibility, as the latent code modulates the style of the output. As a result, high-level aspects like pose, facial structure, or hairstyle can be controlled independently from finer details like textures or color schemes.\n",
    "\n",
    "#### **Progressive growing**\n",
    "\n",
    "StyleGAN uses a **progressive growing** technique, where the model starts by generating low-resolution images and gradually increases the resolution during training. This approach stabilizes training, as the generator and discriminator learn to model simpler, low-resolution features first and progressively handle more complex details as the resolution increases.\n",
    "\n",
    "This progressive growing strategy ensures that both networks improve their ability to handle the complexity of image generation in stages, rather than attempting to generate high-resolution images from the start.\n",
    "\n",
    "#### **Adaptive instance normalization (AdaIN)**\n",
    "\n",
    "One of the key innovations in StyleGAN is the use of **Adaptive Instance Normalization (AdaIN)**, which controls how much style information is injected into the generator at different layers. Instead of simply using the input noise to create variability, AdaIN layers adjust the mean and variance of feature maps based on the latent code, influencing both the content and the style of the generated image.\n",
    "\n",
    "AdaIN enables fine control over the generated images, allowing the model to change global features like facial structure or pose in earlier layers, while modifying local features like texture, colors, and lighting in later layers.\n",
    "\n",
    "#### **Style mixing regularization**\n",
    "\n",
    "Another important feature of StyleGAN is **style mixing regularization**, where two latent codes are mixed at different layers of the generator during training. This encourages the generator to disentangle different features of the image and prevents it from depending too much on the input noise. Style mixing improves the diversity of generated images and ensures that different layers of the generator capture different aspects of the image.\n",
    "\n",
    "By introducing multiple latent codes at different layers, the model can generate more varied and creative outputs, blending features from two different latent spaces into a single cohesive image.\n",
    "\n",
    "#### **Noise injection**\n",
    "\n",
    "To further improve the realism of generated images, StyleGAN injects random noise into the generator at various layers. This noise is not the same as the latent code; rather, it adds subtle stochastic variations to the images, such as minor texture details or irregularities that make the image look more natural.\n",
    "\n",
    "This technique allows the generator to add randomness to the finer details of the image, making it harder for the discriminator to detect that the images are synthetic. For instance, the noise injection can add slight variations in hair texture, wrinkles, or lighting conditions.\n",
    "\n",
    "### **Key advantages of StyleGAN**\n",
    "\n",
    "StyleGAN’s architecture offers several advantages over traditional GANs:\n",
    "- **Control over image style**: By separating the style from the content in the generator, StyleGAN allows users to control the generated image at multiple levels. High-level features like pose and facial structure can be altered without affecting low-level details like texture or color.\n",
    "- **Improved image quality**: Thanks to progressive growing and noise injection, StyleGAN produces high-resolution images with intricate detail, resulting in highly realistic outputs that often surpass those generated by traditional GANs.\n",
    "- **Disentangled representations**: Through style mixing and AdaIN layers, StyleGAN encourages the disentanglement of different features, making it easier to control individual aspects of the generated images, such as facial attributes, hairstyles, or lighting.\n",
    "\n",
    "### **Training StyleGAN**\n",
    "\n",
    "Training StyleGAN follows the same adversarial learning process as traditional GANs, where the generator and discriminator are trained simultaneously. However, the addition of progressive growing, style-based generation, and noise injection introduces more complexity into the training process. These techniques, especially the use of progressive growing, help stabilize training and produce better results as the model gradually learns to generate higher-resolution images.\n",
    "\n",
    "The training process still involves minimizing the generator’s loss while maximizing the discriminator’s accuracy, but the added regularization methods like style mixing and noise injection improve the generator's ability to produce diverse, high-quality images.\n",
    "\n",
    "### **Applications of StyleGAN**\n",
    "\n",
    "StyleGAN has been used in a wide range of applications due to its ability to generate high-quality and highly controllable images. Some of its key applications include:\n",
    "- **Face generation**: StyleGAN has been widely used for generating highly realistic human faces, which has led to its use in applications like virtual avatars, video games, and movie special effects.\n",
    "- **Art and design**: The ability to control the style of generated images makes StyleGAN popular in the fields of graphic design and digital art, where artists can explore new creative possibilities by blending different styles.\n",
    "- **Data augmentation**: In machine learning, StyleGAN can be used to generate synthetic data for tasks like face recognition, allowing models to train on large, diverse datasets even when real data is limited.\n",
    "- **Deepfake technology**: StyleGAN’s ability to produce realistic faces and other types of images has been used in the creation of deepfakes, both for positive creative purposes and in more controversial contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Maths**\n",
    "\n",
    "#### **GAN Objective**\n",
    "\n",
    "Like other GANs, StyleGAN is based on the adversarial process between a generator $ G $ and a discriminator $ D $. The generator aims to produce realistic images that can fool the discriminator, while the discriminator tries to correctly classify real images from fake ones. This adversarial training is formalized by the following minimax objective:\n",
    "\n",
    "$$\n",
    "\\min_G \\max_D \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ x $ is a real image sampled from the true data distribution $ p_{\\text{data}}(x) $,\n",
    "- $ z $ is a random noise vector sampled from a latent distribution $ p_z(z) $,\n",
    "- $ G(z) $ is the image generated by the generator from the noise $ z $,\n",
    "- $ D(x) $ is the discriminator’s estimate of the probability that $ x $ is real.\n",
    "\n",
    "The generator seeks to minimize this objective, meaning it tries to fool the discriminator into believing that $ G(z) $ is a real image. The discriminator, meanwhile, maximizes the objective by correctly identifying real and fake images.\n",
    "\n",
    "#### **Latent space and style transformation**\n",
    "\n",
    "In StyleGAN, the random noise vector $ z $ is not passed directly to the generator as in traditional GANs. Instead, it is first mapped to an intermediate latent space $ W $ through a learned mapping network. This transformation introduces more flexibility and better control over image generation by disentangling the input noise from the output image’s features.\n",
    "\n",
    "The mapping network transforms the input latent code $ z $ into a style vector $ w \\in W $ using a non-linear function, represented as:\n",
    "\n",
    "$$\n",
    "w = M(z; \\theta_m)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ M $ is the mapping network parameterized by $ \\theta_m $,\n",
    "- $ z $ is the input noise vector sampled from the latent distribution.\n",
    "\n",
    "The intermediate latent space $ W $ allows for more meaningful controls over the generated image, as the vector $ w $ represents the style information injected into various layers of the generator.\n",
    "\n",
    "#### **AdaIN (Adaptive Instance Normalization)**\n",
    "\n",
    "One of the key innovations of StyleGAN is the use of **Adaptive Instance Normalization (AdaIN)** to inject the style vector $ w $ into the generator. AdaIN modifies the mean and variance of the feature maps in the generator based on the style vector. The AdaIN operation is defined as:\n",
    "\n",
    "$$\n",
    "\\text{AdaIN}(h, w) = \\sigma(w) \\left( \\frac{h - \\mu(h)}{\\sigma(h)} \\right) + \\mu(w)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ h $ is the feature map from the generator,\n",
    "- $ \\mu(h) $ and $ \\sigma(h) $ are the mean and standard deviation of the feature map $ h $,\n",
    "- $ \\mu(w) $ and $ \\sigma(w) $ are learned affine transformations from the style vector $ w $.\n",
    "\n",
    "AdaIN adjusts the statistics of the feature map $ h $ to match the style defined by $ w $, which allows the generator to control different aspects of the generated image (such as texture, color, or shape) at various layers of the network. This provides fine-grained control over the generated output.\n",
    "\n",
    "#### **Style mixing regularization**\n",
    "\n",
    "To further encourage disentanglement of different image features, StyleGAN introduces **style mixing regularization**. This technique randomly selects two latent codes $ w_1 $ and $ w_2 $ from the intermediate latent space $ W $ and injects them at different layers of the generator. The goal is to prevent the generator from relying too heavily on the input noise for image features.\n",
    "\n",
    "Let $ w_1 $ be the style vector applied to the first part of the network, and $ w_2 $ be the style vector applied to the second part of the network. Style mixing can be represented as:\n",
    "\n",
    "$$\n",
    "\\tilde{h}_l = \n",
    "\\begin{cases}\n",
    "\\text{AdaIN}(h_l, w_1), & \\text{if layer } l \\leq L \\\\\n",
    "\\text{AdaIN}(h_l, w_2), & \\text{if layer } l > L\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where $ L $ is a randomly selected layer at which the style vectors switch. This mixing process ensures that different layers in the generator capture different features, promoting the disentanglement of high-level and low-level attributes.\n",
    "\n",
    "#### **Noise injection**\n",
    "\n",
    "StyleGAN also adds stochastic variation to the generated images by injecting noise into each layer of the generator. This noise is independent of the latent code and is applied at various layers to introduce subtle variations, such as texture details or small changes in lighting.\n",
    "\n",
    "The noise is applied additively to the feature maps:\n",
    "\n",
    "$$\n",
    "h_l' = h_l + n_l\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ h_l $ is the feature map at layer $ l $,\n",
    "- $ n_l $ is the noise injected at layer $ l $.\n",
    "\n",
    "This noise injection allows for randomness in the fine details of the generated images, giving the output more natural variation, such as minor texture or background differences, without affecting the global structure controlled by the latent code.\n",
    "\n",
    "#### **Progressive growing**\n",
    "\n",
    "StyleGAN leverages the idea of **progressive growing**, where the model starts by generating low-resolution images and gradually increases the resolution as training progresses. This allows the generator and discriminator to learn simpler structures at lower resolutions before tackling the complexity of high-resolution images.\n",
    "\n",
    "At each stage, the output resolution increases by adding new layers to the generator and discriminator. This growing process ensures that the networks progressively learn to model high-level features (such as shapes) before moving on to finer details (such as textures). As a result, the generator can produce higher-quality images with detailed structure and variation.\n",
    "\n",
    "#### **Training objectives**\n",
    "\n",
    "The training process in StyleGAN is adversarial, just like in standard GANs, but with additional regularizations, such as **style mixing** and **noise injection**, to ensure better feature disentanglement and image diversity.\n",
    "\n",
    "The generator is optimized to produce images that fool the discriminator, while the discriminator is optimized to correctly classify real versus generated images. The adversarial loss drives both networks to improve iteratively. During training, StyleGAN also benefits from techniques such as **R1 regularization**, which penalizes large gradients in the discriminator to ensure smoother and more stable training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for building and training StyleGAN in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules for constructing the generator, discriminator, and for handling the dataset in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you set up the environment to utilize GPU for training StyleGAN models in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you load a high-resolution image dataset (e.g., CelebA-HQ) using `torchvision.datasets` in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you apply transformations like resizing, normalization, and converting images to tensors using `torchvision.transforms`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you create a DataLoader in PyTorch to load batches of images for training StyleGAN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the mapping network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you define the architecture of the mapping network using `torch.nn.Module` in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you implement the forward pass in the mapping network to map the latent vector $ z $ to the intermediate latent space $ w $?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Generator with style-based latent space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you define the generator architecture that uses the intermediate latent space $ w $ as input?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you use transposed convolutional layers in the generator to progressively generate images from low to high resolution?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you implement the forward pass of the generator to apply the style information at different layers during image generation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Discriminator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you define the architecture of the discriminator to classify images as real or fake?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you use convolutional layers in the discriminator to progressively downsample input images?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you implement the forward pass in the discriminator to output the probability that the input image is real?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaIN for style control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you implement Adaptive Instance Normalization (AdaIN) in the generator to control the style at different stages of image generation?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you apply the style vector using AdaIN to modulate the activations in the generator?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing weights for the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you define a custom weight initialization function for the generator and discriminator models?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you apply the weight initialization to both the generator and discriminator models in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining loss functions and optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you define the loss function for the discriminator using binary cross-entropy (BCE) loss?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you define the loss function for the generator based on how well it fools the discriminator?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you set up the Adam optimizer for the generator and discriminator models in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the StyleGAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you implement the training loop for StyleGAN, alternating between updating the generator and the discriminator?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you compute the loss for the discriminator using both real and generated images during each training step?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you compute the loss for the generator based on the feedback from the discriminator?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you update the weights of the generator and discriminator using backpropagation during training?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing generated images and styles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you generate and visualize images from the generator at different stages of training to monitor progress?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q27: How do you visualize and compare the effects of different styles applied to generated images using the latent vector $ w $?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q28: How do you save the generated images during training to observe the progression of the model’s output over time?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with style mixing and interpolation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q29: How do you implement style mixing by combining two latent vectors and generating images with features from both styles?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q30: How do you interpolate between different latent codes to visualize the transition between styles in generated images?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q31: How do you evaluate the quality of images generated by the StyleGAN model after a certain number of training epochs?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q32: How do you use Frechet Inception Distance (FID) or other metrics to quantitatively evaluate the quality of generated images?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q33: How do you experiment with different latent vector sizes to observe the effect on the quality of generated images?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q34: How do you adjust the learning rates for the generator and discriminator to stabilize the training process?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q35: How do you experiment with different architectures for the generator and discriminator to improve image quality and training stability?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
