{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph neural network (GNN) basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding graph neural networks (GNNs)](#understanding-graph-neural-networks-gnns)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Defining graph data](#defining-graph-data)\n",
    "4. [Building a basic message-passing mechanism](#building-a-basic-message-passing-mechanism)\n",
    "5. [Implementing a simple graph convolution layer](#implementing-a-simple-graph-convolution-layer)\n",
    "6. [Building a basic GNN model](#building-a-basic-gnn-model)\n",
    "7. [Training the GNN on a node classification task](#training-the-gnn-on-a-node-classification-task)\n",
    "8. [Evaluating the GNN model](#evaluating-the-gnn-model)\n",
    "9. [Experimenting with different configurations](#experimenting-with-different-configurations)\n",
    "10. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding graph neural networks (GNNs)\n",
    "\n",
    "Graph Neural Networks (GNNs) are a class of neural networks designed to operate on graph-structured data. Unlike traditional neural networks that work on structured data such as images (grids) or text (sequences), GNNs are capable of processing data in the form of graphs, where information is represented as a collection of nodes and edges. These networks have become essential in various domains, including social networks, knowledge graphs, recommendation systems, and biological networks, as they can effectively capture the relationships and dependencies between entities (nodes) connected by links (edges)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding graph data**\n",
    "\n",
    "A graph consists of two primary components:\n",
    "- **Nodes (vertices)**: These represent the entities in the graph. For instance, in a social network, each node could represent an individual.\n",
    "- **Edges**: These are the connections between nodes, representing the relationships or interactions between entities. In a social network, an edge might represent a friendship or connection between two individuals.\n",
    "\n",
    "Graphs can be directed or undirected. In a directed graph, edges have a direction (e.g., a one-way relationship), while in an undirected graph, edges indicate mutual connections. Additionally, graphs can have weighted edges, where the weights capture the strength or intensity of the connection between nodes.\n",
    "\n",
    "### **Key challenges with graph data**\n",
    "\n",
    "Unlike traditional data structures, graph data can be irregular, meaning that each node can have a varying number of connections (neighbors). This irregularity makes applying standard deep learning techniques, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs), to graphs more difficult. GNNs address this challenge by learning how to aggregate and propagate information across the nodes and edges of the graph in a way that preserves the graph's structure and the relationships between entities.\n",
    "\n",
    "### **How GNNs work**\n",
    "\n",
    "GNNs work by learning to propagate information between nodes through the graph's edges. The idea is to iteratively update the representation (or embedding) of each node by considering its own features as well as the features of its neighboring nodes. This process captures both the local and global structure of the graph, allowing the model to learn a rich representation of each node in the context of its connections.\n",
    "\n",
    "The key operation in GNNs is the **message passing** process, where each node gathers information from its neighbors, aggregates it, and updates its representation. This process can be repeated over multiple layers of the network, allowing the nodes to incorporate information from farther parts of the graph as the depth of the network increases.\n",
    "\n",
    "### **Message passing and node aggregation**\n",
    "\n",
    "During message passing, each node in the graph sends information to its neighbors and receives information from them. This exchange of information allows the nodes to update their features or embeddings based on their neighbors' features. The aggregation process can take many forms, such as summing, averaging, or taking the maximum of the neighboring nodes' features. The updated representation of a node reflects both its own features and the features of its local neighborhood.\n",
    "\n",
    "This iterative aggregation enables GNNs to capture the local structure of the graph while progressively considering information from farther nodes in the graph. With multiple layers of aggregation, a node can gain information from a broader context, eventually representing both its immediate neighborhood and the more distant nodes connected to it.\n",
    "\n",
    "### **Types of GNNs**\n",
    "\n",
    "There are several types of GNN architectures, each designed to handle different types of graph-based tasks:\n",
    "\n",
    "- **Graph Convolutional Networks (GCNs)**: GCNs are the most well-known type of GNN and are based on applying a convolution-like operation to graphs. Each layer of a GCN aggregates information from neighboring nodes and updates the node embeddings. This architecture is particularly useful for tasks like node classification, where the goal is to predict a label for each node in the graph.\n",
    "- **Graph Attention Networks (GATs)**: GATs extend GCNs by incorporating attention mechanisms. Instead of treating all neighbors equally, GATs assign different weights to each neighbor based on its importance, allowing the model to focus on the most relevant connections when aggregating information.\n",
    "- **Graph Recurrent Neural Networks (GRNNs)**: These networks are designed to handle dynamic or temporal graphs, where the graph structure or node features change over time. GRNNs leverage the power of recurrent networks to model these temporal dependencies while processing graph data.\n",
    "- **Graph Autoencoders**: Graph autoencoders are used for unsupervised learning tasks such as link prediction or graph generation. These networks encode the graph into a lower-dimensional representation and then attempt to reconstruct the graph from that representation, learning a compressed version of the graph's structure in the process.\n",
    "\n",
    "### **Applications of GNNs**\n",
    "\n",
    "GNNs are used in a wide variety of tasks where data can be represented as graphs. Some common applications include:\n",
    "\n",
    "- **Social networks**: In social networks, users (nodes) are connected through friendships, interactions, or shared interests (edges). GNNs can be used to predict connections (link prediction), recommend friends, or classify users based on their network behaviors.\n",
    "- **Recommendation systems**: In recommendation systems, GNNs can be used to model interactions between users and items. The graph structure can represent users, products, and their interactions, allowing GNNs to predict user preferences and improve recommendation accuracy.\n",
    "- **Biological networks**: GNNs are used to model protein-protein interactions, molecular structures, or gene regulatory networks, where nodes represent biological entities and edges represent interactions. GNNs can help predict how certain proteins or genes influence one another, aiding in drug discovery and disease modeling.\n",
    "- **Knowledge graphs**: In knowledge graphs, entities (nodes) are connected by relationships (edges) that represent facts or links between concepts. GNNs can be used to perform tasks such as knowledge graph completion (predicting missing links between entities) or entity classification.\n",
    "\n",
    "### **Advantages of GNNs**\n",
    "\n",
    "GNNs offer several advantages that make them suitable for tasks involving graph-structured data:\n",
    "- **Generalization to varying graph structures**: Unlike traditional neural networks that require fixed input sizes, GNNs can handle graphs with varying numbers of nodes and edges, making them highly flexible.\n",
    "- **Capturing local and global dependencies**: GNNs effectively capture both local and global relationships in graph data by iteratively aggregating information across the graph.\n",
    "- **Scalability**: While GNNs can handle large graphs, various optimizations, such as sampling techniques, make them more scalable to even larger graphs, ensuring they remain efficient for real-world tasks.\n",
    "\n",
    "### **Challenges in GNNs**\n",
    "\n",
    "Despite their success, GNNs also face some challenges:\n",
    "- **Over-smoothing**: As the number of GNN layers increases, node representations may become indistinguishable from one another, a phenomenon known as over-smoothing. This limits the depth of GNNs and their ability to model long-range dependencies effectively.\n",
    "- **Scalability**: While GNNs are efficient for small to medium-sized graphs, scaling them to very large graphs can be computationally intensive. Techniques like graph sampling or mini-batch processing are often used to address this challenge.\n",
    "- **Data sparsity**: In some cases, graphs may have sparse connections, making it harder for GNNs to effectively propagate information. This can affect the model's ability to capture meaningful patterns from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Maths**\n",
    "\n",
    "#### **Graph structure**\n",
    "\n",
    "A graph $ G = (V, E) $ consists of a set of nodes (or vertices) $ V $ and edges $ E $. Each node $ v \\in V $ can have its own feature vector $ x_v $, and each edge $ (u, v) \\in E $ may have an associated weight $ w_{uv} $, representing the strength of the connection between nodes $ u $ and $ v $.\n",
    "\n",
    "The structure of a graph can be represented by an **adjacency matrix** $ A \\in \\mathbb{R}^{N \\times N} $, where $ N $ is the number of nodes in the graph. Each entry $ A_{ij} $ in the matrix is 1 if there is an edge between nodes $ i $ and $ j $, and 0 otherwise. In weighted graphs, $ A_{ij} $ represents the weight of the edge between nodes $ i $ and $ j $.\n",
    "\n",
    "#### **Graph convolution**\n",
    "\n",
    "Graph Neural Networks generalize the concept of convolution to graph data by learning to aggregate information from neighboring nodes. The key operation in GNNs is the graph convolution, which updates the feature representation of each node based on its own features and the features of its neighbors.\n",
    "\n",
    "The most basic form of graph convolution is represented as:\n",
    "\n",
    "$$\n",
    "h_v^{(l+1)} = \\sigma \\left( \\sum_{u \\in \\mathcal{N}(v)} \\frac{1}{c_{vu}} W^{(l)} h_u^{(l)} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ h_v^{(l+1)} $ is the updated feature representation of node $ v $ at layer $ l+1 $,\n",
    "- $ \\mathcal{N}(v) $ denotes the set of neighbors of node $ v $,\n",
    "- $ W^{(l)} $ is a learnable weight matrix for layer $ l $,\n",
    "- $ c_{vu} $ is a normalization constant, which can be the degree of node $ v $ or a function of the graph structure, used to normalize the contribution from neighboring nodes,\n",
    "- $ \\sigma $ is a non-linear activation function, such as ReLU.\n",
    "\n",
    "This equation describes how each node updates its representation by aggregating the features of its neighbors, weighted by the adjacency structure of the graph.\n",
    "\n",
    "#### **Message passing framework**\n",
    "\n",
    "The **message passing** framework is a general way to describe how GNNs operate. In each layer of a GNN, nodes exchange \"messages\" with their neighbors, and the information received from neighbors is aggregated to update the node's own representation.\n",
    "\n",
    "The message passing process can be described in two steps:\n",
    "1. **Message computation**: For each edge $ (u, v) \\in E $, a message is computed based on the features of node $ u $ and node $ v $. The message can also depend on the edge features if they are present.\n",
    "2. **Message aggregation**: For each node $ v $, the messages from all its neighbors are aggregated to update its feature representation.\n",
    "\n",
    "Mathematically, the message passing process at layer $ l $ can be written as:\n",
    "\n",
    "$$\n",
    "m_v^{(l)} = \\sum_{u \\in \\mathcal{N}(v)} M(h_u^{(l)}, h_v^{(l)}, e_{uv}; W^{(l)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_v^{(l+1)} = U(h_v^{(l)}, m_v^{(l)}; W^{(l)})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ M $ is a message function that computes the message from node $ u $ to node $ v $,\n",
    "- $ e_{uv} $ represents the edge features between nodes $ u $ and $ v $ (if applicable),\n",
    "- $ m_v^{(l)} $ is the aggregated message for node $ v $,\n",
    "- $ U $ is an update function that combines the current node features $ h_v^{(l)} $ with the aggregated messages $ m_v^{(l)} $,\n",
    "- $ W^{(l)} $ are the learnable parameters at layer $ l $.\n",
    "\n",
    "The update function typically includes a non-linear transformation, ensuring that the GNN can learn complex relationships in the graph.\n",
    "\n",
    "#### **Graph convolutional networks (GCNs)**\n",
    "\n",
    "Graph Convolutional Networks (GCNs) are a popular type of GNN that apply a convolutional operation to graph-structured data. The propagation rule for GCNs can be written as:\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = \\sigma \\left( \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} H^{(l)} W^{(l)} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ H^{(l)} \\in \\mathbb{R}^{N \\times d} $ is the matrix of node feature vectors at layer $ l $, where $ N $ is the number of nodes and $ d $ is the dimensionality of the node features,\n",
    "- $ \\tilde{A} = A + I $ is the adjacency matrix with added self-loops (identity matrix $ I $),\n",
    "- $ \\tilde{D} $ is the degree matrix of $ \\tilde{A} $,\n",
    "- $ W^{(l)} $ is the weight matrix for layer $ l $,\n",
    "- $ \\sigma $ is a non-linear activation function (e.g., ReLU).\n",
    "\n",
    "The normalization term $ \\tilde{D}^{-\\frac{1}{2}} \\tilde{A} \\tilde{D}^{-\\frac{1}{2}} $ ensures that the feature aggregation from neighbors is properly scaled, preventing the features from growing or shrinking too much as they propagate through the graph.\n",
    "\n",
    "#### **Graph attention networks (GATs)**\n",
    "\n",
    "In Graph Attention Networks (GATs), attention mechanisms are used to assign different weights to the neighboring nodes during the aggregation process. Instead of treating all neighbors equally, GATs compute an attention score for each neighbor, which determines its contribution to the node's updated representation.\n",
    "\n",
    "The attention mechanism in GATs is typically implemented as a function of the features of two connected nodes. The attention score between nodes $ u $ and $ v $ is computed as:\n",
    "\n",
    "$$\n",
    "\\alpha_{uv} = \\frac{\\exp \\left( \\text{LeakyReLU} \\left( a^T [W h_u \\| W h_v] \\right) \\right)}{\\sum_{k \\in \\mathcal{N}(v)} \\exp \\left( \\text{LeakyReLU} \\left( a^T [W h_v \\| W h_k] \\right) \\right)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ a $ is the attention weight vector,\n",
    "- $ W $ is the weight matrix for the node features,\n",
    "- $ \\| $ denotes concatenation of the feature vectors of nodes $ u $ and $ v $,\n",
    "- $ \\alpha_{uv} $ is the normalized attention score for node $ u $ relative to node $ v $.\n",
    "\n",
    "This attention mechanism allows the model to focus on more relevant neighbors while ignoring less important ones.\n",
    "\n",
    "#### **Challenges: Over-smoothing**\n",
    "\n",
    "A common challenge in GNNs is the problem of **over-smoothing**, which occurs when node representations become too similar after multiple layers of aggregation. As the number of layers increases, the information from neighboring nodes mixes more extensively, eventually leading to indistinguishable node representations. This limits the depth of GNNs, as deeper networks may fail to capture meaningful distinctions between nodes.\n",
    "\n",
    "To mitigate over-smoothing, various techniques such as residual connections, normalization layers, or modified aggregation schemes can be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for building a GNN in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules for constructing a GNN and handling graph data in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you configure the environment to use GPU for training the GNN model in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining graph data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you represent a graph using an adjacency matrix in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you define node features for each node in the graph as input to the GNN?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you convert graph edges into an edge list to represent the connections between nodes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a basic message-passing mechanism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you implement a basic message-passing mechanism between neighboring nodes in a graph?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you aggregate messages from neighboring nodes using operations like summation or averaging in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you implement node updates by combining aggregated messages with the node's own features?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a simple graph convolution layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you define a simple graph convolution layer using `torch.nn.Module` in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you implement the forward pass of the graph convolution layer to compute new node embeddings?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you apply a non-linearity, such as ReLU, after computing the graph convolution to update node features?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a basic GNN model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you stack multiple graph convolution layers to build a simple GNN model in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you define the forward pass of the GNN model to process node features through multiple graph convolution layers?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you implement dropout and batch normalization in the GNN model to improve generalization?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the GNN on a node classification task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you define the loss function (e.g., `CrossEntropyLoss`) for training the GNN model on a node classification task?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you set up the optimizer (e.g., Adam) to update the GNN model parameters during training?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you implement the training loop for the GNN, including the forward pass, loss computation, and backpropagation?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you track and log the accuracy and loss over training epochs to monitor the GNN model’s performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the GNN model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you evaluate the GNN model on a validation or test dataset and calculate its accuracy for node classification?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you implement a function to perform inference using the trained GNN model on new graph data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with different configurations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you experiment with different numbers of graph convolution layers and observe the effect on model performance?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you adjust the hidden dimension size in the GNN layers to analyze its impact on training time and accuracy?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you experiment with different aggregation functions (e.g., sum, mean, max) in the message-passing mechanism?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you tune learning rates and dropout rates to improve the generalization of the GNN model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
