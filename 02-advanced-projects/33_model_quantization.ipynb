{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model quantization in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding model quantization](#understanding-model-quantization)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Quantization techniques overview](#quantization-techniques-overview)\n",
    "4. [Applying dynamic quantization](#applying-dynamic-quantization)\n",
    "5. [Applying static quantization](#applying-static-quantization)\n",
    "6. [Performing quantization-aware training](#performing-quantization-aware-training)\n",
    "7. [Evaluating quantized models](#evaluating-quantized-models)\n",
    "8. [Comparing performance and memory usage](#comparing-performance-and-memory-usage)\n",
    "9. [Experimenting with different quantization techniques](#experimenting-with-different-quantization-techniques)\n",
    "10. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding model quantization\n",
    "\n",
    "**Model quantization** is a technique used to reduce the size of a neural network model by representing its weights and activations with lower precision. This is done to decrease the memory footprint and computational requirements of the model without sacrificing too much accuracy. Quantization is particularly useful when deploying models on edge devices, mobile phones, or any environment with limited resources where the full precision model would be too large or slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is quantization?**\n",
    "\n",
    "Quantization involves converting a model's parameters (weights) and activations from floating-point precision (typically 32-bit floating point) to lower-bit representations, such as 8-bit integers. This transformation reduces the amount of memory needed to store the model and the computations required for inference, allowing the model to run more efficiently on hardware like CPUs, GPUs, and specialized accelerators.\n",
    "\n",
    "Quantization is effective because deep learning models often do not require the full precision of 32-bit floats to make accurate predictions. By reducing the precision of the weights and activations, the model can be made smaller and faster while still maintaining a high level of accuracy for most tasks.\n",
    "\n",
    "### **Why use model quantization?**\n",
    "\n",
    "Quantization is particularly beneficial for several reasons:\n",
    "- **Reduced memory footprint**: By using lower precision, the size of the model is significantly reduced, allowing it to fit into smaller memory environments.\n",
    "- **Faster inference**: Lower precision computations are faster, which can lead to improved inference times, especially on devices where computational power is limited.\n",
    "- **Energy efficiency**: Reduced precision computations consume less power, which is especially important for battery-powered devices like mobile phones and IoT devices.\n",
    "\n",
    "### **Types of quantization in PyTorch**\n",
    "\n",
    "PyTorch provides several types of quantization techniques, each suited to different scenarios:\n",
    "\n",
    "#### **Post-training quantization**\n",
    "Post-training quantization is applied after the model has been trained. It does not require modifying the training process and can be done by simply converting the trained model to a lower precision. This method is straightforward and works well for models that are already robust to small reductions in precision.\n",
    "\n",
    "There are two main approaches under post-training quantization:\n",
    "- **Dynamic quantization**: Only specific parts of the model, such as weights, are quantized to a lower precision, while other parts (e.g., activations) remain in full precision. Dynamic quantization is particularly useful for models that involve a lot of matrix multiplications, such as transformer-based architectures.\n",
    "- **Static quantization**: Both the weights and activations are quantized. Before this happens, the model is calibrated with sample inputs to determine the range of activations, which allows for more efficient quantization. Static quantization generally offers more performance improvements than dynamic quantization but requires some additional setup.\n",
    "\n",
    "#### **Quantization-aware training (QAT)**\n",
    "Quantization-aware training involves simulating the effects of quantization during the training process, allowing the model to learn how to handle lower precision computations. This method provides higher accuracy than post-training quantization, as the model is explicitly trained to minimize the loss associated with the quantization process.\n",
    "\n",
    "In QAT, the model is trained using floating-point precision, but quantization is simulated by inserting fake quantization nodes in the computation graph. This way, the model learns to adjust its weights during training to compensate for the lower precision. Once the model is trained, it can be converted to an 8-bit version for deployment.\n",
    "\n",
    "QAT is particularly effective for scenarios where the accuracy drop from post-training quantization is too high, and it is essential to minimize the loss in performance.\n",
    "\n",
    "### **The quantization workflow in PyTorch**\n",
    "\n",
    "The typical quantization workflow in PyTorch consists of the following steps:\n",
    "\n",
    "- **Model preparation**: Start with a pre-trained or freshly trained model. Depending on the type of quantization, different preparations are required. For dynamic quantization, minimal changes are needed, while static quantization and QAT require additional steps such as inserting quantization operations and calibrating the model with sample data.\n",
    "- **Calibrating the model**: In static quantization, sample inputs are fed through the model to calculate the range of activations. This step helps to determine the scaling factors and offsets needed to convert activations to lower precision.\n",
    "- **Applying quantization**: Once the model is calibrated or prepared, quantization can be applied to convert weights and activations to their lower-precision representations.\n",
    "- **Model conversion**: The final step involves converting the quantized model into an inference-optimized format, where it can be deployed on hardware that supports lower precision computations.\n",
    "\n",
    "### **Benefits and trade-offs of model quantization**\n",
    "\n",
    "While quantization brings clear advantages, there are trade-offs to consider:\n",
    "- **Accuracy drop**: Quantization can lead to a slight reduction in accuracy because lower precision representations may introduce small errors in the computations. The severity of this drop depends on the model and the type of quantization used. In most cases, the performance trade-off is negligible, but for very sensitive tasks, it may become more pronounced.\n",
    "- **Limited hardware support**: Not all hardware supports low-precision computation natively. Devices such as certain CPUs, GPUs, and accelerators may require specific frameworks or libraries to take full advantage of quantized models.\n",
    "- **Complexity in setup**: Techniques like quantization-aware training require more setup and configuration compared to standard post-training quantization. This added complexity may not always be worth it, depending on the target use case.\n",
    "\n",
    "### **Common applications of model quantization**\n",
    "\n",
    "Quantization is widely used in many real-world applications, especially when deploying deep learning models to resource-constrained environments. Some typical use cases include:\n",
    "- **Mobile applications**: Deploying deep learning models on smartphones requires optimized models that run quickly without consuming too much battery or memory.\n",
    "- **Embedded systems and IoT devices**: These devices often have very limited computational and memory resources, making quantization a necessity for deploying AI models effectively.\n",
    "- **Edge computing**: In edge environments, models need to perform real-time inference efficiently, often under tight hardware constraints. Quantization helps reduce the latency and improve performance in such scenarios.\n",
    "- **Cloud deployment**: Even in cloud environments, where resources are more abundant, quantized models can save significant costs by reducing computational load and speeding up inference times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Maths**\n",
    "\n",
    "#### **Quantization basics**\n",
    "\n",
    "Quantization maps a floating-point number $ x $ into a lower-precision integer representation $ x_q $. The basic equation for quantization is:\n",
    "\n",
    "$$\n",
    "x_q = \\text{round}\\left(\\frac{x - z}{s}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ x $ is the floating-point number,\n",
    "- $ x_q $ is the quantized integer,\n",
    "- $ s $ is the scaling factor (step size),\n",
    "- $ z $ is the zero-point (the quantized value that corresponds to zero in the floating-point range).\n",
    "\n",
    "To convert the quantized value $ x_q $ back to its approximate floating-point equivalent, the following equation is used:\n",
    "\n",
    "$$\n",
    "x \\approx s \\cdot x_q + z\n",
    "$$\n",
    "\n",
    "This process introduces quantization error due to the rounding step, but it allows the model to be more efficient in terms of memory usage and computation.\n",
    "\n",
    "#### **Dynamic quantization**\n",
    "\n",
    "In dynamic quantization, only the model’s weights are quantized, while activations remain in full precision during inference. The transformation for each weight $ w $ from floating-point precision to its quantized representation $ w_q $ is:\n",
    "\n",
    "$$\n",
    "w_q = \\text{round}\\left(\\frac{w - z_w}{s_w}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ w $ is the floating-point weight,\n",
    "- $ w_q $ is the quantized integer weight,\n",
    "- $ s_w $ and $ z_w $ are the scale and zero-point for the weights.\n",
    "\n",
    "During inference, the matrix multiplication between quantized weights and full-precision activations is approximated as:\n",
    "\n",
    "$$\n",
    "y = W_q \\cdot x + z_w\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W_q $ is the quantized weight matrix,\n",
    "- $ x $ is the full-precision activation matrix,\n",
    "- $ z_w $ is the zero-point adjustment for the weights.\n",
    "\n",
    "#### **Static quantization**\n",
    "\n",
    "In static quantization, both weights and activations are quantized. The conversion of activations and weights is as follows:\n",
    "\n",
    "For weights:\n",
    "$$\n",
    "w_q = \\text{round}\\left(\\frac{w - z_w}{s_w}\\right)\n",
    "$$\n",
    "\n",
    "For activations:\n",
    "$$\n",
    "a_q = \\text{round}\\left(\\frac{a - z_a}{s_a}\\right)\n",
    "$$\n",
    "\n",
    "The forward pass during inference is computed as:\n",
    "\n",
    "$$\n",
    "y_q = W_q \\cdot a_q\n",
    "$$\n",
    "\n",
    "The output is converted back to floating-point precision by applying the inverse quantization formula:\n",
    "\n",
    "$$\n",
    "y = s_y \\cdot y_q + z_y\n",
    "$$\n",
    "\n",
    "Where $ s_y $ and $ z_y $ are the scale and zero-point for the output.\n",
    "\n",
    "#### **Quantization-aware training (QAT)**\n",
    "\n",
    "Quantization-aware training simulates quantization during training to minimize the loss of accuracy caused by quantization. The quantized weights $ w_q $ and activations $ a_q $ are used during the forward pass, but gradients are calculated with respect to the original floating-point weights $ w $ and activations $ a $:\n",
    "\n",
    "$$\n",
    "\\hat{y} = W_q \\cdot a_q\n",
    "$$\n",
    "\n",
    "During backpropagation, the gradients $ \\nabla \\mathcal{L}(\\theta) $ are computed based on the original (non-quantized) parameters $ \\theta $, allowing the model to learn how to handle the errors introduced by quantization:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta \\mathcal{L}(\\hat{y})\n",
    "$$\n",
    "\n",
    "This method allows the model to adjust to the effects of quantization during training, leading to better performance when deployed with quantization.\n",
    "\n",
    "#### **Scaling and zero-point calculation**\n",
    "\n",
    "Quantization requires determining the scale $ s $ and zero-point $ z $ for each layer’s weights and activations. These parameters are computed based on the observed range of values during the calibration phase.\n",
    "\n",
    "The scale $ s $ is calculated as:\n",
    "\n",
    "$$\n",
    "s = \\frac{\\text{max}(x) - \\text{min}(x)}{2^n - 1}\n",
    "$$\n",
    "\n",
    "Where $ 2^n $ represents the range of the target quantized format (for 8-bit integers, $ n = 8 $).\n",
    "\n",
    "The zero-point $ z $ is the integer value that corresponds to zero in the original floating-point range:\n",
    "\n",
    "$$\n",
    "z = \\text{round}\\left(\\frac{-\\text{min}(x)}{s}\\right)\n",
    "$$\n",
    "\n",
    "#### **Quantization error**\n",
    "\n",
    "Quantization introduces a small error due to the loss of precision. The quantization error for a single value $ x $ is defined as:\n",
    "\n",
    "$$\n",
    "e_q = x - (s \\cdot x_q + z)\n",
    "$$\n",
    "\n",
    "This error arises from rounding the floating-point number to the nearest integer and depends on the precision of the quantized format. However, quantization-aware training helps mitigate this error by teaching the model to compensate for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for model quantization in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules for quantization, profiling, and model evaluation in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you configure the environment to test quantized models on both CPU and GPU in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization techniques overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you check which quantization methods are available in your version of PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you verify that your hardware supports quantized operations in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying dynamic quantization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you apply dynamic quantization to a pre-trained PyTorch model using `torch.quantization.quantize_dynamic`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you specify which layers (e.g., `nn.Linear`, `nn.LSTM`) to quantize dynamically in your model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you save and load a dynamically quantized model in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you measure the inference time of the model before and after applying dynamic quantization?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying static quantization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you prepare a pre-trained model for static quantization using `torch.quantization.prepare`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you calibrate the prepared model with a representative dataset for static quantization?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you convert the calibrated model to a statically quantized model using `torch.quantization.convert`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you modify your model to insert quantization and dequantization layers required for static quantization?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you save and load a statically quantized model in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing quantization-aware training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you prepare your model for quantization-aware training using `torch.quantization.prepare_qat`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you modify your training loop to accommodate quantization-aware training in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you fine-tune a model with quantization-aware training to minimize accuracy loss after quantization?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you convert the quantization-aware trained model into a quantized model using `torch.quantization.convert`?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating quantized models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you evaluate the accuracy of the quantized model on a test dataset and compare it with the original model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you measure the inference speed and memory usage of the quantized model compared to the full-precision model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing performance and memory usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you create a summary table comparing model size, inference time, and accuracy between the original and quantized models?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you visualize the performance improvements of quantized models using graphs or charts in Python?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with different quantization techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you selectively apply quantization to specific layers, such as quantizing convolutional layers but leaving batch normalization layers in full precision?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you experiment with different quantization configurations, like per-tensor versus per-channel quantization, and observe their effects on model performance?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you implement hybrid quantization by combining dynamic and static quantization techniques within the same model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you test the impact of quantization on different types of models (e.g., CNNs, LSTMs, Transformers) using PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q27: How do you change the quantization backend (e.g., from 'fbgemm' to 'qnnpack') and assess its impact on model performance and compatibility?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q28: How do you enable quantization on custom modules or layers not directly supported by PyTorch's quantization API?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q29: How do you perform post-training quantization on a model that was initially trained using mixed precision?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q30: How do you write unit tests to verify that the outputs of the quantized model are within acceptable tolerances compared to the original model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
