{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer models in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Understanding transformer models](#understanding-transformer-models)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Defining the input data](#defining-the-input-data)\n",
    "4. [Implementing positional encoding](#implementing-positional-encoding)\n",
    "5. [Building the scaled dot-product attention mechanism](#building-the-scaled-dot-product-attention-mechanism)\n",
    "6. [Implementing multi-head attention](#implementing-multi-head-attention)\n",
    "7. [Building the feed-forward network](#building-the-feed-forward-network)\n",
    "8. [Constructing the transformer encoder](#constructing-the-transformer-encoder)\n",
    "9. [Training the transformer model](#training-the-transformer-model)\n",
    "10. [Evaluating the transformer model](#evaluating-the-transformer-model)\n",
    "11. [Experimenting with different transformer configurations](#experimenting-with-different-transformer-configurations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding transformer models\n",
    "\n",
    "The Transformer model is a groundbreaking architecture that has revolutionized natural language processing (NLP) and other sequence-based tasks. It was introduced in the paper \"Attention is All You Need\" and has since become the foundation for many state-of-the-art models, such as BERT and GPT. Unlike previous architectures like recurrent neural networks (RNNs) and long short-term memory (LSTM) networks, the Transformer does not rely on recurrence or convolutions. Instead, it uses an attention mechanism to process input data in parallel, which leads to more efficient training and better performance on long-range dependencies in sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Why Transformers?**\n",
    "\n",
    "Traditional models like RNNs and LSTMs process sequences one element at a time, which makes it difficult to capture long-range dependencies and slows down the training process. Transformers solve these problems by processing the entire input sequence at once using the attention mechanism. This parallelization makes the model faster to train and more effective at capturing global dependencies, regardless of the sequence length.\n",
    "\n",
    "The key innovation of Transformers is the use of **self-attention**, which allows the model to weigh the importance of each token (word or element) in the sequence relative to every other token, thus providing a better understanding of context and relationships between tokens.\n",
    "\n",
    "### **Key components of Transformer models**\n",
    "\n",
    "The Transformer architecture consists of two main components: the **encoder** and the **decoder**. Each of these components is made up of several layers of attention and feedforward neural networks. However, many tasks, like language understanding (e.g., BERT), use only the encoder, while others (e.g., GPT) use only the decoder.\n",
    "\n",
    "#### **Encoder**\n",
    "\n",
    "The encoder processes the input sequence and produces a contextual representation for each token. It consists of multiple layers, each containing:\n",
    "- **Self-attention mechanism**: This mechanism allows the model to attend to all tokens in the input sequence when generating the representation of a particular token. It helps the model understand which parts of the sequence are most relevant for each token.\n",
    "- **Feedforward neural network**: After self-attention, each token's representation is passed through a feedforward neural network to apply further transformations and capture more complex relationships.\n",
    "\n",
    "These operations are repeated across multiple layers, which helps the model learn increasingly abstract representations of the input data.\n",
    "\n",
    "#### **Decoder**\n",
    "\n",
    "The decoder is responsible for generating the output sequence, one token at a time, based on the encoder's representations. It has a similar structure to the encoder but includes an additional **cross-attention mechanism**. The cross-attention mechanism allows the decoder to focus on relevant parts of the encoder's output when generating each token in the output sequence.\n",
    "\n",
    "The decoder's main components include:\n",
    "- **Self-attention mechanism**: Like the encoder, the decoder uses self-attention to process the current state of the output sequence.\n",
    "- **Cross-attention mechanism**: This mechanism attends to the encoder's output, guiding the generation of each new token in the output sequence.\n",
    "- **Feedforward neural network**: The decoder also includes a feedforward network that further transforms the token representations after the attention layers.\n",
    "\n",
    "### **Attention mechanism**\n",
    "\n",
    "At the heart of the Transformer model is the **attention mechanism**, specifically **self-attention**. Self-attention allows the model to relate different tokens in the input sequence to each other. For example, when translating a sentence, self-attention helps the model understand how each word is related to the others, regardless of their position in the sequence.\n",
    "\n",
    "Self-attention works by computing a score for each token relative to all other tokens in the sequence. This score determines how much attention each token should pay to the others when building its representation. The attention mechanism helps the model focus on the most relevant parts of the sequence, improving its understanding of context and relationships between tokens.\n",
    "\n",
    "In the Transformer, **multi-head attention** is used to enable the model to focus on different aspects of the input simultaneously. Each head in the multi-head attention mechanism captures different relationships, such as syntax or meaning, providing a richer representation of the input sequence.\n",
    "\n",
    "### **Positional encoding**\n",
    "\n",
    "Since the Transformer model processes the entire input sequence in parallel, it loses the inherent sequential information that RNNs or LSTMs provide. To compensate for this, Transformers use **positional encodings**, which add information about the position of each token in the sequence. These encodings are added to the input embeddings, allowing the model to understand the order of the tokens and their relative positions.\n",
    "\n",
    "Without positional encodings, the model would treat all tokens as if they were independent of their positions, which would lead to a loss of crucial sequential information.\n",
    "\n",
    "### **Key advantages of Transformers**\n",
    "\n",
    "Transformers offer several important advantages over traditional sequence models like RNNs and LSTMs:\n",
    "- **Parallelization**: Unlike RNNs, which process sequences step-by-step, Transformers process the entire sequence simultaneously, making them much faster to train and more scalable to large datasets.\n",
    "- **Long-range dependencies**: The self-attention mechanism allows Transformers to capture dependencies between distant tokens more effectively than RNNs, which struggle with long-range dependencies due to the vanishing gradient problem.\n",
    "- **Flexibility**: Transformers are highly flexible and can be adapted to a wide range of tasks, from language translation to image processing. This versatility is one reason why they have become the go-to architecture for many state-of-the-art models.\n",
    "\n",
    "### **Applications of Transformer models**\n",
    "\n",
    "Transformers are widely used across various domains, especially in NLP tasks. Some common applications include:\n",
    "- **Machine translation**: In tasks like translating text from one language to another, the Transformer has become the standard model, replacing older sequence-to-sequence models like RNNs and LSTMs.\n",
    "- **Text summarization**: Transformers can generate concise summaries of longer texts by understanding the key points and relationships within the document.\n",
    "- **Question answering**: Models like BERT, based on the Transformer, are used to answer questions by understanding the context of the input text and extracting relevant information.\n",
    "- **Language modeling**: Transformers are the foundation of models like GPT, which are used to generate text, complete sentences, or even create conversational agents.\n",
    "- **Image processing**: While originally designed for NLP, Transformer models have been adapted for image tasks as well, where they excel in areas like image classification and object detection.\n",
    "\n",
    "### **Challenges with Transformers**\n",
    "\n",
    "Despite their success, Transformers come with their own set of challenges:\n",
    "- **Computation cost**: The self-attention mechanism requires computing interactions between all pairs of tokens, leading to quadratic complexity. This makes the model computationally expensive, particularly for long sequences.\n",
    "- **Memory usage**: Since Transformers process the entire sequence at once, they require significant memory to store the attention weights and intermediate representations. This can be a bottleneck for training on large datasets or long sequences.\n",
    "\n",
    "Researchers have proposed various modifications to address these issues, including models like **Sparse Transformers** and **Longformers**, which reduce the computation and memory requirements by focusing attention on only a subset of the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Maths**\n",
    "\n",
    "#### **Self-attention mechanism**\n",
    "\n",
    "At the core of the Transformer model is the self-attention mechanism, which enables each token in the input sequence to focus on every other token. The self-attention mechanism calculates the attention score between pairs of tokens to determine how much focus one token should place on another.\n",
    "\n",
    "For each token in the sequence, the Transformer computes three vectors:\n",
    "- **Query (Q)**: A vector representing what the token is querying.\n",
    "- **Key (K)**: A vector representing the content of other tokens.\n",
    "- **Value (V)**: The actual content to be passed on as a weighted sum.\n",
    "\n",
    "Given an input sequence $ X \\in \\mathbb{R}^{n \\times d} $ with $ n $ tokens, where each token is represented by a $ d $-dimensional vector, the query, key, and value vectors are computed through linear projections:\n",
    "\n",
    "$$\n",
    "Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d_k} $ are learned weight matrices that project the input $ X $ into the query, key, and value spaces, each with dimensionality $ d_k $.\n",
    "\n",
    "Once the query, key, and value vectors are computed, the attention score between a query $ q_i $ and a key $ k_j $ is determined by the dot product:\n",
    "\n",
    "$$\n",
    "\\text{score}(q_i, k_j) = q_i \\cdot k_j\n",
    "$$\n",
    "\n",
    "These attention scores are then scaled by $ \\sqrt{d_k} $, where $ d_k $ is the dimensionality of the key vectors, to prevent the dot product values from becoming too large. The scaled attention scores are passed through a **softmax** function to obtain the attention weights, which determine how much attention each token should place on other tokens:\n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{\\exp\\left( \\frac{q_i \\cdot k_j}{\\sqrt{d_k}} \\right)}{\\sum_{j=1}^n \\exp\\left( \\frac{q_i \\cdot k_j}{\\sqrt{d_k}} \\right)}\n",
    "$$\n",
    "\n",
    "Finally, the weighted sum of the value vectors is computed to produce the output for each token:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\sum_{j=1}^{n} \\alpha_{ij} v_j\n",
    "$$\n",
    "\n",
    "Where $ v_j $ is the value vector corresponding to token $ j $, and $ \\alpha_{ij} $ is the attention weight from token $ i $ to token $ j $.\n",
    "\n",
    "#### **Multi-head attention**\n",
    "\n",
    "In practice, a single attention mechanism might not be enough to capture all the relationships in the input sequence. To improve the model's expressiveness, Transformers use **multi-head attention**, where multiple attention mechanisms (heads) operate in parallel on different parts of the sequence, focusing on different aspects of the input.\n",
    "\n",
    "Each attention head computes its own query, key, and value projections:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(Q W_{Q_i}, K W_{K_i}, V W_{V_i})\n",
    "$$\n",
    "\n",
    "Where $ W_{Q_i}, W_{K_i}, W_{V_i} \\in \\mathbb{R}^{d \\times d_k} $ are the learned weight matrices for the $ i $-th attention head. The results of all attention heads are then concatenated and linearly transformed:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W_O\n",
    "$$\n",
    "\n",
    "Where $ W_O \\in \\mathbb{R}^{hd_k \\times d} $ is the output projection matrix, and $ h $ is the number of attention heads. The use of multiple heads allows the model to capture different patterns and relationships within the input sequence simultaneously.\n",
    "\n",
    "#### **Positional encoding**\n",
    "\n",
    "Since the self-attention mechanism processes the input sequence in parallel, it loses the sequential information typically present in language or other data. To address this, Transformers use **positional encodings**, which inject information about the token's position in the sequence.\n",
    "\n",
    "The positional encoding vector for a position $ i $ is computed using sinusoidal functions:\n",
    "\n",
    "$$\n",
    "PE_{i, 2j} = \\sin\\left( \\frac{i}{10000^{2j/d}} \\right), \\quad PE_{i, 2j+1} = \\cos\\left( \\frac{i}{10000^{2j/d}} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ PE_{i, 2j} $ and $ PE_{i, 2j+1} $ are the positional encodings for the $ j $-th dimension of the token at position $ i $.\n",
    "- $ d $ is the dimensionality of the embedding space.\n",
    "\n",
    "These positional encodings are added to the input embeddings, ensuring that the model retains information about the order of the tokens in the sequence.\n",
    "\n",
    "#### **Feedforward network**\n",
    "\n",
    "After the multi-head attention mechanism, each token's representation is passed through a feedforward neural network (FFN). The FFN applies a non-linear transformation to each token's representation independently. The FFN consists of two fully connected layers with a non-linear activation function (e.g., ReLU) between them:\n",
    "\n",
    "$$\n",
    "FFN(x) = \\max(0, x W_1 + b_1) W_2 + b_2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W_1 \\in \\mathbb{R}^{d \\times d_{\\text{ff}}} $ and $ W_2 \\in \\mathbb{R}^{d_{\\text{ff}} \\times d} $ are the weight matrices,\n",
    "- $ b_1 \\in \\mathbb{R}^{d_{\\text{ff}}} $ and $ b_2 \\in \\mathbb{R}^{d} $ are the bias terms,\n",
    "- $ d_{\\text{ff}} $ is the dimensionality of the feedforward network's hidden layer.\n",
    "\n",
    "This feedforward network applies the same transformation to all tokens, allowing the model to learn more complex representations of the input.\n",
    "\n",
    "#### **Layer normalization and residual connections**\n",
    "\n",
    "To stabilize training and ensure better gradient flow, Transformers use **layer normalization** and **residual connections**. Residual connections add the input to the output of a sublayer, preventing the model from losing important information during processing. This can be represented as:\n",
    "\n",
    "$$\n",
    "\\text{LayerOutput} = \\text{LayerNorm}(x + \\text{SublayerOutput})\n",
    "$$\n",
    "\n",
    "Where $ \\text{SublayerOutput} $ can be the output of either the multi-head attention or the feedforward network, and $ x $ is the input to the sublayer.\n",
    "\n",
    "Layer normalization ensures that the input to each sublayer has a stable distribution, improving convergence during training.\n",
    "\n",
    "#### **Training objective**\n",
    "\n",
    "Transformers are trained to minimize a loss function, typically **cross-entropy loss**, when applied to tasks like machine translation or language modeling. The model is trained to predict the correct output token (or sequence of tokens) by comparing its predictions to the ground truth. During training, the model uses **teacher forcing**, where the true previous token is provided to the decoder at each step, ensuring that the model learns accurate dependencies between tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for building and training transformer models in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required PyTorch modules to construct attention mechanisms and build transformer models?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you configure the environment to use GPU support for training transformer models in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the input data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you define input sequences (e.g., tokenized text) to feed into the transformer model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you preprocess the input data and convert it into embeddings for the transformer model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you pad input sequences to ensure consistent lengths before feeding them into the transformer model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing positional encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you implement sinusoidal positional encoding in PyTorch to represent the order of tokens in sequences?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you add positional encodings to the input embeddings for the transformer model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you verify that the positional encoding has been correctly added to the input data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the scaled dot-product attention mechanism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you implement the scaled dot-product attention mechanism in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you compute attention scores by calculating the dot product of query and key matrices?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you apply softmax to the attention scores and multiply them by the value matrix to get the final output?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing multi-head attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you implement multi-head attention by splitting input sequences into multiple attention heads in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you apply the scaled dot-product attention mechanism separately for each head in the multi-head attention?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you concatenate the outputs of the multiple attention heads and apply a linear projection?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the feed-forward network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you implement the position-wise feed-forward network using `torch.nn.Linear` layers?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you apply activation functions like ReLU after the linear layers in the feed-forward network?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you add dropout and layer normalization to the feed-forward network for regularization?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the transformer encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you combine multi-head attention and the feed-forward network to construct a transformer encoder layer?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you implement residual connections and layer normalization around the attention and feed-forward layers in the transformer encoder?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you stack multiple transformer encoder layers to create a deep transformer model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the transformer model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you define the loss function (e.g., CrossEntropyLoss) for a sequence-based task in the transformer model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you set up the optimizer (e.g., Adam) to update the transformer model’s parameters during training?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you implement the training loop, including forward pass, loss calculation, and backpropagation for the transformer model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you track and log the training loss and accuracy over multiple epochs when training the transformer model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the transformer model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you evaluate the transformer model on a validation or test dataset to calculate performance metrics?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q27: How do you compute metrics such as accuracy or perplexity to evaluate the transformer’s performance?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q28: How do you compare the transformer model's performance to other baseline models, such as LSTMs or RNNs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with different transformer configurations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q29: How do you experiment with different numbers of layers and attention heads in the transformer model to observe their effect on performance?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q30: How do you adjust the hidden dimension size of the transformer and analyze its impact on training time and accuracy?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q31: How do you experiment with different learning rates and dropout rates to optimize the transformer’s generalization and performance?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q32: How do you analyze how the transformer model performs on different tasks by varying the input data and sequence lengths?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
