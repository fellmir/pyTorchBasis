{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying models with Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding model deployment with Flask](#understanding-model-deployment-with-flask)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Loading the pre-trained model](#loading-the-pre-trained-model)\n",
    "4. [Creating a Flask web application](#creating-a-flask-web-application)\n",
    "5. [Building RESTful APIs for model inference](#building-restful-apis-for-model-inference)\n",
    "6. [Handling input data for predictions](#handling-input-data-for-predictions)\n",
    "7. [Returning model predictions through Flask](#returning-model-predictions-through-flask)\n",
    "8. [Testing the Flask app locally](#testing-the-flask-app-locally)\n",
    "9. [Deploying the Flask app to the cloud](#deploying-the-flask-app-to-the-cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding model deployment with Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Key concepts**\n",
    "Deploying models with Flask involves creating a web application that serves machine learning models, enabling users to interact with them via HTTP requests. Flask, a lightweight Python web framework, provides a simple and flexible approach to building RESTful APIs for serving predictions from trained models. This approach is widely used for deploying models to production environments, where they can process real-time or batch requests from external systems.\n",
    "\n",
    "Key components of deploying models with Flask include:\n",
    "- **Flask application**: Defines routes to handle requests and responses.\n",
    "- **Model loading**: Loads the trained model (e.g., PyTorch, TensorFlow) into memory for inference.\n",
    "- **Prediction endpoints**: Provides RESTful APIs to accept input data, perform inference, and return predictions.\n",
    "- **Scalability**: Can integrate with tools like Gunicorn and Docker for handling production-scale traffic.\n",
    "\n",
    "Flask enables seamless integration of machine learning models into applications, providing accessibility to users and systems.\n",
    "\n",
    "### **Applications**\n",
    "Deploying models with Flask is widely used in:\n",
    "- **Web applications**: Serving predictions directly to web interfaces for real-time interaction.\n",
    "- **Mobile and IoT**: Providing APIs that mobile apps or IoT devices can call for predictions.\n",
    "- **Enterprise systems**: Integrating models into business workflows or decision-making systems.\n",
    "- **Data pipelines**: Embedding models as RESTful endpoints in larger data processing architectures.\n",
    "\n",
    "### **Advantages**\n",
    "- **Simplicity**: Flask’s minimalistic framework makes it easy to set up and deploy a model-serving API.\n",
    "- **Flexibility**: Supports custom routing, middleware, and extensions to tailor the deployment environment.\n",
    "- **Integration**: Easily integrates with databases, caching systems, and containerization tools like Docker.\n",
    "- **Portability**: The application can be deployed on local machines, cloud platforms, or edge devices.\n",
    "\n",
    "### **Challenges**\n",
    "- **Concurrency**: Flask’s default server may not handle high traffic effectively, requiring additional tools like Gunicorn.\n",
    "- **Latency**: Real-time inference may face delays, especially for large models or complex computations.\n",
    "- **Scalability**: Scaling Flask applications for large-scale deployments requires careful architecture and additional infrastructure.\n",
    "- **Security**: Ensuring secure data transmission and API access is critical in production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for Flask and machine learning model deployment using `pip`?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flask torch torchvision requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules, such as Flask, PyTorch (or TensorFlow), and `requests` in Python?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you set up the project directory structure for a Flask-based deployment?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the following structure:\n",
    "\n",
    "# project/\n",
    "# ├── app.py                    # main Flask app\n",
    "# ├── model.pth                 # saved PyTorch model\n",
    "# ├── requirements.txt          # dependencies\n",
    "# ├── Procfile                  # required for Heroku deployment\n",
    "# └── utils/                    # optional: for helper modules\n",
    "#     └── preprocess.py         # input preprocessing logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can create it using Python, e.g.:\n",
    "import os\n",
    "\n",
    "os.makedirs('project/utils', exist_ok=True)\n",
    "open('project/app.py', 'a').close()\n",
    "open('project/model.pth', 'a').close()\n",
    "open('project/requirements.txt', 'a').close()\n",
    "open('project/Procfile', 'a').close()\n",
    "open('project/utils/preprocess.py', 'a').close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you configure the environment to enable debug mode for the Flask application?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in app.py, add debug=True when running the app\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(debug=True)  # enables live reload and error display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the pre-trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you load a pre-trained model in PyTorch (or TensorFlow) for use in a Flask application?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(4, 3)  # example architecture for input size 4 and output size 3\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()\n",
    "torch.save(model.state_dict(), 'project/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleModel(\n",
       "  (fc): Linear(in_features=4, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('project/model.pth', map_location='cpu'))  # load to CPU by default\n",
    "model.eval()  # set model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you verify that the model is working correctly by testing it on sample input data before deploying it?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output: tensor([[-0.0655,  0.4912, -0.7613]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sample_input = torch.randn(1, 4)  # single sample with 4 features\n",
    "output = model(sample_input)\n",
    "print('Sample output:', output)  # confirm output is tensor with shape [1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you handle the model’s device allocation (CPU/GPU) when loading it for deployment in a Flask app?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleModel(\n",
       "  (fc): Linear(in_features=4, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleModel()\n",
    "model.load_state_dict(torch.load('project/model.pth', map_location=device))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print('Model loaded on:', next(model.parameters()).device)  # confirm model device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Flask web application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you initialize a basic Flask app in Python and set up the main app file?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app = Flask(__name__)  # initialize the Flask app\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(debug=True)  # run app in debug mode for development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you define a simple home route (`/`) that serves a basic welcome message in Flask?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route('/')  # define the route for the home page\n",
    "# def home():\n",
    "#     return 'Welcome to the model inference API!'  # basic welcome message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you set up route handling for API endpoints in Flask?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route('/api/health', methods=['GET'])  # define an example API endpoint\n",
    "# def health_check():\n",
    "#     return jsonify({'status': 'ok'}), 200  # return JSON response with status code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building RESTful APIs for model inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you define a `/predict` route in Flask to handle POST requests for model inference?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route('/predict', methods=['POST'])  # define the prediction endpoint\n",
    "# def predict():\n",
    "#     if not request.is_json:  # check if request has JSON\n",
    "#         return jsonify({'error': 'Request must be in JSON format'}), 400\n",
    "\n",
    "#     data = request.get_json()\n",
    "#     # dummy placeholder: real input processing and model inference go here\n",
    "#     return jsonify({'message': 'Prediction endpoint hit', 'received': data}), 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you set up the Flask route to accept input data in JSON format for the model prediction?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the logic is included inside /predict route from Q11 using request.get_json()\n",
    "# # here is the relevant part again in context:\n",
    "# @app.route('/predict', methods=['POST'])\n",
    "# def predict():\n",
    "#     if not request.is_json:\n",
    "#         return jsonify({'error': 'Request must be in JSON format'}), 400\n",
    "\n",
    "#     input_data = request.get_json()  # parse JSON body from the request\n",
    "#     # continue with validation and inference\n",
    "#     return jsonify({'input_received': input_data}), 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you configure the Flask app to return appropriate status codes in response to the API requests?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route('/predict', methods=['POST'])\n",
    "# def predict():\n",
    "#     if not request.is_json:\n",
    "#         return jsonify({'error': 'Expected JSON data'}), 400  # return 400 if not JSON\n",
    "\n",
    "#     input_data = request.get_json()\n",
    "\n",
    "#     if 'features' not in input_data:  # check for expected key\n",
    "#         return jsonify({'error': 'Missing \"features\" in input'}), 400  # return 400 if missing\n",
    "\n",
    "#     return jsonify({'prediction': [0.1, 0.7, 0.2]}), 200  # dummy success response with 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling input data for predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you parse input data from a JSON request in Flask using `request.get_json()`?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route('/predict', methods=['POST'])\n",
    "# def predict():\n",
    "#     data = request.get_json()  # parse JSON input\n",
    "#     features = data.get('features')  # retrieve the 'features' key\n",
    "#     if features is None:\n",
    "#         return jsonify({'error': 'Missing \"features\" in input'}), 400\n",
    "\n",
    "#     return jsonify({'received_features': features}), 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you preprocess the input data before passing it to the model for prediction?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route('/predict', methods=['POST'])\n",
    "# def predict():\n",
    "#     data = request.get_json()\n",
    "#     features = data.get('features')\n",
    "#     if not features or not isinstance(features, list):\n",
    "#         return jsonify({'error': '\"features\" must be a non-empty list'}), 400\n",
    "\n",
    "#     try:\n",
    "#         input_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0)  # shape [1, N]\n",
    "#     except Exception as e:\n",
    "#         return jsonify({'error': f'Invalid input format: {str(e)}'}), 400\n",
    "\n",
    "#     return jsonify({'preprocessed_shape': list(input_tensor.shape)}), 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you validate the input data format in Flask to ensure it matches the model’s expected input shape?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route('/predict', methods=['POST'])\n",
    "# def predict():\n",
    "#     data = request.get_json()\n",
    "#     features = data.get('features')\n",
    "#     if not features or not isinstance(features, list):\n",
    "#         return jsonify({'error': '\"features\" must be a list'}), 400\n",
    "\n",
    "#     input_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0)  # shape [1, N]\n",
    "#     if input_tensor.shape[1] != 4:  # expected input size is 4 for SimpleModel\n",
    "#         return jsonify({'error': f'Expected 4 input features, got {input_tensor.shape[1]}'}), 400\n",
    "\n",
    "#     return jsonify({'valid_input': True}), 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Returning model predictions through Flask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you run the model’s inference on the preprocessed input data in Flask?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route('/predict', methods=['POST'])\n",
    "# def predict():\n",
    "#     data = request.get_json()\n",
    "#     features = data.get('features')\n",
    "#     if not features or not isinstance(features, list):\n",
    "#         return jsonify({'error': '\"features\" must be a list'}), 400\n",
    "\n",
    "#     input_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0)  # shape [1, 4]\n",
    "#     if input_tensor.shape[1] != 4:\n",
    "#         return jsonify({'error': f'Expected 4 input features, got {input_tensor.shape[1]}'}), 400\n",
    "\n",
    "#     input_tensor = input_tensor.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_tensor)  # run model inference\n",
    "\n",
    "#     return jsonify({'raw_output': outputs.tolist()}), 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you format the model’s output into a JSON response?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route('/predict', methods=['POST'])\n",
    "# def predict():\n",
    "#     data = request.get_json()\n",
    "#     features = data.get('features')\n",
    "#     if not features or not isinstance(features, list):\n",
    "#         return jsonify({'error': '\"features\" must be a list'}), 400\n",
    "\n",
    "#     input_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0)\n",
    "#     if input_tensor.shape[1] != 4:\n",
    "#         return jsonify({'error': f'Expected 4 input features, got {input_tensor.shape[1]}'}), 400\n",
    "\n",
    "#     input_tensor = input_tensor.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         logits = model(input_tensor)\n",
    "#         probs = torch.softmax(logits, dim=1)  # convert to probabilities\n",
    "#         predicted_class = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "#     return jsonify({\n",
    "#         'predicted_class': predicted_class,\n",
    "#         'probabilities': probs.squeeze().tolist()\n",
    "#     }), 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you return the JSON response with the prediction results to the client in Flask?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already handled in Q18 with `jsonify(...)` and status code\n",
    "# here's a minimal final structure:\n",
    "\n",
    "# @app.route('/predict', methods=['POST'])\n",
    "# def predict():\n",
    "#     data = request.get_json()\n",
    "#     features = data.get('features')\n",
    "#     if not features or not isinstance(features, list):\n",
    "#         return jsonify({'error': '\"features\" must be a list'}), 400\n",
    "\n",
    "#     input_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0)\n",
    "#     if input_tensor.shape[1] != 4:\n",
    "#         return jsonify({'error': f'Expected 4 input features, got {input_tensor.shape[1]}'}), 400\n",
    "\n",
    "#     input_tensor = input_tensor.to(device)\n",
    "#     with torch.no_grad():\n",
    "#         logits = model(input_tensor)\n",
    "#         probs = torch.softmax(logits, dim=1)\n",
    "#         predicted_class = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "#     return jsonify({\n",
    "#         'prediction': {\n",
    "#             'class': predicted_class,\n",
    "#             'probabilities': probs.squeeze().tolist()\n",
    "#         }\n",
    "#     }), 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Flask app locally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you use `curl` to send POST requests with input data to the Flask app for testing?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this in terminal (not in Python):\n",
    "# curl -X POST http://127.0.0.1:5000/predict \\\n",
    "#      -H \"Content-Type: application/json\" \\\n",
    "#      -d \"{\\\"features\\\": [0.1, 0.2, 0.3, 0.4]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected JSON response (example):\n",
    "{\n",
    "  \"prediction\": {\n",
    "    \"class\": 1,\n",
    "    \"probabilities\": [0.12, 0.78, 0.10]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you use Postman to test the Flask API by sending input data and receiving predictions?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Open Postman.\n",
    "# 2. Set method to POST.\n",
    "# 3. Enter URL: http://127.0.0.1:5000/predict\n",
    "# 4. Go to 'Body' tab → choose 'raw' → select 'JSON' from the dropdown.\n",
    "# 5. Enter input:\n",
    "{\n",
    "  \"features\": [0.1, 0.2, 0.3, 0.4]\n",
    "}\n",
    "# 6. Hit 'Send' and inspect the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you debug common issues such as incorrect input formats or missing model files in Flask?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use structured error handling and logging:\n",
    "\n",
    "# @app.route('/predict', methods=['POST'])\n",
    "# def predict():\n",
    "#     try:\n",
    "#         data = request.get_json(force=True)\n",
    "#         features = data.get('features')\n",
    "#         if not features or not isinstance(features, list):\n",
    "#             return jsonify({'error': '\"features\" must be a list'}), 400\n",
    "\n",
    "#         input_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0)\n",
    "#         if input_tensor.shape[1] != 4:\n",
    "#             return jsonify({'error': f'Expected 4 features, got {input_tensor.shape[1]}'}), 400\n",
    "\n",
    "#         input_tensor = input_tensor.to(device)\n",
    "#         with torch.no_grad():\n",
    "#             logits = model(input_tensor)\n",
    "#             probs = torch.softmax(logits, dim=1)\n",
    "#             predicted_class = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "#         return jsonify({\n",
    "#             'prediction': {\n",
    "#                 'class': predicted_class,\n",
    "#                 'probabilities': probs.squeeze().tolist()\n",
    "#             }\n",
    "#         }), 200\n",
    "\n",
    "#     except FileNotFoundError:\n",
    "#         return jsonify({'error': 'Model file not found'}), 500\n",
    "#     except Exception as e:\n",
    "#         return jsonify({'error': str(e)}), 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Flask app to the cloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you set up a `Procfile` for deploying the Flask app to Heroku?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('project/Procfile', 'w') as f:\n",
    "    f.write('web: python app.py')  # defines the command Heroku will run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you deploy the Flask app to Heroku and test the live API?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Initialize git (if not already done)\n",
    "# git init\n",
    "# git add .\n",
    "# git commit -m \"Initial commit\"\n",
    "\n",
    "# # 2. Login to Heroku CLI\n",
    "# heroku login\n",
    "\n",
    "# # 3. Create a new Heroku app\n",
    "# heroku create flask-model-api\n",
    "\n",
    "# # 4. Add a Heroku-compatible requirements.txt\n",
    "# pip freeze > requirements.txt\n",
    "\n",
    "# # 5. Deploy to Heroku\n",
    "# git push heroku master\n",
    "\n",
    "# # 6. Open the app or test the endpoint\n",
    "# heroku open\n",
    "# # or test via:\n",
    "# curl -X POST https://flask-model-api.herokuapp.com/predict \\\n",
    "#      -H \"Content-Type: application/json\" \\\n",
    "#      -d \"{\\\"features\\\": [0.1, 0.2, 0.3, 0.4]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you deploy the Flask app to AWS or Google Cloud for real-time model serving?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Option A: AWS Elastic Beanstalk\n",
    "# 1. Install EB CLI and configure:\n",
    "#    eb init -p python-3.9 flask-model-api\n",
    "# 2. Create environment:\n",
    "#    eb create flask-env\n",
    "# 3. Deploy:\n",
    "#    eb deploy\n",
    "\n",
    "# # Option B: Google Cloud Run\n",
    "# 1. Containerize the app using Docker:\n",
    "#    - Create a Dockerfile in project root:\n",
    "#      ------------------\n",
    "#      FROM python:3.9\n",
    "#      WORKDIR /app\n",
    "#      COPY . .\n",
    "#      RUN pip install -r requirements.txt\n",
    "#      CMD [\"python\", \"app.py\"]\n",
    "#      ------------------\n",
    "# 2. Deploy:\n",
    "#    gcloud builds submit --tag gcr.io/YOUR_PROJECT_ID/flask-model-api\n",
    "#    gcloud run deploy flask-model-api --image gcr.io/YOUR_PROJECT_ID/flask-model-api --platform managed\n",
    "\n",
    "# # After deployment, the service URL is returned by the CLI for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile_content = '''\\\n",
    "FROM python:3.9\n",
    "WORKDIR /app\n",
    "COPY . .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "CMD [\"python\", \"app.py\"]\n",
    "'''\n",
    "\n",
    "with open('project/Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you test the deployed Flask API by sending remote requests to the live application?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use curl or any HTTP client with your live app URL (replace <URL>)\n",
    "\n",
    "# curl -X POST https://<your-app-url>/predict \\\n",
    "#      -H \"Content-Type: application/json\" \\\n",
    "#      -d \"{\\\"features\\\": [0.1, 0.2, 0.3, 0.4]}\"\n",
    "\n",
    "# you should receive a JSON prediction response with class and probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree('project')  # deletes the entire 'project' directory and its contents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
