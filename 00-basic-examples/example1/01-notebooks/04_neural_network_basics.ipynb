{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network basics\n",
    "\n",
    "Welcome to the `04_neural_network_basics` notebook. This part of the portfolio is designed to introduce fundamental concepts and techniques in PyTorch, with a particular emphasis on building and understanding neural networks.\n",
    "\n",
    "Throughout this notebook, I'll explore essential topics such as setting up the environment, defining neural network architectures, and implementing both forward and backward propagation. I'll also cover training procedures, model evaluation, and techniques to save and load trained models.\n",
    "\n",
    "By working through these exercises, the aim is to gain practical experience in constructing and optimizing neural networks, forming a solid foundation for more advanced machine learning endeavors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Understanding neural networks](#understanding-neural-networks)\n",
    "3. [Setting up the environment](#setting-up-the-environment)\n",
    "4. [Building a neural network](#building-a-neural-network)\n",
    "5. [Forward propagation](#forward-propagation)\n",
    "6. [Loss function](#loss-function)\n",
    "7. [Backpropagation](#backpropagation)\n",
    "8. [Training the neural network](#training-the-neural-network)\n",
    "9. [Evaluating the model](#evaluating-the-model)\n",
    "10. [Saving and loading the model](#saving-and-loading-the-model)\n",
    "11. [Optimizations](#optimizations)\n",
    "12. [Handling real-world data](#handling-real-world-data)\n",
    "13. [Conclusion](#conclusion)\n",
    "14. [Further exercises](#further-exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding neural networks\n",
    "\n",
    "Neural networks are a foundational technique in machine learning and artificial intelligence, utilized for identifying patterns and relationships within data. Unlike traditional algorithms, which often rely on predefined rules, neural networks learn directly from examples, making them highly adaptable and powerful. These models can handle a variety of tasks, from classification to regression, by adjusting their parameters through training, thereby improving their performance with more data and experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key concepts\n",
    "\n",
    "#### 1. Neurons and layers\n",
    "Neural networks are inspired by the structure and function of the human brain, consisting of interconnected units called neurons. These neurons are organized into layers: the input layer, hidden layers, and the output layer.\n",
    "\n",
    "- **Input layer**: The first layer in the network that receives the input data. Each neuron in this layer represents a feature of the input data.\n",
    "- **Hidden layers**: Layers between the input and output layers. These layers perform various transformations on the inputs received, allowing the network to learn complex patterns. There can be multiple hidden layers, which is why deep neural networks are also known as deep learning models.\n",
    "- **Output layer**: The final layer that produces the output of the network. The number of neurons in this layer corresponds to the number of desired outputs.\n",
    "\n",
    "#### 2. Activation functions\n",
    "Activation functions introduce non-linearity into the network, enabling it to learn and model complex relationships in the data. Without activation functions, the network would only be able to model linear relationships.\n",
    "\n",
    "- **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. It is widely used due to its simplicity and effectiveness.\n",
    "- **Sigmoid**: Compresses the input to a range between 0 and 1. It is often used in binary classification problems.\n",
    "- **Tanh (Hyperbolic tangent)**: Compresses the input to a range between -1 and 1, centering the data. It is often used in practice but can lead to issues with gradient vanishing.\n",
    "\n",
    "#### 3. Forward propagation\n",
    "Forward propagation is the process of passing input data through the network to obtain an output. During this process, each neuron computes a weighted sum of its inputs, adds a bias term, and applies an activation function to produce its output. The output of one layer becomes the input to the next layer, and this process continues until the final output layer.\n",
    "\n",
    "#### 4. Loss function\n",
    "The loss function measures the difference between the predicted outputs and the actual targets. It quantifies how well the neural network is performing. The goal of training the network is to minimize this loss.\n",
    "\n",
    "- **Mean Squared Error (MSE)**: Commonly used for regression tasks, it calculates the average squared difference between predicted and actual values.\n",
    "- **Cross-entropy loss**: Used for classification tasks, it measures the difference between the predicted probability distribution and the actual distribution.\n",
    "\n",
    "#### 5. Backpropagation\n",
    "Backpropagation is the process of updating the network's weights to minimize the loss. It involves calculating the gradient of the loss function with respect to each weight and adjusting the weights in the opposite direction of the gradient. This ensures that the loss decreases with each iteration.\n",
    "\n",
    "#### 6. Optimizers\n",
    "Optimizers are algorithms that adjust the weights of the network to minimize the loss. They use the gradients calculated during backpropagation to update the weights.\n",
    "\n",
    "- **Stochastic Gradient Descent (SGD)**: Updates the weights using a small, randomly chosen subset of the data (a mini-batch). This makes the optimization process faster and allows the model to learn from a diverse set of examples.\n",
    "- **Adam (Adaptive Moment Estimation)**: An extension of SGD that adapts the learning rate for each parameter, making it more efficient and robust in practice.\n",
    "\n",
    "#### 7. Training and validation\n",
    "Training a neural network involves iteratively feeding data through the network, calculating the loss, and updating the weights. This process is repeated for a specified number of epochs (complete passes through the training dataset).\n",
    "\n",
    "- **Training set**: The subset of data used to train the model.\n",
    "- **Validation set**: A separate subset of data used to evaluate the model's performance during training, helping to tune hyperparameters and prevent overfitting.\n",
    "\n",
    "#### 8. Overfitting and underfitting\n",
    "- **Overfitting**: Occurs when the model learns the training data too well, capturing noise and specific patterns that do not generalize to new data. This results in poor performance on the validation set.\n",
    "- **Underfitting**: Occurs when the model is too simple to capture the underlying patterns in the data, leading to poor performance on both the training and validation sets.\n",
    "\n",
    "#### 9. Regularization techniques\n",
    "Regularization techniques are used to prevent overfitting by adding constraints or penalties to the model.\n",
    "\n",
    "- **Dropout**: Randomly drops a fraction of neurons during training, forcing the network to learn redundant representations and improving generalization.\n",
    "- **L2 regularization (Ridge)**: Adds a penalty proportional to the sum of the squares of the weights, discouraging large weights and promoting simpler models.\n",
    "\n",
    "#### 10. Model evaluation\n",
    "After training, the model's performance is evaluated on a test set that was not seen during training. This provides an unbiased estimate of how well the model generalizes to new data.\n",
    "\n",
    "- **Accuracy**: The proportion of correctly classified instances out of the total instances.\n",
    "- **Precision, recall, and F1 score**: Metrics that provide deeper insights into the model's performance, especially for imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maths\n",
    "\n",
    "#### 1. Structure of Neural Networks\n",
    "\n",
    "##### Layers\n",
    "- **Input layer**: This is the first layer of the network, which receives the raw input data. Each neuron in this layer corresponds to one feature of the input data.\n",
    "- **Hidden layers**: These layers are located between the input and output layers. They perform computations and extract features from the input data. A neural network can have multiple hidden layers, which allows it to learn complex patterns.\n",
    "- **Output layer**: This is the final layer of the network, which produces the output. The number of neurons in this layer depends on the type of problem (e.g., one neuron for binary classification, multiple neurons for multi-class classification).\n",
    "\n",
    "##### Neurons\n",
    "Each neuron in a neural network computes a weighted sum of its inputs, adds a bias term, and applies an activation function to produce its output.\n",
    "\n",
    "#### 2. Forward propagation\n",
    "\n",
    "Forward propagation is the process by which input data passes through the network to generate an output. It involves the following steps:\n",
    "\n",
    "##### Weighted sum\n",
    "For a given neuron $ j $ in layer $ l $, the input $ z_j^{(l)} $ is computed as:\n",
    "$$ z_j^{(l)} = \\sum_{i=1}^{n} w_{ij}^{(l-1)} a_i^{(l-1)} + b_j^{(l)} $$\n",
    "where:\n",
    "- $ w_{ij}^{(l-1)} $ is the weight connecting neuron $ i $ in layer $ l-1 $ to neuron $ j $ in layer $ l $.\n",
    "- $ a_i^{(l-1)} $ is the activation of neuron $ i $ in layer $ l-1 $.\n",
    "- $ b_j^{(l)} $ is the bias term for neuron $ j $ in layer $ l $.\n",
    "- $ n $ is the number of neurons in layer $ l-1 $.\n",
    "\n",
    "##### Activation function\n",
    "The output $ a_j^{(l)} $ of neuron $ j $ in layer $ l $ is obtained by applying an activation function $ f $ to the weighted sum:\n",
    "$$ a_j^{(l)} = f(z_j^{(l)}) $$\n",
    "\n",
    "Common activation functions include:\n",
    "- **Sigmoid**: $ f(z) = \\frac{1}{1 + e^{-z}} $\n",
    "- **Tanh**: $ f(z) = \\tanh(z) $\n",
    "- **ReLU (Rectified Linear Unit)**: $ f(z) = \\max(0, z) $\n",
    "\n",
    "#### 3. Loss function\n",
    "\n",
    "The loss function quantifies the difference between the predicted output and the actual target. The goal of training a neural network is to minimize this loss. Common loss functions include:\n",
    "\n",
    "- **Mean Squared Error (MSE)**: Used for regression tasks, defined as:\n",
    "  $$ \\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y_i})^2 $$\n",
    "  where $ y_i $ is the true value and $ \\hat{y_i} $ is the predicted value.\n",
    "\n",
    "- **Cross-entropy loss**: Used for classification tasks, defined as:\n",
    "  $$ \\text{Cross-Entropy} = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{k} y_{ij} \\log(\\hat{y_{ij}}) $$\n",
    "  where $ y_{ij} $ is the binary indicator (0 or 1) if class label $ j $ is the correct classification for input $ i $, and $ \\hat{y_{ij}} $ is the predicted probability of $ i $ being in class $ j $.\n",
    "\n",
    "#### 4. Backpropagation\n",
    "\n",
    "Backpropagation is the process of adjusting the network's weights to minimize the loss. It involves calculating the gradient of the loss function with respect to each weight and updating the weights using gradient descent.\n",
    "\n",
    "##### Gradient descent\n",
    "The weight update rule for gradient descent is:\n",
    "$$ w_{ij} = w_{ij} - \\eta \\frac{\\partial L}{\\partial w_{ij}} $$\n",
    "where:\n",
    "- $ \\eta $ is the learning rate, a hyperparameter that controls the step size of the update.\n",
    "- $ \\frac{\\partial L}{\\partial w_{ij}} $ is the partial derivative of the loss function with respect to the weight $ w_{ij} $.\n",
    "\n",
    "##### Calculating gradients\n",
    "The gradients are computed using the chain rule of calculus. For a given weight $ w_{ij} $, the gradient is calculated as:\n",
    "$$ \\frac{\\partial L}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial a_j} \\cdot \\frac{\\partial a_j}{\\partial z_j} \\cdot \\frac{\\partial z_j}{\\partial w_{ij}} $$\n",
    "\n",
    "The partial derivatives are:\n",
    "- $ \\frac{\\partial L}{\\partial a_j} $: The derivative of the loss with respect to the neuron's activation, which depends on the loss function.\n",
    "- $ \\frac{\\partial a_j}{\\partial z_j} $: The derivative of the activation function.\n",
    "- $ \\frac{\\partial z_j}{\\partial w_{ij}} $: The derivative of the weighted sum with respect to the weight, which is the input to the neuron.\n",
    "\n",
    "#### 5. Training the network\n",
    "\n",
    "Training a neural network involves the following steps:\n",
    "1. **Initialize weights**: Set the initial values of the weights, typically using small random values.\n",
    "2. **Forward propagation**: Compute the outputs of the network for a batch of input data.\n",
    "3. **Compute loss**: Calculate the loss using the predicted outputs and the true targets.\n",
    "4. **Backpropagation**: Compute the gradients of the loss with respect to the weights.\n",
    "5. **Update weights**: Adjust the weights using gradient descent.\n",
    "6. **Repeat**: Iterate over the training data for a specified number of epochs until the loss converges.\n",
    "\n",
    "#### 6. Regularization techniques\n",
    "\n",
    "To prevent overfitting, various regularization techniques can be applied:\n",
    "\n",
    "- **L2 regularization (Ridge)**: Adds a penalty proportional to the sum of the squares of the weights to the loss function.\n",
    "  $$ L_{\\text{ridge}} = L + \\lambda \\sum_{j} w_j^2 $$\n",
    "  where $ \\lambda $ is the regularization parameter.\n",
    "\n",
    "- **L1 regularization (Lasso)**: Adds a penalty proportional to the sum of the absolute values of the weights to the loss function.\n",
    "  $$ L_{\\text{lasso}} = L + \\lambda \\sum_{j} |w_j| $$\n",
    "\n",
    "- **Dropout**: Randomly sets a fraction of the neurons to zero during training, which helps prevent the network from becoming too reliant on any single neuron and improves generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for building neural networks in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install numpy matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch version: 2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# Verify CUDA availability\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch version:\", torch.__version__)\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules for building neural networks in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you define the architecture of a neural network using `nn.Module` in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define a simple neural network model\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(784, 128)  # Input layer to first hidden layer\n",
    "        self.fc2 = nn.Linear(128, 64)   # First hidden layer to second hidden layer\n",
    "        self.fc3 = nn.Linear(64, 10)    # Second hidden layer to output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        x = F.relu(self.fc1(x))  # Apply ReLU activation after first layer\n",
    "        x = F.relu(self.fc2(x))  # Apply ReLU activation after second layer\n",
    "        x = self.fc3(x)          # Output layer\n",
    "        return x\n",
    "\n",
    "# Instantiate the network\n",
    "model = SimpleNet()\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you initialize the weights and biases of a neural network?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0772, -0.0792,  0.0447,  ...,  0.0433, -0.0731,  0.0249],\n",
      "        [-0.0182,  0.0168,  0.0360,  ..., -0.0565, -0.0811,  0.0800],\n",
      "        [-0.0569,  0.0500,  0.0031,  ..., -0.0366,  0.0690,  0.0143],\n",
      "        ...,\n",
      "        [ 0.0274,  0.0310,  0.0844,  ..., -0.0455,  0.0587,  0.0036],\n",
      "        [ 0.0530,  0.0009, -0.0198,  ...,  0.0386,  0.0513, -0.0079],\n",
      "        [-0.0089,  0.0064,  0.0331,  ..., -0.0568,  0.0276, -0.0304]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# One way to do it: custom initialization\n",
    "import torch.nn.init as init\n",
    "\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        # Custom initialization of weights and biases\n",
    "        init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')  # Suitable for layers with ReLU activation functions. Maintains variance of input and output distributions\n",
    "        init.constant_(self.fc1.bias, 0)  # Sets biases to a constant value, often zero\n",
    "        \n",
    "        init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n",
    "        init.constant_(self.fc2.bias, 0)\n",
    "        \n",
    "        init.xavier_uniform_(self.fc3.weight)  # Suitable for layers with linear or tanh activation functions. Keeps variance of activations uniform across layers\n",
    "        init.constant_(self.fc3.bias, 0)\n",
    "\n",
    "# Instantiate the network\n",
    "model = CustomNet()\n",
    "\n",
    "# Print the initialized weights and biases\n",
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomNet(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative: use \"apply\" function\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        init.constant_(m.bias, 0)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you choose activation functions for the layers of your neural network?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU: Widely used in hidden layers of deep neural networks.\n",
    "def relu(x):\n",
    "    return F.relu(x)\n",
    "\n",
    "# Leaky ReLU: Alternative to ReLU to address the dying ReLU problem.\n",
    "def leaky_relu(x, negative_slope=0.01):\n",
    "    return F.leaky_relu(x, negative_slope=negative_slope)\n",
    "\n",
    "# Sigmoid: Used in the output layer for binary classification.\n",
    "def sigmoid(x):\n",
    "    return torch.sigmoid(x)\n",
    "\n",
    "# Tanh: Used in hidden layers when data is normalized.\n",
    "def tanh(x):\n",
    "    return torch.tanh(x)\n",
    "\n",
    "# Softmax: Used in the output layer for multi-class classification.\n",
    "def softmax(x, dim=0):  # Note: Adjust dim as needed for higher-dimensional tensors\n",
    "    return F.softmax(x, dim=dim)\n",
    "\n",
    "# Swish: Alternative to ReLU for potential performance gains.\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:  tensor([-1.,  0.,  1.,  2.])\n",
      "ReLU:  tensor([0., 0., 1., 2.])\n",
      "Leaky ReLU:  tensor([-0.0100,  0.0000,  1.0000,  2.0000])\n",
      "Sigmoid:  tensor([0.2689, 0.5000, 0.7311, 0.8808])\n",
      "Tanh:  tensor([-0.7616,  0.0000,  0.7616,  0.9640])\n",
      "Softmax:  tensor([0.0321, 0.0871, 0.2369, 0.6439])\n",
      "Swish:  tensor([-0.2689,  0.0000,  0.7311,  1.7616])\n"
     ]
    }
   ],
   "source": [
    "# Example tensor\n",
    "x = torch.tensor([-1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "# Apply and print each activation function\n",
    "print(\"Input Tensor: \", x)\n",
    "print(\"ReLU: \", relu(x))\n",
    "print(\"Leaky ReLU: \", leaky_relu(x))\n",
    "print(\"Sigmoid: \", sigmoid(x))\n",
    "print(\"Tanh: \", tanh(x))\n",
    "print(\"Softmax: \", softmax(x, dim=0))\n",
    "print(\"Swish: \", swish(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomNet(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# In practice, using Leaky ReLU:\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # ReLU activation for hidden layer 1\n",
    "        x = F.leaky_relu(self.fc2(x), negative_slope=0.01)  # Leaky ReLU for hidden layer 2\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)  # Softmax for output layer\n",
    "\n",
    "model = CustomNet()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: What is the forward pass in a neural network?**\n",
    "\n",
    "It's the process of passing input data through the network's layers to produce an output. This involves applying a series of linear transformations (like matrix multiplications) and nonlinear transformations (activation functions) to the input data, layer by layer, until the final output is obtained.\n",
    "\n",
    "In practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the forward pass: tensor([[ 0.1288, -0.0053,  0.0402,  0.0342,  0.0256,  0.1955,  0.0056, -0.0327,\n",
      "          0.0131, -0.0289]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass: apply linear transformation and activation function\n",
    "        x = F.relu(self.fc1(x))  # First hidden layer with ReLU activation\n",
    "        x = F.relu(self.fc2(x))  # Second hidden layer with ReLU activation\n",
    "        x = self.fc3(x)          # Output layer (typically followed by a softmax or other activation in practice)\n",
    "        return x\n",
    "\n",
    "model = SimpleNet()\n",
    "\n",
    "# Example input (batch size of 1, 784 features)\n",
    "input_data = torch.randn(1, 784)\n",
    "\n",
    "# Perform a forward pass\n",
    "output = model(input_data)\n",
    "\n",
    "# Print the output\n",
    "print(\"Output of the forward pass:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you compute the outputs of a neural network during the forward pass?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the forward pass: tensor([[ 0.1225,  0.0690, -0.0277, -0.0237,  0.1951, -0.0845, -0.1613, -0.0534,\n",
      "          0.1497,  0.0286]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Same as above, so no need to redefine the model\n",
    "model = SimpleNet()\n",
    "\n",
    "input_data = torch.randn(1, 784)\n",
    "\n",
    "output = model(input_data)\n",
    "\n",
    "print(\"Output of the forward pass:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you define the loss function for a neural network in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss: Used for multi-class classification problems, combining nn.LogSoftmax and nn.NLLLoss in a single class\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Mean Squared Error (MSE) loss: Used for regression problems\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Binary cross-entropy loss: Used for binary classification problems\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.5102105140686035\n"
     ]
    }
   ],
   "source": [
    "# Define a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNet()\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "input_data = torch.randn(1, 784)\n",
    "target = torch.tensor([3])  # Example target class\n",
    "\n",
    "output = model(input_data)\n",
    "\n",
    "# Compute the loss\n",
    "loss = loss_fn(output, target)\n",
    "\n",
    "# Print the loss\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: What are some common loss functions used in neural networks?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross entropy with logits loss: Used in binary classification problems, combining a sigmoid layer and the BCELoss in a single class\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Kullback-Leibler divergence loss: Used in variational autoencoders and other generative models\n",
    "loss_fn = nn.KLDivLoss()\n",
    "\n",
    "# Huber loss: Used in regression problems, combining advantages of MSE and MAE\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "# Negative log-likelihood loss: Used in classification tasks with probabilistic outputs\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "# Cosine embedding loss: Used in similarity learning\n",
    "loss_fn = nn.CosineEmbeddingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 2.1368021965026855\n",
      "Cross Entropy Loss: 1.2892800569534302\n"
     ]
    }
   ],
   "source": [
    "# In practice:\n",
    "input_data = torch.randn(1, 5)\n",
    "target_regression = torch.randn(1, 5)\n",
    "\n",
    "input_classification = torch.randn(1, 5)\n",
    "target_classification = torch.tensor([1])\n",
    "\n",
    "# Define a simple linear model\n",
    "model = nn.Linear(5, 5)\n",
    "\n",
    "# Forward pass for regression\n",
    "output_regression = model(input_data)\n",
    "loss_fn_mse = nn.MSELoss()\n",
    "loss_mse = loss_fn_mse(output_regression, target_regression)\n",
    "print(\"MSE Loss:\", loss_mse.item())\n",
    "\n",
    "# Forward pass for classification\n",
    "output_classification = model(input_classification)\n",
    "loss_fn_cross_entropy = nn.CrossEntropyLoss()  # CrossEntropyLoss expects the output to be logits (raw scores) and target to be class indices\n",
    "loss_cross_entropy = loss_fn_cross_entropy(output_classification, target_classification)\n",
    "print(\"Cross Entropy Loss:\", loss_cross_entropy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: What is the backpropagation algorithm and how does it work?**\n",
    "\n",
    "The backpropagation algorithm is a fundamental method used for training artificial neural networks, which works by minimizing the loss function by adjusting the weights of the network through a process called gradient descent. It works as follows:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Initialize the weights and biases of the network randomly or using specific initialization techniques.\n",
    "\n",
    "2. **Forward pass**:\n",
    "   - For each layer in the network, compute the output by applying the linear transformation and activation function.\n",
    "   - Calculate the loss using the loss function.\n",
    "\n",
    "3. **Compute gradients**:\n",
    "   - Starting from the output layer, compute the gradient of the loss with respect to each weight and bias by applying the chain rule.\n",
    "   - The gradient for a weight is computed as the derivative of the loss with respect to the weight.\n",
    "\n",
    "4. **Update weights**:\n",
    "   - Adjust the weights and biases using the computed gradients. This is typically done using an optimization algorithm like gradient descent.\n",
    "\n",
    "5. **Repeat**:\n",
    "   - Repeat the forward and backward passes for a number of epochs or until the loss converges to a minimum value.\n",
    "\n",
    "In practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.299036979675293\n"
     ]
    }
   ],
   "source": [
    "# Define a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the network, loss function, and optimizer\n",
    "model = SimpleNet()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "input_data = torch.randn(64, 784)  # Batch of 64 samples, each with 784 features\n",
    "target = torch.randint(0, 10, (64,))  # Random target classes for the batch\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_data)\n",
    "loss = loss_fn(output, target)\n",
    "\n",
    "# Backward pass and optimization\n",
    "optimizer.zero_grad()  # Zero the gradients\n",
    "loss.backward()        # Compute the gradients\n",
    "optimizer.step()       # Update the weights\n",
    "\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you calculate gradients during backpropagation in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for fc1.weight: tensor([[-4.3481e-04,  4.3330e-04,  2.4760e-04,  ..., -2.0326e-04,\n",
      "          9.0320e-04,  4.4621e-04],\n",
      "        [-9.8926e-05, -2.0385e-03, -2.6405e-03,  ..., -1.1967e-03,\n",
      "          1.0194e-03, -9.1280e-04],\n",
      "        [-4.0644e-03,  8.3270e-04,  1.0809e-04,  ..., -1.4996e-03,\n",
      "          2.4512e-03, -1.6319e-04],\n",
      "        ...,\n",
      "        [ 1.1106e-03, -7.4263e-04,  1.0373e-04,  ...,  2.6913e-04,\n",
      "         -2.9604e-04, -1.4903e-03],\n",
      "        [-8.0136e-04,  1.4919e-03,  2.0869e-03,  ...,  6.7085e-04,\n",
      "         -6.0105e-04,  6.1063e-04],\n",
      "        [-8.5334e-04, -2.2786e-03, -1.3009e-04,  ..., -3.9481e-04,\n",
      "         -2.1577e-04,  1.7658e-03]])\n",
      "Gradients for fc1.bias: tensor([-1.1967e-03, -1.4085e-03,  2.5377e-03,  2.6322e-03,  1.1606e-03,\n",
      "         2.8614e-04, -1.4909e-04, -6.4631e-04, -9.6031e-04,  1.5689e-03,\n",
      "        -7.1519e-04, -1.6154e-03, -3.7719e-03,  4.1774e-03,  1.6580e-03,\n",
      "         2.3365e-03,  2.1302e-03, -1.0760e-03, -2.3221e-03, -2.2153e-03,\n",
      "        -1.4630e-03,  2.5310e-03,  7.3476e-04,  1.5472e-03,  8.3611e-04,\n",
      "         1.3223e-03,  2.6751e-03, -2.0281e-03,  3.9283e-04, -3.3107e-04,\n",
      "         2.4596e-04, -2.9502e-03,  3.1029e-03,  1.0022e-03,  2.1813e-03,\n",
      "         7.6592e-05, -6.7226e-04, -4.4645e-04,  1.4312e-03,  1.5185e-03,\n",
      "        -2.0576e-03, -1.0160e-03,  3.0458e-04,  3.6727e-03,  4.2785e-04,\n",
      "        -2.5668e-03,  2.0076e-04, -6.6385e-04,  2.9796e-04, -3.6326e-03,\n",
      "        -2.3858e-03, -2.3659e-03, -3.7913e-04,  3.0605e-05,  1.7446e-03,\n",
      "        -1.0294e-03, -6.9054e-04, -1.1972e-03, -4.1190e-03,  2.9125e-03,\n",
      "        -1.2963e-03,  2.7772e-03, -1.5481e-03, -2.3004e-04, -2.0618e-03,\n",
      "        -1.2092e-03, -2.6943e-03,  1.9980e-04,  4.1814e-03, -2.8013e-04,\n",
      "         3.0622e-04, -3.0820e-03, -1.6334e-03, -6.1651e-05,  4.5383e-03,\n",
      "         2.1259e-05,  2.3258e-03, -3.4280e-05,  4.5207e-04, -1.4821e-03,\n",
      "        -1.0088e-04,  8.9997e-04,  3.5621e-03, -2.6814e-03,  5.6287e-04,\n",
      "        -3.9543e-03,  3.9323e-04, -9.9298e-04,  6.8140e-04, -1.8723e-04,\n",
      "         1.9205e-03,  5.1656e-04,  1.4720e-03,  1.3366e-03,  2.4244e-03,\n",
      "        -2.6584e-03, -3.4716e-03, -8.2636e-04,  5.9217e-04, -1.0041e-03,\n",
      "         9.1489e-04, -1.1355e-03,  1.5190e-03,  3.5627e-04, -2.3879e-03,\n",
      "         4.7240e-03,  2.3689e-03,  3.5780e-03, -1.0358e-03,  6.6032e-04,\n",
      "        -7.1751e-04, -6.9448e-04, -1.0518e-03,  1.2038e-03, -9.8570e-04,\n",
      "         1.0622e-04, -3.9815e-03,  1.1102e-03,  1.6476e-03,  4.4691e-04,\n",
      "         6.8160e-04,  2.0214e-03,  6.9384e-04,  5.5790e-04, -3.4076e-04,\n",
      "         1.2659e-03,  1.0527e-03,  3.4924e-03])\n",
      "Gradients for fc2.weight: tensor([[-3.5107e-04, -8.4735e-04, -5.4734e-04,  ...,  7.0027e-04,\n",
      "          3.8212e-04, -2.9319e-04],\n",
      "        [-2.7273e-03,  6.3815e-04,  4.7829e-04,  ..., -6.7158e-04,\n",
      "         -6.7925e-03, -2.6496e-03],\n",
      "        [-1.4012e-03, -4.1326e-05,  1.6177e-04,  ..., -3.2347e-03,\n",
      "         -1.8354e-03, -2.7205e-03],\n",
      "        ...,\n",
      "        [-1.5261e-04,  1.8556e-03,  8.3167e-04,  ...,  3.2071e-03,\n",
      "         -2.9657e-03,  3.1255e-03],\n",
      "        [-1.2736e-03, -1.0030e-03, -1.0638e-05,  ..., -2.5626e-03,\n",
      "          1.5359e-03, -1.6339e-03],\n",
      "        [-3.1567e-03,  1.4670e-03,  1.5982e-03,  ...,  2.2468e-04,\n",
      "          1.8633e-03,  1.9570e-03]])\n",
      "Gradients for fc2.bias: tensor([ 7.5800e-04, -9.8857e-03, -1.1716e-02,  6.1815e-03,  2.8904e-03,\n",
      "         9.1996e-03, -9.6232e-03,  2.3993e-03, -1.7660e-03,  5.4194e-04,\n",
      "         2.6271e-04,  4.4619e-03, -1.3298e-02,  8.0802e-03, -4.2728e-03,\n",
      "         3.1839e-03, -3.2330e-03,  8.6078e-03, -1.3903e-03,  2.7902e-03,\n",
      "         2.9298e-03,  1.9581e-03, -1.0945e-02,  9.9970e-03,  2.8185e-03,\n",
      "        -6.0244e-03,  6.7592e-03, -7.6107e-03,  3.3202e-03,  3.8494e-03,\n",
      "        -5.8841e-03,  3.4626e-03, -2.3930e-03, -7.7728e-03, -7.5565e-04,\n",
      "        -3.0364e-03, -2.0490e-03, -2.2681e-03,  6.6126e-04, -1.6381e-03,\n",
      "        -1.4578e-02,  5.9093e-03,  2.9543e-04,  1.7520e-03,  2.7927e-03,\n",
      "        -2.2137e-03,  6.2569e-03, -8.8405e-03,  1.4790e-02,  1.7484e-04,\n",
      "         8.6344e-03,  4.4926e-04, -9.8258e-03,  5.5155e-05, -6.5941e-03,\n",
      "        -7.7385e-04,  2.7097e-03, -9.0731e-03,  5.6076e-04,  3.7144e-03,\n",
      "        -2.3936e-03,  1.7883e-03, -2.3427e-03,  8.2198e-03])\n",
      "Gradients for fc3.weight: tensor([[ 4.1829e-04,  8.2072e-03,  7.6197e-03, -4.0020e-05, -5.1038e-04,\n",
      "          1.4558e-02,  2.3053e-03,  1.4411e-02,  6.1407e-03,  3.9798e-03,\n",
      "         -8.4729e-04,  3.3126e-03,  2.2790e-02,  4.3868e-03,  6.5576e-03,\n",
      "          1.3682e-02,  2.8741e-03,  8.3410e-03, -1.3259e-03,  1.3696e-02,\n",
      "          9.0896e-03,  5.8234e-03,  1.1243e-02,  2.1397e-02,  1.5939e-02,\n",
      "          6.5773e-03,  3.0382e-02,  4.9982e-03,  1.9763e-03,  5.0337e-03,\n",
      "         -2.6961e-03,  3.6726e-03,  9.1992e-03,  1.3627e-02,  1.6083e-03,\n",
      "          6.5682e-04,  5.9792e-03,  1.8973e-04,  4.2901e-03,  3.9196e-03,\n",
      "          5.9520e-03,  1.0544e-02,  1.9485e-03,  2.9269e-03,  7.1049e-03,\n",
      "          5.0864e-03,  3.4976e-03,  7.4733e-03,  1.3515e-02,  4.4816e-03,\n",
      "          6.4048e-03,  3.9617e-03,  5.4932e-03,  1.7355e-03,  7.1892e-03,\n",
      "          6.1704e-03,  1.8615e-02,  2.3022e-02,  4.4262e-03,  1.5811e-02,\n",
      "          1.5702e-03,  2.7337e-03,  2.7961e-03,  7.9021e-03],\n",
      "        [ 3.6451e-04, -8.8224e-03,  3.8985e-04, -6.3226e-04, -1.7188e-03,\n",
      "         -2.5581e-03, -3.6460e-03, -1.7071e-03,  8.0809e-04,  3.3660e-03,\n",
      "         -4.5251e-04, -7.3443e-03,  3.6899e-04, -4.1948e-04, -2.0223e-03,\n",
      "         -6.4508e-03, -3.9914e-03,  3.6930e-03,  8.1175e-04,  7.5840e-03,\n",
      "         -5.0834e-04,  8.8442e-04, -4.9829e-03, -6.9378e-03,  1.6681e-03,\n",
      "          5.6785e-03,  1.1794e-03, -5.2343e-03,  1.7362e-03, -1.0947e-03,\n",
      "         -9.2872e-03,  2.9956e-03, -5.6373e-03, -9.5119e-03, -2.5426e-03,\n",
      "          5.5661e-04,  2.1469e-03, -4.3837e-04, -3.1760e-05,  6.8065e-04,\n",
      "         -1.5268e-03, -3.5119e-03, -2.0950e-03, -8.9819e-04,  5.5238e-04,\n",
      "         -1.6080e-03, -2.6559e-04, -1.5387e-03,  2.9021e-03, -9.5708e-03,\n",
      "          8.0155e-04,  8.4959e-04, -1.1345e-04,  3.1195e-04,  1.6239e-03,\n",
      "         -1.0178e-03, -4.6870e-03, -1.7291e-02, -8.0733e-04, -6.3333e-03,\n",
      "          1.2217e-03, -2.1068e-03, -7.6579e-04, -7.0256e-03],\n",
      "        [ 1.8022e-04, -5.0860e-03,  1.6145e-03,  1.5394e-03, -3.0945e-03,\n",
      "         -1.1875e-02, -2.5172e-03,  3.0571e-03, -9.5642e-03, -2.6539e-03,\n",
      "          5.8154e-04,  4.0736e-03, -8.9410e-03, -1.8237e-02, -4.1165e-04,\n",
      "          7.5996e-04, -4.3453e-03,  5.1515e-03, -7.7384e-03, -1.2872e-02,\n",
      "         -1.0638e-02, -6.5904e-03, -7.6747e-03, -1.7838e-02, -1.9169e-02,\n",
      "         -4.3358e-03, -1.1365e-02, -5.7247e-03,  1.4478e-03,  1.2564e-03,\n",
      "          2.9252e-03, -2.2744e-03,  2.9628e-03, -8.1215e-04, -2.9675e-03,\n",
      "         -2.4872e-03, -9.1638e-04,  1.7294e-04, -1.1411e-02, -1.0347e-02,\n",
      "         -5.0808e-03, -6.7306e-03,  2.3064e-03, -2.5582e-03,  3.3188e-03,\n",
      "          2.4711e-03, -2.2124e-03, -4.2826e-03,  3.3605e-03, -1.7305e-03,\n",
      "          2.5670e-03, -1.1043e-02,  1.2043e-03, -3.1919e-03,  4.0584e-03,\n",
      "          1.6073e-04, -9.7272e-03, -1.1103e-02,  1.0060e-04, -1.5016e-03,\n",
      "         -1.6473e-03, -1.2163e-02,  2.4806e-03, -1.1529e-04],\n",
      "        [ 1.9335e-04, -8.9748e-03, -3.8557e-03,  5.9268e-04,  1.4670e-03,\n",
      "         -2.9491e-03, -4.4128e-03, -1.2153e-02, -1.8059e-04, -4.5842e-03,\n",
      "         -1.9800e-03, -1.6749e-03, -1.6030e-02,  4.5880e-03,  5.1483e-03,\n",
      "         -4.0997e-03,  1.0680e-03, -7.1974e-03,  4.1177e-03, -3.9730e-03,\n",
      "         -1.6505e-03,  4.8597e-05, -1.1419e-02, -3.8659e-04,  6.2985e-03,\n",
      "         -7.1251e-03, -4.3801e-03,  5.5779e-03,  1.5117e-03, -1.0251e-02,\n",
      "          3.2162e-03, -1.3329e-03,  1.8276e-03, -6.3503e-03, -2.3684e-04,\n",
      "         -8.3907e-04, -6.5293e-04,  1.9899e-04,  2.8364e-03,  1.0412e-04,\n",
      "         -1.8841e-02, -1.2980e-02, -1.4467e-02,  1.5256e-04, -2.8981e-03,\n",
      "          2.1098e-03, -2.1244e-03,  2.7085e-04, -1.6616e-02,  5.4796e-03,\n",
      "         -7.7978e-03, -7.1584e-03, -6.6228e-03, -1.7864e-03, -9.9220e-03,\n",
      "         -9.7922e-03, -1.9035e-03, -8.2111e-03, -4.2014e-03, -1.6630e-02,\n",
      "         -7.3064e-04, -1.5704e-03, -1.4481e-03, -8.2740e-03],\n",
      "        [-2.2957e-04,  5.2588e-03, -1.0763e-03, -2.9484e-03,  4.3158e-03,\n",
      "          7.7860e-03,  9.5374e-04,  3.5018e-03, -3.8014e-04, -1.8048e-04,\n",
      "          5.9528e-04,  2.0139e-03, -4.7595e-03,  2.8239e-03,  3.3876e-03,\n",
      "          3.5840e-04,  3.0210e-03, -4.6264e-03,  4.8283e-03,  1.0514e-02,\n",
      "          5.9856e-03,  1.6358e-03,  3.7194e-03,  1.3554e-02,  1.1862e-02,\n",
      "          4.8305e-03,  7.5502e-03,  2.0231e-03,  1.7818e-03, -2.4259e-03,\n",
      "          8.5259e-04, -1.8193e-04,  8.3020e-03,  6.5022e-03,  3.2499e-03,\n",
      "          4.7829e-04,  3.2315e-03,  1.7259e-04, -2.4094e-05,  5.6571e-04,\n",
      "          4.0595e-03,  7.8208e-03,  1.4191e-03,  6.4553e-03,  4.7354e-04,\n",
      "          2.3753e-04, -1.7581e-03,  5.6395e-03,  6.2152e-03, -9.3345e-04,\n",
      "          5.7345e-03,  3.0577e-04,  6.7482e-03,  5.4346e-05, -1.2391e-03,\n",
      "          5.1892e-03,  6.7188e-03,  1.2861e-02,  1.4054e-03,  4.0974e-03,\n",
      "          1.3383e-03,  3.9105e-03, -2.2793e-03,  7.1053e-03],\n",
      "        [ 3.8408e-04, -1.1562e-03,  4.4102e-03,  3.8382e-03,  5.0292e-03,\n",
      "         -3.7260e-03,  7.4563e-04, -5.7376e-03, -3.6465e-04, -3.3009e-03,\n",
      "          6.6561e-04,  5.1884e-03, -3.8971e-03, -3.4400e-03,  1.5827e-03,\n",
      "          6.3859e-03, -2.1775e-04,  2.2501e-03,  5.7418e-04,  8.2809e-03,\n",
      "          4.3715e-03,  3.3749e-03,  1.1873e-02,  1.1466e-02,  1.3173e-02,\n",
      "          3.7898e-03,  1.2730e-03,  1.4461e-03,  1.8930e-03,  2.3613e-03,\n",
      "          2.1290e-03,  2.5703e-04,  1.8941e-03, -1.0004e-03, -1.8419e-03,\n",
      "          5.2713e-04, -3.6763e-03,  1.9541e-04,  4.1059e-03,  1.7244e-03,\n",
      "         -3.1147e-03,  4.6544e-03,  4.8142e-03, -4.8924e-03,  4.0726e-03,\n",
      "         -7.0076e-03,  3.3178e-03,  1.8658e-03,  1.6237e-03,  4.1675e-03,\n",
      "         -2.5944e-03,  1.8856e-03, -6.5664e-04,  1.7318e-03,  3.5493e-03,\n",
      "          5.2079e-03,  6.8545e-03,  1.0263e-02,  2.9100e-03,  1.2369e-03,\n",
      "         -1.3136e-03,  1.7574e-03,  1.6109e-03,  4.1531e-03],\n",
      "        [-6.8874e-04, -1.1992e-03, -6.9555e-05,  2.9376e-04,  4.8074e-03,\n",
      "         -3.2535e-03,  3.1098e-03, -9.7558e-03,  3.5660e-03,  3.9849e-03,\n",
      "          6.7384e-04, -4.0851e-03,  6.5001e-03,  5.5604e-04, -9.1387e-03,\n",
      "          7.4971e-03,  5.1540e-03, -5.5042e-03,  3.5823e-03,  7.0170e-03,\n",
      "          6.6419e-03,  3.3274e-03,  2.0422e-03, -2.7431e-03, -7.9883e-03,\n",
      "         -2.0587e-03,  1.1332e-02, -2.0188e-03, -2.7666e-03,  5.0116e-04,\n",
      "          3.7856e-03, -1.9827e-03, -4.2602e-03,  5.7265e-03, -1.5491e-03,\n",
      "          4.2920e-04, -7.2312e-04, -3.6138e-04, -4.4373e-04,  5.3928e-03,\n",
      "          4.8902e-03,  5.9010e-03,  7.0244e-04,  3.7753e-03, -4.5480e-03,\n",
      "          4.0515e-03,  2.4597e-04, -7.0345e-03,  1.3684e-03, -4.0077e-03,\n",
      "         -1.4476e-03,  3.7847e-03,  2.8440e-04, -2.8163e-04, -2.7671e-03,\n",
      "          5.0518e-03,  5.0950e-03,  9.8233e-03,  6.8078e-04,  1.0588e-02,\n",
      "          1.4806e-03,  2.8910e-03,  2.9297e-03, -6.8307e-03],\n",
      "        [ 4.1154e-04,  5.3440e-03,  1.9926e-03,  2.2591e-03, -6.4500e-03,\n",
      "          1.5031e-02,  2.9184e-03,  1.9005e-02,  5.7510e-03,  3.6281e-03,\n",
      "          6.3102e-04,  6.5024e-03,  1.9369e-02,  4.0708e-03,  4.3447e-03,\n",
      "          9.9958e-03,  2.0695e-03,  7.0036e-03,  4.6533e-03,  5.1584e-03,\n",
      "         -7.6769e-04, -2.5865e-03, -4.8941e-04,  7.5259e-03,  1.8131e-03,\n",
      "          4.4380e-03,  5.7935e-03,  1.4744e-03, -2.6100e-03,  4.7236e-03,\n",
      "          3.6370e-03,  3.3880e-03,  8.5851e-03,  2.1853e-03,  1.5375e-03,\n",
      "          5.3124e-04,  5.4866e-03, -4.8999e-04, -1.1206e-03,  6.6389e-03,\n",
      "          4.3231e-03,  8.6644e-03,  5.6368e-03,  4.8135e-03,  2.8659e-04,\n",
      "          3.2743e-03,  3.2542e-03,  7.2177e-03,  8.6181e-03,  9.6087e-04,\n",
      "          3.1232e-03,  3.9398e-03,  5.5858e-03,  1.7308e-03,  4.4221e-03,\n",
      "         -3.0812e-03, -1.8179e-03,  1.8992e-02,  5.0189e-04,  9.4061e-03,\n",
      "          1.3759e-03,  5.8187e-03,  5.9374e-04, -4.5652e-04],\n",
      "        [-1.4655e-03, -6.5060e-04, -4.4042e-03,  1.8850e-03,  1.8405e-03,\n",
      "         -1.1091e-02, -1.9151e-03, -6.0632e-03, -3.1626e-03, -2.5842e-03,\n",
      "         -5.4662e-04, -6.6364e-03, -1.9126e-02,  1.4435e-03, -5.3334e-03,\n",
      "         -9.1592e-03, -7.0225e-03, -7.1661e-03, -5.8832e-03, -1.4361e-02,\n",
      "         -1.1374e-02, -2.5252e-03, -4.7452e-03, -1.0911e-02, -1.9342e-02,\n",
      "          7.8998e-04, -1.6850e-02, -3.2704e-04, -3.3618e-03, -6.2097e-04,\n",
      "         -2.4327e-04, -3.7117e-03, -1.1546e-02, -6.5153e-03, -4.8779e-05,\n",
      "         -3.9976e-05, -4.1717e-03,  1.5665e-04,  2.1943e-03, -6.8136e-03,\n",
      "          3.8158e-03, -4.4317e-03, -7.2620e-04, -3.7100e-03, -1.6100e-03,\n",
      "         -3.4510e-03, -7.4295e-03, -1.0160e-02, -9.4796e-03,  1.1452e-03,\n",
      "         -8.0107e-03, -5.3876e-04, -4.3654e-03, -2.1614e-03, -3.9911e-03,\n",
      "         -6.5213e-03, -1.3444e-02, -1.6375e-02, -5.7104e-03, -1.3003e-02,\n",
      "         -2.3701e-03, -1.5885e-03, -5.2288e-03,  1.7378e-03],\n",
      "        [ 4.3178e-04,  7.0793e-03, -6.6211e-03, -6.7874e-03, -5.6862e-03,\n",
      "         -1.9219e-03,  2.4582e-03, -4.5589e-03, -2.6136e-03, -1.6550e-03,\n",
      "          6.7909e-04, -1.3501e-03,  3.7245e-03,  4.2279e-03, -4.1149e-03,\n",
      "         -1.8969e-02,  1.3903e-03, -1.9451e-03, -3.6200e-03, -2.1044e-02,\n",
      "         -1.1504e-03, -3.3925e-03,  4.3360e-04, -1.5126e-02, -4.2540e-03,\n",
      "         -1.2584e-02, -2.4915e-02, -2.2149e-03, -1.6084e-03,  5.1645e-04,\n",
      "         -4.3190e-03, -8.2956e-04, -1.1328e-02, -3.8509e-03,  2.7909e-03,\n",
      "          1.8698e-04, -6.7039e-03,  2.0343e-04, -3.9550e-04, -1.8661e-03,\n",
      "          5.5228e-03, -9.9301e-03,  4.6072e-04, -6.0649e-03, -6.7526e-03,\n",
      "         -5.1641e-03,  3.4744e-03,  5.4887e-04, -1.1507e-02,  7.7536e-06,\n",
      "          1.2196e-03,  4.0133e-03, -7.5576e-03,  1.8569e-03, -2.9235e-03,\n",
      "         -1.3675e-03, -5.7031e-03, -2.1980e-02,  6.9427e-04, -3.6714e-03,\n",
      "         -9.2512e-04,  3.1717e-04, -6.8904e-04,  1.8038e-03]])\n",
      "Gradients for fc3.bias: tensor([ 0.0741, -0.0165, -0.0325, -0.0450,  0.0324,  0.0251,  0.0113,  0.0374,\n",
      "        -0.0544, -0.0320])\n",
      "Loss: 2.312669038772583\n"
     ]
    }
   ],
   "source": [
    "# Reinstantiate the network\n",
    "model = SimpleNet()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "input_data = torch.randn(64, 784)\n",
    "target = torch.randint(0, 10, (64,))\n",
    "\n",
    "output = model(input_data)\n",
    "loss = loss_fn(output, target)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# Print gradients for each layer\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Gradients for {name}: {param.grad}\")\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you update the weights and biases of a neural network during training?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 2.3123\n",
      "Epoch [200/1000], Loss: 2.3144\n",
      "Epoch [300/1000], Loss: 2.3031\n",
      "Epoch [400/1000], Loss: 2.2769\n",
      "Epoch [500/1000], Loss: 2.2985\n",
      "Epoch [600/1000], Loss: 2.3022\n",
      "Epoch [700/1000], Loss: 2.3070\n",
      "Epoch [800/1000], Loss: 2.2929\n",
      "Epoch [900/1000], Loss: 2.2957\n",
      "Epoch [1000/1000], Loss: 2.3108\n"
     ]
    }
   ],
   "source": [
    "# Use a training loop!\n",
    "model = SimpleNet()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Dummy training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Example input (batch size of 64, 784 features) and target\n",
    "    input_data = torch.randn(64, 784)\n",
    "    target = torch.randint(0, 10, (64,))\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(input_data)\n",
    "    loss = loss_fn(output, target)\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you set up the training loop for a neural network in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Batch 15, Loss: 0.669472873210907\n",
      "Epoch 200/1000, Batch 15, Loss: 0.023556523025035858\n",
      "Epoch 300/1000, Batch 15, Loss: 0.00922602042555809\n",
      "Epoch 400/1000, Batch 15, Loss: 0.0049431053921580315\n",
      "Epoch 500/1000, Batch 15, Loss: 0.003929974511265755\n",
      "Epoch 600/1000, Batch 15, Loss: 0.0027577995788306\n",
      "Epoch 700/1000, Batch 15, Loss: 0.002288937335833907\n",
      "Epoch 800/1000, Batch 15, Loss: 0.0019522793591022491\n",
      "Epoch 900/1000, Batch 15, Loss: 0.0014999427367001772\n",
      "Epoch 1000/1000, Batch 15, Loss: 0.0012638990301638842\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Same as above, but this time using a TensorDataset and a DataLoader. i.e.,\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "input_data = torch.randn(1000, 784)\n",
    "target_data = torch.randint(0, 10, (1000,))\n",
    "\n",
    "dataset = TensorDataset(input_data, target_data)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "model = SimpleNet()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you implement batch training in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Batch 15, Loss: 0.5232671499252319\n",
      "Epoch [200/1000], Batch 15, Loss: 0.024760937318205833\n",
      "Epoch [300/1000], Batch 15, Loss: 0.0106257488951087\n",
      "Epoch [400/1000], Batch 15, Loss: 0.004773051477968693\n",
      "Epoch [500/1000], Batch 15, Loss: 0.003490960691124201\n",
      "Epoch [600/1000], Batch 15, Loss: 0.002814643085002899\n",
      "Epoch [700/1000], Batch 15, Loss: 0.0019772653467953205\n",
      "Epoch [800/1000], Batch 15, Loss: 0.0016450363909825683\n",
      "Epoch [900/1000], Batch 15, Loss: 0.0014117287937551737\n",
      "Epoch [1000/1000], Batch 15, Loss: 0.0012639611959457397\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Same as above, but with new dataset:\n",
    "input_data = torch.randn(1000, 784)\n",
    "target_data = torch.randint(0, 10, (1000,))\n",
    "\n",
    "dataset = TensorDataset(input_data, target_data)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "model = SimpleNet()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you monitor training progress during the training of a neural network?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 0.4798, Accuracy: 99.40%\n",
      "Epoch 200/1000, Loss: 0.0229, Accuracy: 100.00%\n",
      "Epoch 300/1000, Loss: 0.0090, Accuracy: 100.00%\n",
      "Epoch 400/1000, Loss: 0.0053, Accuracy: 100.00%\n",
      "Epoch 500/1000, Loss: 0.0037, Accuracy: 100.00%\n",
      "Epoch 600/1000, Loss: 0.0028, Accuracy: 100.00%\n",
      "Epoch 700/1000, Loss: 0.0022, Accuracy: 100.00%\n",
      "Epoch 800/1000, Loss: 0.0018, Accuracy: 100.00%\n",
      "Epoch 900/1000, Loss: 0.0015, Accuracy: 100.00%\n",
      "Epoch 1000/1000, Loss: 0.0013, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Reinstantiate the network, loss function, and optimizer\n",
    "model = SimpleNet()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Lists to store loss and accuracy\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    # Record loss and accuracy\n",
    "    avg_loss = running_loss / len(data_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    losses.append(avg_loss)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+kAAAHACAYAAADeASmoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABkXklEQVR4nO3de3yT9f3+8StJ2/R8AnqCAlUYICAiKNTDRK0iHkGmwx8bqEy+OlAZTh0eUPGAsqkMdTAVQTcRxSnziGN4YGgBOQoeEBUpUNoCpU3Pp9y/P9IEAuVQmvbO4fV8PCLNnTvJO7fip1c+J4thGIYAAAAAAIDprGYXAAAAAAAAXAjpAAAAAAD4CUI6AAAAAAB+gpAOAAAAAICfIKQDAAAAAOAnCOkAAAAAAPgJQjoAAAAAAH6CkA4AAAAAgJ8IM7uAtuZ0OpWfn6+4uDhZLBazywEAQIZhqKysTBkZGbJa+f7cF2jvAQD+pDltfciF9Pz8fGVmZppdBgAAh9mxY4c6depkdhlBgfYeAOCPjqetD7mQHhcXJ8l1ceLj402uBgAAyeFwKDMz09NGoeVo7wEA/qQ5bX3IhXT3kLf4+HgabQCAX2FYtu/Q3gMA/NHxtPVMfAMAAAAAwE8Q0gEAAAAA8BOEdAAAAAAA/ETIzUkHALSMYRiqr69XQ0OD2aUEDJvNprCwMOacAwCAYyKkAwCOW21trXbv3q3KykqzSwk40dHRSk9PV0REhNmlAAAAP0ZIBwAcF6fTqW3btslmsykjI0MRERH0DB8HwzBUW1urPXv2aNu2berevbusVmabAQCAphHSAQDHpba2Vk6nU5mZmYqOjja7nIASFRWl8PBwbd++XbW1tYqMjDS7JAAA4Kf4Kh8A0Cz0Ap8YrhsAADge/MYAAAAAAICfIKQDAAAAAOAnCOkAAMBnli9friuuuEIZGRmyWCxavHix1+OGYWjq1KlKT09XVFSUcnJytHXrVq9ziouLNXr0aMXHxysxMVHjxo1TeXl5G34KAADMQ0gHAAS966+/XsOHDze7jJBQUVGhfv366bnnnmvy8RkzZmjWrFmaM2eOVq1apZiYGA0dOlTV1dWec0aPHq2vv/5aS5cu1Xvvvafly5dr/PjxbfURAAAwFau7t1Blbb2iwm1sQwQAgKRhw4Zp2LBhTT5mGIZmzpyp++67T1dddZUk6ZVXXlFqaqoWL16sUaNG6dtvv9WSJUv05ZdfauDAgZKkZ555Rpdeeqn+8pe/KCMjo80+CwAAZiCkt8A/V27X00u/11+u6afze6aYXQ4AtCnDMFRV12DKe/vyy9HPPvtMd955pzZu3Kjk5GSNHTtWjzzyiMLCXE3km2++qYceekg//PCDoqOj1b9/f/373/9WTEyMPv30U9111136+uuvFR4ert69e2vBggXq0qWLT2oLNtu2bVNBQYFycnI8xxISEjRo0CDl5uZq1KhRys3NVWJioiegS1JOTo6sVqtWrVqlESNGNPnaNTU1qqmp8dx3OByt90H83Jc/F2vjjhKt/Gmfdu6vMrscAAh4/xg3SB3i7G32foT0FsgrrtS+ilrN+GiLzvtFB1mt9KYDCB1VdQ06ZepHprz3N9OGKjqi5U3Yrl27dOmll+r666/XK6+8ou+++0433XSTIiMj9eCDD2r37t267rrrNGPGDI0YMUJlZWX63//+J8MwVF9fr+HDh+umm27Sa6+9ptraWq1evZqRVUdRUFAgSUpNTfU6npqa6nmsoKBAKSneX3yHhYUpOTnZc05Tpk+froceesjHFfu3/JIqPffJD9pfWes59n1huX4oYv4+APhSg9No0/cjpLfALeedrNdW5enb3Q69v2m3rujHEDwACCR/+9vflJmZqWeffVYWi0U9e/ZUfn6+7r77bk2dOlW7d+9WfX29rr76ak/veN++fSW5FjcrLS3V5ZdfrpNPPlmS1KtXL9M+S6ibMmWKJk+e7LnvcDiUmZlpYkWtq6isWufO+OSIvzie3jlR3VPidEZWstLiI9u4OgAILonR4W36foT0FkiKidBNvzxJTy39Xk8t/V6X9ElTuI21+ACEhqhwm76ZNtS09/aFb7/9VtnZ2V6932effbbKy8u1c+dO9evXTxdeeKH69u2roUOH6uKLL9avfvUrJSUlKTk5Wddff72GDh2qiy66SDk5Obr22muVnp7uk9qCUVpamiSpsLDQ6zoVFhbqtNNO85xTVFTk9bz6+noVFxd7nt8Uu90uu73thiKayek0dOeir9TgNGSzWvTHi3soxu76O2GRlH1ye3VLiTW3SADACSNRttCN52SpXUyEtu2t0Cu5280uBwDajMViUXREmCm3thpSbrPZtHTpUn344Yc65ZRT9Mwzz6hHjx7atm2bJGnevHnKzc3VWWedpddff12/+MUvtHLlyjapLRBlZWUpLS1Ny5Yt8xxzOBxatWqVsrOzJUnZ2dkqKSnR2rVrPed8/PHHcjqdGjRoUJvX7I+WfF2gz77fI0l6/Oq+umXIyRqT3VVjsrvqt9ldCegAEOAI6S0Uaw/TnUN7SJJm/vd77SuvOcYzAAD+olevXsrNzZVhHBgy/PnnnysuLk6dOnWS5Poy4uyzz9ZDDz2k9evXKyIiQm+//bbn/P79+2vKlCn64osv1KdPHy1YsKDNP4c/KS8v14YNG7RhwwZJrsXiNmzYoLy8PFksFk2aNEmPPPKI3nnnHW3atEljxoxRRkaGZ4u8Xr166ZJLLtFNN92k1atX6/PPP9fEiRM1atQoVnZvtGjNDknSuHOydM3A4B3SDwChiuHuPnDNwEz9Y+V2fZ3v0F/+872mX93X7JIAAIcoLS31BEe38ePHa+bMmbr11ls1ceJEbdmyRQ888IAmT57sWU182bJluvjii5WSkqJVq1Zpz5496tWrl7Zt26bnn39eV155pTIyMrRlyxZt3bpVY8aMMecD+ok1a9bo/PPP99x3zxMfO3as5s+fr7vuuksVFRUaP368SkpKdM4552jJkiWKjDwwb/rVV1/VxIkTdeGFF8pqtWrkyJGaNWtWm38Wf7Q+b78+2bJHVos0elBns8sBALQCQroP2KwWPXBFb13791wt/DJPvxncWb0zEswuCwBwkE8//VT9+/f3OjZu3Dh98MEHuvPOO9WvXz8lJydr3Lhxuu+++yRJ8fHxWr58uWbOnCmHw6EuXbroySef1LBhw1RYWKjvvvtOL7/8svbt26f09HRNmDBB//d//2fGx/MbQ4YM8RqZcCiLxaJp06Zp2rRpRzwnOTk55EckHMm9b2+WJF3ZL0MndWBYOwAEI4txtJY0CDkcDiUkJKi0tFTx8fE+fe1bX1uvdzfm68yuyXr9/wazDQ+AoFJdXa1t27YpKyvLq9cTx+do168126ZQFYzXtLSyTv2m/UeStOLu89UpKdrkigAAx6s57RJz0n3oT8N6KjLcqtU/F+v9TbvNLgcAAASRdTv2S5Iyk6MI6AAQxAjpPtQxMUo3n+faK3f6B9+pqrbB5IoAAECweHdDviTp3O4dTK4EANCaCOk+9n+/PFkdE6O0q6RKf1/+o9nlAACAIFBV26B3NrpC+jUDOplcDQCgNRHSfSwqwqYpl/aUJM357EftKqkyuSIAABDodu6vVL3TUJw9TP07J5ldDgCgFRHSW8FlfdN1ZlayquucevzD78wuBwB8KsTWG/UZrhtaYud+15f+nZKZiw4AwY6Q3gosFoseuOIUWSzSuxvztXpbsdklAUCLhYeHS5IqKytNriQwua+b+zoCzbFjv+u/n05JUSZXAgBobeyT3kp6ZyRo1Bmd9drqPD307td6Z+I5slnZkg1A4LLZbEpMTFRRUZEkKTo6mq0mj4NhGKqsrFRRUZESExNls9nMLgkBaGthuSTppA4xJlcCAGhthPRW9MeLf6H3vsrX1/kOvbFmh647s7PZJQFAi6SlpUmSJ6jj+CUmJnquH9BcWwrLJEm/SIkzuRIAQGsjpLeidrF2Tcr5hR5+7xs9vfR7jejfUZHh9KAACFwWi0Xp6elKSUlRXV2d2eUEjPDwcHrQccL2V9RqfZ5rj/TTOieaWwwAoNUR0lvZbwd30UsrtmlXSZVeXZWncedkmV0SALSYzWYjdAJtZMHqPNU1GOqdEa+TO8SaXQ4AoJWxcFwriwizauIF3SS5tmSrrmswuSIAABBI3L3oI09nf3QACAWE9DYw8vROykiI1J6yGr21bpfZ5QAAgADyfeOicT3TmY8OAKGAkN4GIsKsGnfuSZKk55f/qAYne+UCAIBjq6pt8Gy/9otUQjoAhAJCehsZdUamEqPD9fO+Sn2wabfZ5QAAgADwQ1G5DENKjolQ+1i72eUAANoAIb2NxNjDNDa7qyTpjTU7zC0GAAAEhG92l0qSuqewYBwAhApCehtyL/jy+Q97lV9SZXI1AADA363aVixJOqNrssmVAADaCiG9DXVuF63sk9rJaUgvrdhmdjkAAMDPfbu7TJJ0WmaiuYUAANoMIb2NufdJX7xhl+obnCZXAwAA/JVhGNq+r0KSlNUhxuRqAABthZDexs7r0UHtYiK0t7xW//thr9nlAAAAP7WnrEaVtQ2yWqTMpGizywEAtBFCehsLt1l12anpkqQPvmKVdwAA0LRte1296B2TohQRxq9sABAq+D++CYb1cYX0/3xTqDqGvAMAgCb83DjUvWs7hroDQCghpJvgzKxktYuJUGlVnVb+tM/scgAAgB/KK66UJHVpx1B3AAglhHQT2KwWXdw7TZL0waYCk6sBAAD+qNBRI0lKT4gyuRIAQFsipJvk0r6ukP6frwtY5R0AABymqMwV0jvE2U2uBADQlgjpJhl8UjslRodrX0Wt1mzfb3Y5AADAzxQ5qiVJqfGRJlcCAGhLhHSThNusOu8XHSRJn7MVGwAAOIS7Jz01np50AAglhHQTnd2tvSRpyeYCGYZhcjUAAMBf1NQ3qLiiVpKUEkdPOgCEEkK6iS7pk6aIMKu2FpV79kIFAADY09iLHm6zKCk63ORqAABtiZBuovjIcJ2WmShJ+vLnYnOLAQAAfsM91D0lLlIWi8XkagAAbYmQbrJBWcmSpFXbCOkAAMDFvWhcCvPRASDkENJNdmZjSF9NSAcAAI3ce6SnMh8dAEIOId1kp3dOks1q0c79VdpVUmV2OQAAwA8Ulbm3X6MnHQBCDSHdZDH2MPXpmCBJWvXTPpOrAQAA/mB3qXu4Oz3pABBqCOl+YLB7XvpPDHkHACDUGYahz3/YK0nqmRZncjUAgLZGSPcDg05yLx5HTzoAAKGupLLOMyf97G7tTa4GANDWTA3p06dP1xlnnKG4uDilpKRo+PDh2rJlyzGft2jRIvXs2VORkZHq27evPvjggzaotvUM6OIK6T/vq1RJZa3J1QAAADPtb/xdINYepshwm8nVAADamqkh/bPPPtOECRO0cuVKLV26VHV1dbr44otVUVFxxOd88cUXuu666zRu3DitX79ew4cP1/Dhw7V58+Y2rNy3EqLClZkcJUn6ZrfD5GoAAICZSqrqJEmJ0eEmVwIAMIOpIX3JkiW6/vrr1bt3b/Xr10/z589XXl6e1q5de8Tn/PWvf9Ull1yiO++8U7169dLDDz+s008/Xc8++2wbVu57vdLiJUnf7i4zuRIAAGCm0kpCOgCEMr+ak15aWipJSk5OPuI5ubm5ysnJ8To2dOhQ5ebmtmptra1Xujuk05MOAEAoK6lyDXdPjIowuRIAgBnCzC7Azel0atKkSTr77LPVp0+fI55XUFCg1NRUr2OpqakqKCho8vyamhrV1NR47jsc/hmCe6W7Vm8lpAMAENr2V7h60hPoSQeAkOQ3PekTJkzQ5s2btXDhQp++7vTp05WQkOC5ZWZm+vT1fcXdk761sFx1DU6TqwEAAGZxz0lPIqQDQEjyi5A+ceJEvffee/rkk0/UqVOno56blpamwsJCr2OFhYVKS0tr8vwpU6aotLTUc9uxY4fP6valzKRoxUTYVNvg1La9R144DwAABLfSSoa7A0AoMzWkG4ahiRMn6u2339bHH3+srKysYz4nOztby5Yt8zq2dOlSZWdnN3m+3W5XfHy8180fWa0W9WReOgAAIY/V3QEgtJka0idMmKB//vOfWrBggeLi4lRQUKCCggJVVVV5zhkzZoymTJniuX/77bdryZIlevLJJ/Xdd9/pwQcf1Jo1azRx4kQzPoJPueelsw0bAACha79ndXd60gEgFJka0mfPnq3S0lINGTJE6enpntvrr7/uOScvL0+7d+/23D/rrLO0YMECPf/88+rXr5/efPNNLV68+KiLzQWKAyu8sw0bAAChqqRxuHtCFD3pABCKTF3d3TCMY57z6aefHnbsmmuu0TXXXNMKFZmLbdgAAMDu0mpJUlp8pMmVAADM4BcLx8GlZ1qcLBZpT1mN9pbXHPsJAAAgqFTXNWhPmet3gE5JUSZXAwAwAyHdj0RHhKlruxhJ9KYDABCKdpW41uWJibCxcBwAhChCup9xLx5HSAcAIPQUV7jmo7ePs8tisZhcDQDADIR0P9MrjcXjAAAIVY7G7ddYNA4AQhch3c/0SHP1pG8tIqQDABBqHNWukB4fSUgHgFBFSPcz3VJiJUk/FlXI6Tz26vcAACB4OKrqJUnxUaZuwAMAMBEh3c90To5WhM2qqroGz+IxAAAgNDDcHQBASPczYTaruraPliT9sKfc5GoAAEBbYrg7AICQ7ocODHknpAMAEEqKKxp70tl+DQBCFiHdD3Vp3Ct9R3GlyZUAAIC2tKe8RpKUEhdpciUAALMQ0v1Qp6QoSdKO/cxJBwAglBQ5qiVJHeLsJlcCADALId0PZSa55qTv3E9POgAAoWSvpyedkA4AoYqQ7ocyk10hfUdxlQyDbdgAAAgF9Q1O7auolURPOgCEMkK6H8pIjJTFIlXVNai4sbEGAADBbV9FrQxDslktSo6OMLscAIBJCOl+yB5mU2rjgjHMSwcAIDTsKXMNdW8fGyGr1WJyNQAAsxDS/ZRn8ThWeAcAICQUlbkWjWNldwAIbYR0P+Wel76TnnQAAEKCe4/05BiGugNAKCOk+6kD27DRkw4AQChwVLlCenxUuMmVAADMREj3Uwe2YaMnHQCAUFBWXS9JiosMM7kSAICZCOl+qlOyqyd9J3PSAQBBpKGhQffff7+ysrIUFRWlk08+WQ8//LDXlqOGYWjq1KlKT09XVFSUcnJytHXrVhOrbhtl1a6edEI6AIQ2Qrqf8vSkl1TJ6WSvdABAcHjiiSc0e/ZsPfvss/r222/1xBNPaMaMGXrmmWc858yYMUOzZs3SnDlztGrVKsXExGjo0KGqrq42sfLW52gM6fGRDHcHgFDGV7V+Kj0hUjarRbX1Tu0pr1FqPCu9AgAC3xdffKGrrrpKl112mSSpa9eueu2117R69WpJrl70mTNn6r777tNVV10lSXrllVeUmpqqxYsXa9SoUabV3trcw93j6UkHgJBGT7qfCrNZldYYzNmGDQAQLM466ywtW7ZM33//vSRp48aNWrFihYYNGyZJ2rZtmwoKCpSTk+N5TkJCggYNGqTc3Nwjvm5NTY0cDofXLdA4PMPd6UkHgFDGV7V+LDM5SrtKqrRzf5UGdjW7GgAAWu5Pf/qTHA6HevbsKZvNpoaGBj366KMaPXq0JKmgoECSlJqa6vW81NRUz2NNmT59uh566KHWK7wNsHAcAECiJ92vpSe4Fo8rcAT3HDwAQOh444039Oqrr2rBggVat26dXn75Zf3lL3/Ryy+/3KLXnTJlikpLSz23HTt2+KjituMZ7s4WbAAQ0viq1o+lxNslSUWOGpMrAQDAN+6880796U9/8swt79u3r7Zv367p06dr7NixSktLkyQVFhYqPT3d87zCwkKddtppR3xdu90uu93eqrW3Nvc+6fSkA0Booyfdj6XEueakF5bRkw4ACA6VlZWyWr1//bDZbHI6nZKkrKwspaWladmyZZ7HHQ6HVq1apezs7Datta0dGO5OTzoAhDK+qvVjqY096XvoSQcABIkrrrhCjz76qDp37qzevXtr/fr1euqpp3TjjTdKkiwWiyZNmqRHHnlE3bt3V1ZWlu6//35lZGRo+PDh5hbfiqrrGlTb4Pqigp50AAhttAJ+jJ50AECweeaZZ3T//ffr97//vYqKipSRkaH/+7//09SpUz3n3HXXXaqoqND48eNVUlKic845R0uWLFFkZPBuR+pe2d1ikWIj+PUMAEIZrYAfSz1oTrphGLJYLCZXBABAy8TFxWnmzJmaOXPmEc+xWCyaNm2apk2b1naFmcw91D3WHiarlfYeAEIZc9L9mLsnvaquQWU19SZXAwAAWotnZXfmowNAyCOk+7GoCJtnXhorvAMAELxY2R0A4EZI93Mpce4h78xLBwAgWNGTDgBwI6T7udR4Fo8DACDYlVXTkw4AcCGk+7kDPekMdwcAIFg5COkAgEaEdD/n6UknpAMAELQ8w92jGO4OAKGOkO7nUhpDehHD3QEACFrukE5POgCAkO7nGO4OAEDwO7C6Oz3pABDqCOl+LpWedAAAgp6DnnQAQCNCup/z9KSX0ZMOAECwcq/uzhZsAABCup9r3xjSK2sbVFFTb3I1AACgNdCTDgBwI6T7uZgIm6LCbZKkveX0pgMAEIyYkw4AcCOk+zmLxaIOjb3pexjyDgBA0KmoqVd+aZUkKTM5yuRqAABmI6QHgPaxEZLoSQcAIBh9V+CQYUip8XalxEWaXQ4AwGSE9ABATzoAAMFrb3mtJKljIr3oAABCekDwhPTGRhwAAASPMs+iccxHBwAQ0gNC+1h60gEACFbljduvxbKyOwBAhPSAwHB3AACCV3njFqtxdkI6AICQHhDaxbgWjttfyXB3AACCTVljSI8lpAMAREgPCEnRjSG9gpAOAECwYU46AOBghPQAkNzYk15MTzoAAEHHHdKZkw4AkAjpAcEd0kur6lTf4DS5GgAA4EsFpVWSXPukAwBASA8ACVHhslgkw5BKqurMLgcAAPjQzv2ukN4pKdrkSgAA/oCQHgDCbFYlRLnmqTEvHQCA4FHX4FSBo1qS1DExyuRqAAD+gJAeIJIbF48rJqQDABA09lfUyjAkq+XAbi4AgNBGSA8QnsXjCOkAAAQN96KwidERslotJlcDAPAHhPQAkcQK7wAABJ39Fa61ZpKi2X4NAOBCSA8QyeyVDgBA0Nnf+OV7UjRD3QEALoT0AOHpSa9gdXcAAILF/oOGuwMAIBHSA0Y7T0ivMbkSAADgKyWVri/fk2MY7g4AcCGkB4gDc9LpSQcAIFi4F4RluDsAwI2QHiDc37AzJx0AgODBcHcAwKEI6QEiiX3SAQAIOgx3BwAcipAeINrF2CUR0gEACCbudp2edACAGyE9QCQ1fsNeVdegqtoGk6sBAAC+UNI43D05hpAOAHAxNaQvX75cV1xxhTIyMmSxWLR48eKjnv/pp5/KYrEcdisoKGibgk0Uaw9TuM0i6cD8NQAAENgOLBzHcHcAgIupIb2iokL9+vXTc88916znbdmyRbt37/bcUlJSWqlC/2GxWJiXDgBAEKlvcMpRXS+J4e4AgAPCzHzzYcOGadiwYc1+XkpKihITE31fkJ9Lio5QUVmNZ5EZAAAQuEqrDrTniVH0pAMAXAJyTvppp52m9PR0XXTRRfr888+Pem5NTY0cDofXLVAlNDbgJVX0pAMAEOjc09fiI8MUZgvIX8kAAK0goFqE9PR0zZkzR//617/0r3/9S5mZmRoyZIjWrVt3xOdMnz5dCQkJnltmZmYbVuxbCY3z1Q7+5h0AAASm/Y0j45JYNA4AcBBTh7s3V48ePdSjRw/P/bPOOks//vijnn76af3jH/9o8jlTpkzR5MmTPfcdDkfABnV3TzohHQCAwLffs2gcIR0AcEBAhfSmnHnmmVqxYsURH7fb7bLb7W1YUevxhHTmpAMAEPDcw91Z2R0AcLCAGu7elA0bNig9Pd3sMtpEIj3pAAAEDYa7AwCaYmpPenl5uX744QfP/W3btmnDhg1KTk5W586dNWXKFO3atUuvvPKKJGnmzJnKyspS7969VV1drRdffFEff/yx/vOf/5j1EdoUc9IBAAgeDHcHADTF1JC+Zs0anX/++Z777rnjY8eO1fz587V7927l5eV5Hq+trdUdd9yhXbt2KTo6Wqeeeqr++9//er1GMPOs7s5wdwAAAp67PWe4OwDgYKaG9CFDhsgwjCM+Pn/+fK/7d911l+66665Wrsp/sXAcAADBwz0nPZGedADAQQJ+TnooIaQDABA8Shrb80R60gEAByGkBxD3N+2EdAAAAp97t5bEKHrSAQAHENIDiLsnvbymXvUNTpOrAQAALVFS5R7uTk86AOAAQnoAiY88sISAo7rexEoAAEBLGIbh2YKNkA4AOBghPYCE2ayKs7uCeknjYjMAACDwVNc5VVvvGhXHwnEAgIMR0gNMPIvHAQAQ8NxD3cNtFsVE2EyuBgDgTwjpAYYV3gEACHz7K1zteEJUhCwWi8nVAAD8CSE9wLjnrRHSAQAIXCwaBwA4EkJ6gKEnHQCAwFfSuGhcEiEdAHAIQnqA8YT0SkI6AACBauf+SklSanykyZUAAPwNIT3AJDR+415CTzoAAAHrx6IKSdLJHWJNrgQA4G8I6QGG4e4AAAS+n/aWS5JO6hBjciUAAH9DSA8whHQAAALfj3voSQcANI2QHmASoyIkMScdAIBAVVJZq+IK1+ruWe3pSQcAeCOkBxh60gEACGx7y10BPS4yTDH2MJOrAQD4G0J6gCGkAwAQ2Mpr6iVJ8ZFsvwYAOBwhPcAkelZ3rzW5EgBAsOjataumTZumvLw8s0sJCeXVrpAeF0kvOgDgcIT0ABPf2JNeXedUTX2DydUAAILBpEmT9NZbb+mkk07SRRddpIULF6qmpsbssoJWeY1rNFwsQ90BAE0gpAeYOHuYLBbXzwx5BwD4wqRJk7RhwwatXr1avXr10q233qr09HRNnDhR69atM7u8oONo7EmPpScdANAEQnqAsVotnjlsrPAOAPCl008/XbNmzVJ+fr4eeOABvfjiizrjjDN02mmn6aWXXpJhGGaXGBTcw93pSQcANIWQHoDci8e5v4kHAMAX6urq9MYbb+jKK6/UHXfcoYEDB+rFF1/UyJEjdc8992j06NE+eZ9du3bpN7/5jdq1a6eoqCj17dtXa9as8TxuGIamTp2q9PR0RUVFKScnR1u3bvXJe/uDMuakAwCOgtYhAMVHuf61OarpSQcAtNy6des0b948vfbaa7JarRozZoyefvpp9ezZ03POiBEjdMYZZ7T4vfbv36+zzz5b559/vj788EN16NBBW7duVVJSkuecGTNmaNasWXr55ZeVlZWl+++/X0OHDtU333yjyMjIFtdgtn0Vrvn+yTERJlcCAPBHhPQA5B7u7mBOOgDAB8444wxddNFFmj17toYPH67w8MO3BsvKytKoUaNa/F5PPPGEMjMzNW/ePK/XdjMMQzNnztR9992nq666SpL0yiuvKDU1VYsXL/ZJDWYrdFRLklLjA/8LBwCA7zHcPQC5h8cx3B0A4As//fSTlixZomuuuabJgC5JMTExXsH6RL3zzjsaOHCgrrnmGqWkpKh///564YUXPI9v27ZNBQUFysnJ8RxLSEjQoEGDlJube8TXrampkcPh8Lr5q6IyV096ShwhHQBwOEJ6AKInHQDgS0VFRVq1atVhx1etWuU1V9wXfvrpJ82ePVvdu3fXRx99pFtuuUW33XabXn75ZUlSQUGBJCk1NdXreampqZ7HmjJ9+nQlJCR4bpmZmT6t21ecTkM7iislSanxdpOrAQD4I0J6AIr3LBxHSAcAtNyECRO0Y8eOw47v2rVLEyZM8Ol7OZ1OnX766XrsscfUv39/jR8/XjfddJPmzJnTotedMmWKSktLPbemPo8/2Jxfqr3ltYq1h6lXerzZ5QAA/BAhPQC5e9LLGO4OAPCBb775Rqeffvphx/v3769vvvnGp++Vnp6uU045xetYr169lJeXJ0lKS0uTJBUWFnqdU1hY6HmsKXa7XfHx8V43f5Rf4pqP/ovUWEWG20yuBgDgjwjpAcizujvD3QEAPmC32w8LxZK0e/duhYX5do3Zs88+W1u2bPE69v3336tLly6SXIvIpaWladmyZZ7HHQ6HVq1apezsbJ/WYoayxlFwcZFNz/0HAICQHoA8c9LpSQcA+MDFF1/sGS7uVlJSonvuuUcXXXSRT9/rD3/4g1auXKnHHntMP/zwgxYsWKDnn3/eM6zeYrFo0qRJeuSRR/TOO+9o06ZNGjNmjDIyMjR8+HCf1mIG9kgHABwLLUQA8sxJpycdAOADf/nLX/TLX/5SXbp0Uf/+/SVJGzZsUGpqqv7xj3/49L3OOOMMvf3225oyZYqmTZumrKwszZw5U6NHj/acc9ddd6miokLjx49XSUmJzjnnHC1ZsiQo9kh3h3R3Ww4AwKEI6QEo3rMFGyEdANByHTt21FdffaVXX31VGzduVFRUlG644QZdd911R9ySrSUuv/xyXX755Ud83GKxaNq0aZo2bZrP39tsB4a78ysYAKBptBAB6EBPOsPdAQC+ERMTo/Hjx5tdRtBzf8Eez5x0AMARENIDEFuwAQBawzfffKO8vDzV1tZ6Hb/yyitNqij47K9sDOkMdwcAHAEhPQC5h7vX1jtVXdfAFi4AgBb56aefNGLECG3atEkWi0WGYUhyDTuXpIaGBjPLCyr5JVWSpI6JgT+/HgDQOk5odfcdO3Zo586dnvurV6/WpEmT9Pzzz/usMBxZTESYrK7fm+hNBwC02O23366srCwVFRUpOjpaX3/9tZYvX66BAwfq008/Nbu8oLKrMaRnJEaZXAkAwF+dUEj/f//v/+mTTz6RJBUUFOiiiy7S6tWrde+99wblIi/+xmq1KNbu3iudeekAgJbJzc3VtGnT1L59e1mtVlmtVp1zzjmaPn26brvtNrPLCxoVNfUqaRzu3pGQDgA4ghMK6Zs3b9aZZ54pSXrjjTfUp08fffHFF3r11Vc1f/58X9aHI2BeOgDAVxoaGhQXFydJat++vfLz8yVJXbp00ZYtW8wsLai4e9HjI8MUx8JxAIAjOKE56XV1dbLb7ZKk//73v54FZXr27Kndu3f7rjockWtV2Cr2SgcAtFifPn20ceNGZWVladCgQZoxY4YiIiL0/PPP66STTjK7vKDBUHcAwPE4oZ703r17a86cOfrf//6npUuX6pJLLpEk5efnq127dj4tEE2Lj3J9v1JWzXB3AEDL3HfffXI6nZKkadOmadu2bTr33HP1wQcfaNasWSZXFzx27XeF9E5JhHQAwJGdUE/6E088oREjRujPf/6zxo4dq379+kmS3nnnHc8weLQu9/6qDHcHALTU0KFDPT9369ZN3333nYqLi5WUlORZ4R0tt8uzsjshHQBwZCcU0ocMGaK9e/fK4XAoKSnJc3z8+PGKjo72WXE4Ms+cdBaOAwC0QF1dnaKiorRhwwb16dPHczw5OdnEqoKTuye9Iz3pAICjOKHh7lVVVaqpqfEE9O3bt2vmzJnasmWLUlJSfFogmkZPOgDAF8LDw9W5c2f2Qm8D+cxJBwAchxMK6VdddZVeeeUVSVJJSYkGDRqkJ598UsOHD9fs2bN9WiCa5p6TzsJxAICWuvfee3XPPfeouLjY7FKC2r6KWklSh1i7yZUAAPzZCYX0devW6dxzz5Ukvfnmm0pNTdX27dv1yiuvsMBMGznQk85wdwBAyzz77LNavny5MjIy1KNHD51++uleN/iG+4v1hGi2XwMAHNkJzUmvrKz07Kf6n//8R1dffbWsVqsGDx6s7du3+7RANO3AnHR60gEALTN8+HCzSwh6hmF4pqjFs0c6AOAoTiikd+vWTYsXL9aIESP00Ucf6Q9/+IMkqaioSPHx8T4tEE2Lj2wc7s6cdABACz3wwANmlxD0quucqmswJB34oh0AgKac0HD3qVOn6o9//KO6du2qM888U9nZ2ZJcver9+/f3aYFoGj3pAAAEDveX6laLFBNhM7kaAIA/O6Ge9F/96lc655xztHv3bs8e6ZJ04YUXasSIET4rDkcW5+lJZ046AKBlrFbrUfdDZ+X3lnN/qR4fFc7e8wCAozqhkC5JaWlpSktL086dOyVJnTp10plnnumzwnB0noXj6EkHALTQ22+/7XW/rq5O69ev18svv6yHHnrIpKqCy67G7ddY2R0AcCwnFNKdTqceeeQRPfnkkyovL5ckxcXF6Y477tC9994rq/WERtGjGdzD3Wvqnaqpb5A9jKFzAIATc9VVVx127Fe/+pV69+6t119/XePGjTOhquCytdD1+1L31FiTKwEA+LsTCun33nuv5s6dq8cff1xnn322JGnFihV68MEHVV1drUcffdSnReJwcfYwWSySYUhl1fWyxxLSAQC+NXjwYI0fP97sMoLCT3srJEndOhDSAQBHd0Ih/eWXX9aLL76oK6+80nPs1FNPVceOHfX73/+ekN4GrFaLYu1hKquul6OqTu0ZPgcA8KGqqirNmjVLHTt2NLuUoFDkqJYkpSdGmVwJAMDfnVBILy4uVs+ePQ873rNnTxUXF7e4KByf+MhwV0hn8TgAQAskJSV5LWZmGIbKysoUHR2tf/7znyZWFjyKymokSSlxfKkOADi6Ewrp/fr107PPPqtZs2Z5HX/22Wd16qmn+qQwHFt8VLh2lVSxeBwAoEWefvppr5ButVrVoUMHDRo0SElJSSZWFjwKG3vSU+MjTa4EAODvTiikz5gxQ5dddpn++9//evZIz83N1Y4dO/TBBx/4tEAcWbxnGzZCOgDgxF1//fVmlxDUDMPQvopaSWJ6GgDgmE5oGfbzzjtP33//vUaMGKGSkhKVlJTo6quv1tdff61//OMfvq4RR+Be4d1RxXB3AMCJmzdvnhYtWnTY8UWLFunll182oaLgUl5TrwanIUlKjA43uRoAgL874b3SMjIy9Oijj+pf//qX/vWvf+mRRx7R/v37NXfuXF/Wh6Pw7JVOTzoAoAWmT5+u9u3bH3Y8JSVFjz32mAkVBZfSxmlpEWFWRYazGwsA4OjY0DyAxUc1DndnTjoAoAXy8vKUlZV12PEuXbooLy/PhIqCizukJ0TRiw4AODZCegCjJx0A4AspKSn66quvDju+ceNGtWvXzoSKggshHQDQHIT0ABbnXjiOOekAgBa47rrrdNttt+mTTz5RQ0ODGhoa9PHHH+v222/XqFGjzC4v4JVWEtIBAMevWau7X3311Ud9vKSkpCW1oJk8C8fRkw4AaIGHH35YP//8sy688EKFhbl+NXA6nRozZgxz0n1gT7lrj/TkmAiTKwEABIJmhfSEhIRjPj5mzJgWFYTj5xnuzpx0AEALRERE6PXXX9cjjzyiDRs2KCoqSn379lWXLl3MLi0o5Je49kjvmBhlciUAgEDQrJA+b9681qoDJ8CzcFw1w90BAC3XvXt3de/e3ewygk5+SZUkKSMx0uRKAACBgDnpAczdk17GcHcAQAuMHDlSTzzxxGHHZ8yYoWuuucaEioLLgZBOTzoA4NgI6QHMvQANC8cBAFpi+fLluvTSSw87PmzYMC1fvtyEioLL7lLXcPf0BEI6AODYTA3py5cv1xVXXKGMjAxZLBYtXrz4mM/59NNPdfrpp8tut6tbt26aP39+q9fpr9w96VV1Daqtd5pcDQAgUJWXlysi4vBFzcLDw+VwOEyoKHg0OA0VOJiTDgA4fqaG9IqKCvXr10/PPffccZ2/bds2XXbZZTr//PO1YcMGTZo0Sb/73e/00UcftXKl/ik28sCSAgx5BwCcqL59++r1118/7PjChQt1yimnmFBR8Cgqq1aD01CY1aIOcXazywEABIBmLRzna8OGDdOwYcOO+/w5c+YoKytLTz75pCSpV69eWrFihZ5++mkNHTq0tcr0WzarRXH2MJXV1MtRXa92sTT+AIDmu//++3X11Vfrxx9/1AUXXCBJWrZsmRYsWKA333zT5OoCm3s+emp8pGxWi8nVAAACgakhvblyc3OVk5PjdWzo0KGaNGnSEZ9TU1Ojmpoaz/1gG7YXHxXuCulswwYAOEFXXHGFFi9erMcee0xvvvmmoqKi1K9fP3388cdKTk42u7yAxvZrAIDmCqiF4woKCpSamup1LDU1VQ6HQ1VVVU0+Z/r06UpISPDcMjMz26LUNhMX6d6GjZAOADhxl112mT7//HNVVFTop59+0rXXXqs//vGP6tevn9mlBTR3T3o6268BAI5TQIX0EzFlyhSVlpZ6bjt27DC7JJ+KZ4V3AICPLF++XGPHjlVGRoaefPJJXXDBBVq5cqXZZQW0ojLXaL7UeEI6AOD4BNRw97S0NBUWFnodKywsVHx8vKKimh5GZrfbZbcH71zteHrSAQAtUFBQoPnz52vu3LlyOBy69tprVVNTo8WLF7NonA/sK3eF9Paxh6+eDwBAUwKqJz07O1vLli3zOrZ06VJlZ2ebVJH53NuwMScdANBcV1xxhXr06KGvvvpKM2fOVH5+vp555hmzywoqe8trJUntWdwVAHCcTO1JLy8v1w8//OC5v23bNm3YsEHJycnq3LmzpkyZol27dumVV16RJN1888169tlnddddd+nGG2/Uxx9/rDfeeEPvv/++WR/BdJ7h7vSkAwCa6cMPP9Rtt92mW265Rd27dze7nKC0t7EnnR1YAADHy9Se9DVr1qh///7q37+/JGny5Mnq37+/pk6dKknavXu38vLyPOdnZWXp/fff19KlS9WvXz89+eSTevHFF0Ny+zU3z3B35qQDAJppxYoVKisr04ABAzRo0CA9++yz2rt3r9llBZV9Fa6e9HYxDHcHABwfU3vShwwZIsMwjvj4/Pnzm3zO+vXrW7GqwOLuSS+jJx0A0EyDBw/W4MGDNXPmTL3++ut66aWXNHnyZDmdTi1dulSZmZmKi4szu8yA5XQaKq5guDsAoHkCak46DueZk15NTzoA4MTExMToxhtv1IoVK7Rp0ybdcccdevzxx5WSkqIrr7zS7PICVklVnRqcrs6IZHrSAQDHiZAe4OKj3MPd6UkHALRcjx49NGPGDO3cuVOvvfaa2eUENPfK7glR4YoI41cuAMDxocUIcAd60gnpAADfsdlsGj58uN555x2zSwlYezyLxtGLDgA4foT0AOdZ3Z2F4wAA8Cu7S6olSekJkSZXAgAIJIT0AEdPOgAA/mnn/ipJUqfEaJMrAQAEEkJ6gHPPSa+sbVBdg9PkagAAgNvO/ZWSpE5JUSZXAgAIJIT0ABdrP7CLXhkrvAMA4Dfce6SnxLP9GgDg+BHSA1yYzaqYCJskVngHAMCf7K90hfTEaBaOAwAcP0J6EPAsHse8dAAA/EZppatdTmxspwEAOB6E9CDgWTyOFd4BAPAb7p70pBh60gEAx4+QHgTci8fRkw4AgH9wOg2VVtGTDgBoPkJ6EHD3pJcR0gEA8AulVXVyGq6fE6IJ6QCA40dIDwKeOekMdwcAwC/kl7r2SG8fGyF7mM3kagAAgYSQHgTiIxnuDgCAPykorZYkpSVEmlwJACDQENKDwIGedEI6AAD+IL8xpKcnRJlcCQAg0BDSg4BndfdqhrsDAOAPChqHu6fTkw4AaCZCehDwrO5OTzoAAH5hdwk96QCAE0NIDwIHetIJ6QAA+IPdnuHu9KQDAJqHkB4EWN0dAAD/UuAgpAMATgwhPQjEsbo7AAB+wzAM5Ze456Qz3B0A0DyE9CDgGe7OnHQAAExXUlmnmnqnJCk1wW5yNQCAQENIDwLu4e4VtQ2qb3CaXA0AAMfv8ccfl8Vi0aRJkzzHqqurNWHCBLVr106xsbEaOXKkCgsLzSuymdzz0dvFRMgeZjO5GgBAoCGkBwH3cHdJKmMbNgBAgPjyyy/197//XaeeeqrX8T/84Q969913tWjRIn322WfKz8/X1VdfbVKVzbe/slaS1C42wuRKAACBiJAeBMJtVkVHuL6pJ6QDAAJBeXm5Ro8erRdeeEFJSUme46WlpZo7d66eeuopXXDBBRowYIDmzZunL774QitXrjSx4uNXUumafpYYRUgHADQfIT1IsA0bACCQTJgwQZdddplycnK8jq9du1Z1dXVex3v27KnOnTsrNzf3iK9XU1Mjh8PhdTNLSZWrJz0hOty0GgAAgSvs2KcgEMRHhanAweJxAAD/t3DhQq1bt05ffvnlYY8VFBQoIiJCiYmJXsdTU1NVUFBwxNecPn26HnroIV+XekLcPelJhHQAwAmgJz1I0JMOAAgEO3bs0O23365XX31VkZG+20N8ypQpKi0t9dx27Njhs9duru8KyiRJidEMdwcANB8hPUi4V3h3VDEnHQDgv9auXauioiKdfvrpCgsLU1hYmD777DPNmjVLYWFhSk1NVW1trUpKSryeV1hYqLS0tCO+rt1uV3x8vNfNLJ98VyRJ6p1hXg0AgMDFcPcgEd+4wjs96QAAf3bhhRdq06ZNXsduuOEG9ezZU3fffbcyMzMVHh6uZcuWaeTIkZKkLVu2KC8vT9nZ2WaU3Cy19U6V17i+MB/yixSTqwEABCJCepA40JNOSAcA+K+4uDj16dPH61hMTIzatWvnOT5u3DhNnjxZycnJio+P16233qrs7GwNHjzYjJKbpaLmwIi2GDt7pAMAmo+QHiTiPD3pDHcHAAS2p59+WlarVSNHjlRNTY2GDh2qv/3tb2aXdVzcvej2MKvCbMwqBAA0HyE9SHgWjqMnHQAQYD799FOv+5GRkXruuef03HPPmVNQC1TUukJ6rJ1fsQAAJ4aveIOEZ7g7c9IBADBNRU2DJCmGkA4AOEGE9CBxoCed4e4AAJjFPSedkA4AOFGE9CARH8Xq7gAAmM0d0mNZNA4AcIII6UHC3ZNexsJxAACY5sc95ZKklPhIkysBAAQqQnqQYAs2AADM9+XP+yVJg7KSTa4EABCoCOlBIr5xC7aymno1OA2TqwEAIDTlFVdKkn6RGmdyJQCAQEVIDxJxjcPdJamcIe8AALS5HcWV2ra3QpLUKSnK5GoAAIGKkB4kIsKsigp3LVLD4nEAALS9l7/42fNzGnPSAQAniJAeRNwrvJcyLx0AgDbnbn/j7GEKs/ErFgDgxNCCBBHPXun0pAMA0Oaq6hokSZMu+oXJlQAAAhkhPYjENS4e56hiTjoAAG3N3ZOeEBV+jDMBADgyQnoQ8WzDRk86AABtzkFIBwD4ACE9iHiGuzMnHQCANkdPOgDAFwjpQcS9cJyDLdgAAGhThmFob3mtJCk5JsLkagAAgYyQHkToSQcAwByO6nqV17i+JM9IZPs1AMCJI6QHEfec9DJ60gEAaFO7S6skSYnR4YqOCDO5GgBAICOkBxF3T3ppVa3JlQAAEFp2l1RLktITokyuBAAQ6AjpQSQ5xhXS91cy3B0AgLaU39iTnpHAUHcAQMsQ0oNIUrRroZriCnrSAQBoS/kljSE9kZ50AEDLENKDSLtYV0jfV15jciUAAIQWz3B3Fo0DALQQIT2IJMfYJblWmK1rcJpcDQAAoePAcHd60gEALUNIDyIJUeGyWFw/769kyDsAAG0lv7EnneHuAICWIqQHEZvVwrx0AADamNNpqKDUvbo7w90BAC1DSA8ySdGuFd4J6QAAtI19FbWqbXDKYpHSCOkAgBYipAeZdo3z0gnpAAC0DffK7ilxdoXb+NUKANAytCRBJjmG4e4AALSl3Y2LxqWzaBwAwAcI6UEmKca9DRshHQCAtsB8dACALxHSg0y7xpDO6u4AALSNsup6SVJi47owAAC0BCE9yLiHu+9juDsAAG2irMYV0mPtYSZXAgAIBoT0IOOZk85wdwAA2oS7Jz3WTk86AKDlCOlBJpnh7gAAtKnyxp70uEh60gEALUdIDzIMdwcAoG2VV9dJkmIJ6QAAHyCkBxlPT3pFrQzDMLkaAACCn6cnnTnpAAAf8IuQ/txzz6lr166KjIzUoEGDtHr16iOeO3/+fFksFq9bZCRbnri5Q3q905CjcY4cAABoPSWVrp70+CjmpAMAWs70kP76669r8uTJeuCBB7Ru3Tr169dPQ4cOVVFR0RGfEx8fr927d3tu27dvb8OK/VtkuE0xETZJUjFD3gEAaFWGYWhXSZUkKSMxyuRqAADBwPSQ/tRTT+mmm27SDTfcoFNOOUVz5sxRdHS0XnrppSM+x2KxKC0tzXNLTU1tw4r9X5J7Xnp5jcmVAAAQ3PZX1qmytkGSlJHIyD4AQMuZGtJra2u1du1a5eTkeI5ZrVbl5OQoNzf3iM8rLy9Xly5dlJmZqauuukpff/31Ec+tqamRw+HwugW7DnF2SdJeQjoAAK0qv7EXvUOcXfYwm8nVAACCgakhfe/evWpoaDisJzw1NVUFBQVNPqdHjx566aWX9O9//1v//Oc/5XQ6ddZZZ2nnzp1Nnj99+nQlJCR4bpmZmT7/HP4mpTGkF5UR0gEAaE3uLU/bNY5iAwCgpUwf7t5c2dnZGjNmjE477TSdd955euutt9ShQwf9/e9/b/L8KVOmqLS01HPbsWNHG1fc9lLiXMPtihyEdAAAWpN70bjEaBaNAwD4hql7hbRv3142m02FhYVexwsLC5WWlnZcrxEeHq7+/fvrhx9+aPJxu90uu93e4loDyYGe9GqTKwEAILiVNPakJ0XTkw4A8A1Te9IjIiI0YMAALVu2zHPM6XRq2bJlys7OPq7XaGho0KZNm5Sent5aZQacDgx3BwCgTewtd4V0etIBAL5iak+6JE2ePFljx47VwIEDdeaZZ2rmzJmqqKjQDTfcIEkaM2aMOnbsqOnTp0uSpk2bpsGDB6tbt24qKSnRn//8Z23fvl2/+93vzPwYfiUlvjGkM9wdAIBW9ddlWyVJCVH0pAMAfMP0kP7rX/9ae/bs0dSpU1VQUKDTTjtNS5Ys8Swml5eXJ6v1QIf//v37ddNNN6mgoEBJSUkaMGCAvvjiC51yyilmfQS/456TvofV3QEAaDV1DU7PzwO7JJlYCQAgmFgMwzDMLqItORwOJSQkqLS0VPHx8WaX0yqKHNU687FlslqkrY9eKpvVYnZJAICjCIW2qa21xTXdW16jgY/8V5L042O0twCAI2tOuxRwq7vj2NrF2mW1SE5D2kdvOgAArcK9aFx8ZBgBHQDgM4T0IGSzWtQulsXjAABoTQe2X2M+OgDAdwjpQYpt2AAAaF3ukJ7Eyu4AAB8ipAcpT0hnhXcAAFpFYeMX4ckx9KQDAHyHkB6kPCu8M9wdAIBW8WNRhSTppA6xJlcCAAgmhPQg1SGOOekAALSmH/aUS5K6pRDSAQC+Q0gPUinxzEkHAKA1/VjkCukn05MOAPAhQnqQcg93L2ROOgAAPldV26BdJVWS6EkHAPgWIT1IdUyMkiTPLxAAAMB3fmwc6p4UHc7CcQAAnyKkB6lOSa6QvqesRtV1DSZXAwBAcPmR+egAgFZCSA9SidHhio6wSZLy6U0HAMCnft5bKUnKah9jciUAgGBDSA9SFovF05u+cz8hHQAAXyqtqpMktYu1m1wJACDYENKDWKekaEnMSwcAwNfKa1whPdYeZnIlAIBgQ0gPYu7F43burzS5EgAAgktZdb0kKS6SkA4A8C1CehBzD3ffxXB3AAB8qrzGFdLpSQcA+BohPYh1ZE46AACt4kBPerjJlQAAgg0hPYi556QT0gEA8C160gEArYWQHsTcw90Ly6pVW+80uRoAAIKHo3F1d+akAwB8jZAexNrFRCgy3CrDkHaX0psOAIAvGIahkkpXSE+KiTC5GgBAsCGkBzGLxXLQCu+EdAAAfKGytkG1Da4RaknRzEkHAPgWIT3IdWkXI0natrfC5EoAAAgOxRW1kiR7mFVR4TaTqwEABBtCepA7uYMrpP+4p9zkSgAAkKZPn64zzjhDcXFxSklJ0fDhw7Vlyxavc6qrqzVhwgS1a9dOsbGxGjlypAoLC02q+HB7ymskSckxEbJYLCZXAwAINoT0IHdyh1hJ0o976EkHAJjvs88+04QJE7Ry5UotXbpUdXV1uvjii1VRcaCd+sMf/qB3331XixYt0meffab8/HxdffXVJlbt7aOvCyRJqfGRJlcCAAhGLEka5E5OaQzpRfSkAwDMt2TJEq/78+fPV0pKitauXatf/vKXKi0t1dy5c7VgwQJdcMEFkqR58+apV69eWrlypQYPHmxG2V7cber5PVJMrgQAEIzoSQ9y3Rp70neVVKmqtsHkagAA8FZaWipJSk5OliStXbtWdXV1ysnJ8ZzTs2dPde7cWbm5uabUeKiKGld72rV9tMmVAACCESE9yCXFRCi5cXsY5qUDAPyJ0+nUpEmTdPbZZ6tPnz6SpIKCAkVERCgxMdHr3NTUVBUUFBzxtWpqauRwOLxuraWytl6SFGtnQCIAwPcI6SGAxeMAAP5owoQJ2rx5sxYuXNji15o+fboSEhI8t8zMTB9U2LSKxpFp0RGEdACA7xHSQ0C3xnnp3xeWmVwJAAAuEydO1HvvvadPPvlEnTp18hxPS0tTbW2tSkpKvM4vLCxUWlraEV9vypQpKi0t9dx27NjRWqWrssbVkx5jZ/s1AIDvEdJDwCnp8ZKkr/Nbb+gfAADHwzAMTZw4UW+//bY+/vhjZWVleT0+YMAAhYeHa9myZZ5jW7ZsUV5enrKzs4/4una7XfHx8V631lLeGNLpSQcAtAZalxBwSkaCJEI6AMB8EyZM0IIFC/Tvf/9bcXFxnnnmCQkJioqKUkJCgsaNG6fJkycrOTlZ8fHxuvXWW5Wdne0XK7sbhqHKxuHuzEkHALQGWpcQ0Cs9ThaLtKesRkVl1UqJY19XAIA5Zs+eLUkaMmSI1/F58+bp+uuvlyQ9/fTTslqtGjlypGpqajR06FD97W9/a+NKm1bb4FS905AkRTPcHQDQCgjpISA6IkwntY/Rj3sq9HW+Qyk9COkAAHMYhnHMcyIjI/Xcc8/pueeea4OKmmdPWY0kKdxmUSzD3QEArYA56SGid+OQ928Y8g4AwAnLL6mWJKUnRMlqtZhcDQAgGBHSQ0Tfjq6QvnFHibmFAAAQwPJLqiRJHROjTK4EABCsCOkh4vQuiZKkdXn7j2uoIQAAONyuxpCeQUgHALQSQnqI6J2RoAibVXvLa5VXXGl2OQAABKQDPems7wIAaB2E9BARGW5T306uIe9rft5vcjUAAASmfHrSAQCtjJAeQgZ0SZIkrc0jpAMAcCJ27CekAwBaFyE9hAxsDOkrf9xnciUAAASe2nqnft5bIUnqlhJrcjUAgGBFSA8h2Se3U5jVop/2Vmj7vgqzywEAIKDkFVeo3mko1h6m9ATmpAMAWgchPYTERYbrjK7JkqRPvisyuRoAAAJLkaNGkpSeECmLhT3SAQCtg5AeYs7v2UGS9MmWPSZXAgBAYCmurJUkJcVEmFwJACCYEdJDzAU9UyRJX/y4V/srak2uBgCAwOFuN5Oiw02uBAAQzAjpIaZbSpxOSY9XXYOhd7/KN7scAAACxv7KOklSMj3pAIBWREgPQSMHdJIk/WvtTpMrAQAgcBR7etIJ6QCA1kNID0FXnZahMKtFG3eWamthmdnlAAAQEPZXEtIBAK2PkB6C2sfadX7j3PQ319GbDgDA8fD0pDPcHQDQigjpIWrk6a4h72+v26X6BqfJ1QAA4P9KPHPSWTgOANB6COkh6oKeKUqKDldRWY0+3FxgdjkAAPg95qQDANoCIT1ERYRZdf1ZWZKkp5d+T286AABHYRgGIR0A0CYI6SHsxnO6Kik6XD/trdBb63eZXQ4AAH5rV0mVquoaFGa1KCMxyuxyAABBjJAewuIiw3XLkJMlSU98+J2KHNUmVwQAgH/6brdrN5RuKbGKCOPXJwBA66GVCXFjsruqZ1qc9lXU6o5FG+V0GmaXBACA3/l2t0OS1Cs93uRKAADBLszsAmCuyHCbnv1//XX5Myv0v6179eKKnzT+lyebXRYAAH7luwJXT3qv9DiTKwEQ7AzDUH19vRoaGswuBc0UHh4um83W4tchpEPdUuI09fLeuuftTfrzR1s0oEuyBnRJMrssAAD8Rn5plSSpc3KMyZUACGa1tbXavXu3KisrzS4FJ8BisahTp06KjY1t0esQ0iFJuu7MTP1v6x59uLlAN87/UgvHD2ZIHwAAjUqrXHukJ0azRzqA1uF0OrVt2zbZbDZlZGQoIiJCFovF7LJwnAzD0J49e7Rz50517969RT3qhHRIcn3r85dr+qmorEZrt+/X6BdX6cWxA3V6Z3rUAQBwNIb0hChCOoDWUVtbK6fTqczMTEVHR5tdDk5Ahw4d9PPPP6uurq5FIZ2F4+ARYw/TS9efoVM7Jai4olaj/r5S/1i5XYbBYnIAgNBlGIanJ52QDqC1Wa1EtEDlq5EP/BcALwlR4XrtpsEa2jtVtQ1O3b94s66f96W276swuzQAAExRVdegugbXF9aEdABAayOk4zAx9jDN+c0A3XdZL0WEWfXZ93uU89Rn+uOijfq+sMzs8gAAaFP7K1296GFWi6IjWr5qLwAAR0NIR5MsFot+d+5J+uC2c3Vu9/aqazD05tqduvjp5bph3mrl/riPYfAAgJDw817XaLLM5GgWcQKAI8jNzZXNZtNll11mdikBj4XjcFTdUmL1j3GDtD5vv55f/pOWfF2gT7bs0Sdb9qhjYpSG9UnTBT1TdHqXJEWG07sAAAg+7lFk3VJatqUOAASzuXPn6tZbb9XcuXOVn5+vjIwMU+qora1VRESEKe/tK/Sk47j075yk2b8ZoE/uGKLfDO6s6AibdpVU6cUV2/T/XlylUx/8j679e66e+s8WffR1gXYUV9LTDgAICrv2u/ZI79qO1ZYBoCnl5eV6/fXXdcstt+iyyy7T/PnzvR5/9913dcYZZygyMlLt27fXiBEjPI/V1NTo7rvvVmZmpux2u7p166a5c+dKkubPn6/ExESv11q8eLHXqKYHH3xQp512ml588UVlZWUpMjJSkrRkyRKdc845SkxMVLt27XT55Zfrxx9/9HqtnTt36rrrrlNycrJiYmI0cOBArVq1Sj///LOsVqvWrFnjdf7MmTPVpUsXOZ3Oll6yo6InHc3StX2MHhneV/dddoo+3bJHH31doM9/2Kuishqt3las1duKPefGR4bplIx4dU+JU1b7GGW1j1HHpCh1iLUrMTqcIYMAgIBQXFErSWoXaze5EgChxDAMVdU1mPLeUeG2Zv2u/sYbb6hnz57q0aOHfvOb32jSpEmaMmWKLBaL3n//fY0YMUL33nuvXnnlFdXW1uqDDz7wPHfMmDHKzc3VrFmz1K9fP23btk179+5tVr0//PCD/vWvf+mtt97ybH1WUVGhyZMn69RTT1V5ebmmTp2qESNGaMOGDbJarSovL9d5552njh076p133lFaWprWrVsnp9Oprl27KicnR/PmzdPAgQM97zNv3jxdf/31rb4Cv1+E9Oeee05//vOfVVBQoH79+umZZ57RmWeeecTzFy1apPvvv18///yzunfvrieeeEKXXnppG1aMyHCbLumTpkv6pMkwDG3bW6Hcn/ZpfV6Jvsl3aGtRmRzV9Vr5U7FW/lR82PPDbRZ1iLWrQ5xdHeIiG/903VIa/2wfY1eM3aYYe5jsYVZCPQDAFHsbQ3pyTGAPnwQQWKrqGnTK1I9Mee9vpg1VdMTxR8W5c+fqN7/5jSTpkksuUWlpqT777DMNGTJEjz76qEaNGqWHHnrIc36/fv0kSd9//73eeOMNLV26VDk5OZKkk046qdn11tbW6pVXXlGHDh08x0aOHOl1zksvvaQOHTrom2++UZ8+fbRgwQLt2bNHX375pZKTkyVJ3bp185z/u9/9TjfffLOeeuop2e12rVu3Tps2bdK///3vZtfXXKaH9Ndff12TJ0/WnDlzNGjQIM2cOVNDhw7Vli1blJKSctj5X3zxha677jpNnz5dl19+uRYsWKDhw4dr3bp16tOnjwmfABaLRSd1iNVJHWI1elAXSVJtvVNbi8r0Tb5DP+2t0M97K7Rtb4UKHNUqqaxTXYOh/NJq5ZdWSyo95ntYLa5v9KLtYYqOsLl+jnAFePfP0fYwRR/8s+e8MEXbbY2PuX6ODLcpwmZVhM2q8DCLwm1WhVktfBEAADhMcUWNJKl9LCEdAA61ZcsWrV69Wm+//bYkKSwsTL/+9a81d+5cDRkyRBs2bNBNN93U5HM3bNggm82m8847r0U1dOnSxSugS9LWrVs1depUrVq1Snv37vUMUc/Ly1OfPn20YcMG9e/f3xPQDzV8+HBNmDBBb7/9tkaNGqX58+fr/PPPV9euXVtU6/EwPaQ/9dRTuummm3TDDTdIkubMmaP3339fL730kv70pz8ddv5f//pXXXLJJbrzzjslSQ8//LCWLl2qZ599VnPmzGnT2nFkEWFW9c5IUO+MhMMeq6lv0N7yWu0pq9GeshoVlVV7fnbdd/25r6JG1XWuv0xOQ6qobVBFbesO+YmwWRVusyg8zKowqyu4h9kOhPiwxsfdP9ssFtmsB25Wi+sxm9Uiq9X1s9Vikc0q2axW158Wi+dn9zk2y4HzLRb3a0lWS+N9i+tci+XAcZvFIkvjz1Zr458HHbPI9QXKwfetVskii3TwscZzLBbXY1aLPO/j+s7ikGM6cP7h7+c63/Vaje+vA68tzzmNxw553PNY4z8sx3gteeo++nu5NVmH5zGL5/6BGvnSBgh1hmGo0OEK6ckxDHcH0Haiwm36ZtpQ0977eM2dO1f19fVeC8UZhiG73a5nn31WUVFRR36fozwmSVar9bB1rurq6g47LyYm5rBjV1xxhbp06aIXXnhBGRkZcjqd6tOnj2pra4/rvSMiIjRmzBjNmzdPV199tRYsWKC//vWvR32Or5ga0mtra7V27VpNmTLFc8xqtSonJ0e5ublNPic3N1eTJ0/2OjZ06FAtXry4yfNrampUU1Pjue9wOFpeOFrEHmZTx8QodUw8+l8MSWpwuubiVNbWq7KmQZW1jT/Xev9cVdugitp6VTUeP/jnQ8+prG1QdV2D6hoOX9iutsGp2gZJrfxlAAKT+wsB188HQr37Mdf9Ayc1+dgxXufgOwcfO/gLg4O/O2jqHO9jTbywDv+S4vDX9f6C4kjveWhth/J63qFfmjRRT1Pv0dTrN/mOTRxs6rzjfb2mPtah16Vzu2i9MGbg4SciaOwortKeshqFWS36RSqruwNoOxaLpVlDzs1QX1+vV155RU8++aQuvvhir8eGDx+u1157TaeeeqqWLVvm6ZQ9WN++feV0OvXZZ595hrsfrEOHDiorK1NFRYUniG/YsOGYde3bt09btmzRCy+8oHPPPVeStGLFCq9zTj31VL344osqLi4+Ym/67373O/Xp00d/+9vfVF9fr6uvvvqY7+0Lpv5b37t3rxoaGpSamup1PDU1Vd99912TzykoKGjy/IKCgibPnz59utf8BwQWm9WiWHuYYu1hUpxvX9swDNU7DdU1OFVXb6i2wen6ucGp2nqn6hoM1Tsb/2xwqsFpqM7p+tn9WIPTkNMw1OCUGpxO15+GIafT9dpOp6EGw1CD85Bb4zkN7vMOOsdpGHIaktMwZBjyHDMaj7nfzzC8z3W/hmFIRuPncz/HkGs0ghrPN+R+7MB5hoxD7h+owZAh9yKWzkNe99D3MdyvIUkH1dJ41/N67vfQQfcPnHPgtfyJp2b3nabPaqNq4A+c/vYfKXxu5U/7JEn9MhP9/pdlAGhr7733nvbv369x48YpIcF7BO3IkSM1d+5c/fnPf9aFF16ok08+WaNGjVJ9fb0++OAD3X333eratavGjh2rG2+80bNw3Pbt21VUVKRrr71WgwYNUnR0tO655x7ddtttWrVq1WErxzclKSlJ7dq10/PPP6/09HTl5eUdNkr7uuuu02OPPabhw4dr+vTpSk9P1/r165WRkaHs7GxJUq9evTR48GDdfffduvHGG4/Z++4rQd/aTJkyxavn3eFwKDMz08SK4C8sFotraLvNKjHN0K8d/MXBkQK/Drrv/tl9nuc1JK/H5DnHOOj5B873fv6B83TYax94nabe1+s9Dzn/sMcOed7Bxw499+BHDn3fw1+3iXOP8XhTDn3saM/zrsU4ymOHvctRazn00GHv28STjvUc1zlH+QBHOBTZjOGACEzD+3fUySkxqq3nCxkAONTcuXOVk5NzWECXXCF9xowZSk5O1qJFi/Twww/r8ccfV3x8vH75y196zps9e7buuece/f73v9e+ffvUuXNn3XPPPZKk5ORk/fOf/9Sdd96pF154QRdeeKEefPBBjR8//qh1Wa1WLVy4ULfddpv69OmjHj16aNasWRoyZIjnnIiICP3nP//RHXfcoUsvvVT19fU65ZRT9Nxzz3m91rhx4/TFF1/oxhtvbMGVah6LYeJm1rW1tYqOjtabb76p4cOHe46PHTtWJSUlTa6c17lzZ02ePFmTJk3yHHvggQe0ePFibdy48Zjv6XA4lJCQoNLSUsXHx/viYwAA0CK0Tb7HNQUQaKqrq7Vt2zavvb5hvocffliLFi3SV199dcxzj/bvsDntUutu8HYMERERGjBggJYtW+Y55nQ6tWzZMs8Qg0NlZ2d7nS9JS5cuPeL5AAAAAAA0R3l5uTZv3qxnn31Wt956a5u+t6khXZImT56sF154QS+//LK+/fZb3XLLLaqoqPAsLDBmzBivheVuv/12LVmyRE8++aS+++47Pfjgg1qzZo0mTpxo1kcAAAAAAASRiRMnasCAARoyZEibDnWX/GBO+q9//Wvt2bNHU6dOVUFBgU477TQtWbLEszhcXl6erNYD3yWcddZZWrBgge677z7dc8896t69uxYvXswe6QAAAAAAn5g/f/5xLVLXGkydk24G5qgBAPwNbZPvcU0BBBrmpAe+oJiTDgAAAAAADiCkAwAAAICfCLGBzkHFV//uCOkAAAAAYLLw8HBJUmVlpcmV4ETV1tZKkmw2W4tex/SF4wAAAAAg1NlsNiUmJqqoqEiSFB0dLYvFYnJVOF5Op1N79uxRdHS0wsJaFrMJ6QAAAADgB9LS0iTJE9QRWKxWqzp37tziL1cI6QAAAADgBywWi9LT05WSkqK6ujqzy0EzRUREeG0ffqII6QAAAADgR2w2W4vnNSNwsXAcAAAAAAB+gpAOAAAAAICfIKQDAAAAAOAnQm5OunuDeYfDYXIlAAC4uNskdxuFlqO9BwD4k+a09SEX0svKyiRJmZmZJlcCAIC3srIyJSQkmF1GUKC9BwD4o+Np6y1GiH1t73Q6lZ+fr7i4uBbvX+dwOJSZmakdO3YoPj7eRxUGN65Z83HNmo9r1nxcs+bz5TUzDENlZWXKyMjwydYtoL03G9es+bhmzcc1az6uWfOY1daHXE+61WpVp06dfPqa8fHx/EfeTFyz5uOaNR/XrPm4Zs3nq2tGD7pv0d77B65Z83HNmo9r1nxcs+Zp67aer+sBAAAAAPAThHQAAAAAAPwEIb0F7Ha7HnjgAdntdrNLCRhcs+bjmjUf16z5uGbNxzULHfy7bj6uWfNxzZqPa9Z8XLPmMet6hdzCcQAAAAAA+Ct60gEAAAAA8BOEdAAAAAAA/AQhHQAAAAAAP0FIBwAAAADATxDSW+C5555T165dFRkZqUGDBmn16tVml2SK6dOn64wzzlBcXJxSUlI0fPhwbdmyxeuc6upqTZgwQe3atVNsbKxGjhypwsJCr3Py8vJ02WWXKTo6WikpKbrzzjtVX1/flh/FNI8//rgsFosmTZrkOcY1O9yuXbv0m9/8Ru3atVNUVJT69u2rNWvWeB43DENTp05Venq6oqKilJOTo61bt3q9RnFxsUaPHq34+HglJiZq3LhxKi8vb+uP0iYaGhp0//33KysrS1FRUTr55JP18MMP6+D1QkP9mi1fvlxXXHGFMjIyZLFYtHjxYq/HfXV9vvrqK5177rmKjIxUZmamZsyY0dofDT5CW38A7X3L0NYfH9r65qGtP7aAbOsNnJCFCxcaERERxksvvWR8/fXXxk033WQkJiYahYWFZpfW5oYOHWrMmzfP2Lx5s7Fhwwbj0ksvNTp37myUl5d7zrn55puNzMxMY9myZcaaNWuMwYMHG2eddZbn8fr6eqNPnz5GTk6OsX79euODDz4w2rdvb0yZMsWMj9SmVq9ebXTt2tU49dRTjdtvv91znGvmrbi42OjSpYtx/fXXG6tWrTJ++ukn46OPPjJ++OEHzzmPP/64kZCQYCxevNjYuHGjceWVVxpZWVlGVVWV55xLLrnE6Nevn7Fy5Urjf//7n9GtWzfjuuuuM+MjtbpHH33UaNeunfHee+8Z27ZtMxYtWmTExsYaf/3rXz3nhPo1++CDD4x7773XeOuttwxJxttvv+31uC+uT2lpqZGammqMHj3a2Lx5s/Haa68ZUVFRxt///ve2+pg4QbT13mjvTxxt/fGhrW8+2vpjC8S2npB+gs4880xjwoQJnvsNDQ1GRkaGMX36dBOr8g9FRUWGJOOzzz4zDMMwSkpKjPDwcGPRokWec7799ltDkpGbm2sYhusvj9VqNQoKCjznzJ4924iPjzdqamra9gO0obKyMqN79+7G0qVLjfPOO8/TcHPNDnf33Xcb55xzzhEfdzqdRlpamvHnP//Zc6ykpMSw2+3Ga6+9ZhiGYXzzzTeGJOPLL7/0nPPhhx8aFovF2LVrV+sVb5LLLrvMuPHGG72OXX311cbo0aMNw+CaHerQhttX1+dvf/ubkZSU5PX38u677zZ69OjRyp8ILUVbf3S098eHtv740dY3H2198wRKW89w9xNQW1urtWvXKicnx3PMarUqJydHubm5JlbmH0pLSyVJycnJkqS1a9eqrq7O63r17NlTnTt39lyv3Nxc9e3bV6mpqZ5zhg4dKofDoa+//roNq29bEyZM0GWXXeZ1bSSuWVPeeecdDRw4UNdcc41SUlLUv39/vfDCC57Ht23bpoKCAq9rlpCQoEGDBnlds8TERA0cONBzTk5OjqxWq1atWtV2H6aNnHXWWVq2bJm+//57SdLGjRu1YsUKDRs2TBLX7Fh8dX1yc3P1y1/+UhEREZ5zhg4dqi1btmj//v1t9GnQXLT1x0Z7f3xo648fbX3z0da3jL+29WEn+oFC2d69e9XQ0OD1P0xJSk1N1XfffWdSVf7B6XRq0qRJOvvss9WnTx9JUkFBgSIiIpSYmOh1bmpqqgoKCjznNHU93Y8Fo4ULF2rdunX68ssvD3uMa3a4n376SbNnz9bkyZN1zz336Msvv9Rtt92miIgIjR071vOZm7omB1+zlJQUr8fDwsKUnJwclNfsT3/6kxwOh3r27CmbzaaGhgY9+uijGj16tCRxzY7BV9enoKBAWVlZh72G+7GkpKRWqR8tQ1t/dLT3x4e2vnlo65uPtr5l/LWtJ6TDpyZMmKDNmzdrxYoVZpfi13bs2KHbb79dS5cuVWRkpNnlBASn06mBAwfqsccekyT1799fmzdv1pw5czR27FiTq/NPb7zxhl599VUtWLBAvXv31oYNGzRp0iRlZGRwzQC0CO39sdHWNx9tffPR1gcnhrufgPbt28tmsx22+mZhYaHS0tJMqsp8EydO1HvvvadPPvlEnTp18hxPS0tTbW2tSkpKvM4/+HqlpaU1eT3djwWbtWvXqqioSKeffrrCwsIUFhamzz77TLNmzVJYWJhSU1O5ZodIT0/XKaec4nWsV69eysvLk3TgMx/t72VaWpqKioq8Hq+vr1dxcXFQXrM777xTf/rTnzRq1Cj17dtXv/3tb/WHP/xB06dPl8Q1OxZfXZ9Q+7saLGjrj4z2/vjQ1jcfbX3z0da3jL+29YT0ExAREaEBAwZo2bJlnmNOp1PLli1Tdna2iZWZwzAMTZw4UW+//bY+/vjjw4Z6DBgwQOHh4V7Xa8uWLcrLy/Ncr+zsbG3atMnrL8DSpUsVHx9/2P+sg8GFF16oTZs2acOGDZ7bwIEDNXr0aM/PXDNvZ5999mFb/Xz//ffq0qWLJCkrK0tpaWle18zhcGjVqlVe16ykpERr1671nPPxxx/L6XRq0KBBbfAp2lZlZaWsVu//zdtsNjmdTklcs2Px1fXJzs7W8uXLVVdX5zln6dKl6tGjB0Pd/Rht/eFo75uHtr75aOubj7a+Zfy2rT+h5eZgLFy40LDb7cb8+fONb775xhg/fryRmJjotfpmqLjllluMhIQE49NPPzV2797tuVVWVnrOufnmm43OnTsbH3/8sbFmzRojOzvbyM7O9jzu3mLk4osvNjZs2GAsWbLE6NChQ9BuMdKUg1d8NQyu2aFWr15thIWFGY8++qixdetW49VXXzWio6ONf/7zn55zHn/8cSMxMdH497//bXz11VfGVVdd1eQWGv379zdWrVplrFixwujevXvQbDFyqLFjxxodO3b0bMvy1ltvGe3btzfuuusuzzmhfs3KysqM9evXG+vXrzckGU899ZSxfv16Y/v27YZh+Ob6lJSUGKmpqcZvf/tbY/PmzcbChQuN6OhotmALALT13mjvW462/uho65uPtv7YArGtJ6S3wDPPPGN07tzZiIiIMM4880xj5cqVZpdkCklN3ubNm+c5p6qqyvj9739vJCUlGdHR0caIESOM3bt3e73Ozz//bAwbNsyIiooy2rdvb9xxxx1GXV1dG38a8xzacHPNDvfuu+8affr0Mex2u9GzZ0/j+eef93rc6XQa999/v5GammrY7XbjwgsvNLZs2eJ1zr59+4zrrrvOiI2NNeLj440bbrjBKCsra8uP0WYcDodx++23G507dzYiIyONk046ybj33nu9tgcJ9Wv2ySefNPn/r7FjxxqG4bvrs3HjRuOcc84x7Ha70bFjR+Pxxx9vq4+IFqKtP4D2vuVo64+Ntr55aOuPLRDbeothGEbz+98BAAAAAICvMScdAAAAAAA/QUgHAAAAAMBPENIBAAAAAPAThHQAAAAAAPwEIR0AAAAAAD9BSAcAAAAAwE8Q0gEAAAAA8BOEdABtzmKxaPHixWaXAQAAWgltPXDiCOlAiLn++utlsVgOu11yySVmlwYAAHyAth4IbGFmFwCg7V1yySWaN2+e1zG73W5SNQAAwNdo64HARU86EILsdrvS0tK8bklJSZJcw9Nmz56tYcOGKSoqSieddJLefPNNr+dv2rRJF1xwgaKiotSuXTuNHz9e5eXlXue89NJL6t27t+x2u9LT0zVx4kSvx/fu3asRI0YoOjpa3bt31zvvvNO6HxoAgBBCWw8ELkI6gMPcf//9GjlypDZu3KjRo0dr1KhR+vbbbyVJFRUVGjp0qJKSkvTll19q0aJF+u9//+vVMM+ePVsTJkzQ+PHjtWnTJr3zzjvq1q2b13s89NBDuvbaa/XVV1/p0ksv1ejRo1VcXNymnxMAgFBFWw/4MQNASBk7dqxhs9mMmJgYr9ujjz5qGIZhSDJuvvlmr+cMGjTIuOWWWwzDMIznn3/eSEpKMsrLyz2Pv//++4bVajUKCgoMwzCMjIwM49577z1iDZKM++67z3O/vLzckGR8+OGHPvucAACEKtp6ILAxJx0IQeeff75mz57tdSw5Odnzc3Z2ttdj2dnZ2rBhgyTp22+/Vb9+/RQTE+N5/Oyzz5bT6dSWLVtksViUn5+vCy+88Kg1nHrqqZ6fY2JiFB8fr6KiohP9SAAA4CC09UDgIqQDISgmJuawIWm+EhUVdVznhYeHe923WCxyOp2tURIAACGHth4IXMxJB3CYlStXHna/V69ekqRevXpp48aNqqio8Dz++eefy2q1qkePHoqLi1PXrl21bNmyNq0ZAAAcP9p6wH/Rkw6EoJqaGhUUFHgdCwsLU/v27SVJixYt0sCBA3XOOefo1Vdf1erVqzV37lxJ0ujRo/XAAw9o7NixevDBB7Vnzx7deuut+u1vf6vU1FRJ0oMPPqibb75ZKSkpGjZsmMrKyvT555/r1ltvbdsPCgBAiKKtBwIXIR0IQUuWLFF6errXsR49eui7776T5FqNdeHChfr973+v9PR0vfbaazrllFMkSdHR0froo490++2364wzzlB0dLRGjhypp556yvNaY8eOVXV1tZ5++mn98Y9/VPv27fWrX/2q7T4gAAAhjrYeCFwWwzAMs4sA4D8sFovefvttDR8+3OxSAABAK6CtB/wbc9IBAAAAAPAThHQAAAAAAPwEw90BAAAAAPAT9KQDAAAAAOAnCOkAAAAAAPgJQjoAAAAAAH6CkA4AAAAAgJ8gpAMAAAAA4CcI6QAAAAAA+AlCOgAAAAAAfoKQDgAAAACAnyCkAwAAAADgJ/4/00jRzBWc/4MAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss and accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, label='Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracies, label='Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 0.0012, Accuracy: 100.00%\n",
      "Epoch 200/1000, Loss: 0.0010, Accuracy: 100.00%\n",
      "Epoch 300/1000, Loss: 0.0009, Accuracy: 100.00%\n",
      "Epoch 400/1000, Loss: 0.0008, Accuracy: 100.00%\n",
      "Epoch 500/1000, Loss: 0.0008, Accuracy: 100.00%\n",
      "Epoch 600/1000, Loss: 0.0007, Accuracy: 100.00%\n",
      "Epoch 700/1000, Loss: 0.0007, Accuracy: 100.00%\n",
      "Epoch 800/1000, Loss: 0.0006, Accuracy: 100.00%\n",
      "Epoch 900/1000, Loss: 0.0006, Accuracy: 100.00%\n",
      "Epoch 1000/1000, Loss: 0.0005, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Alternative: use TensorBoard!\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Reinstantiate the network, loss function, and optimizer\n",
    "model = SimpleNet()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Instantiate the TensorBoard writer\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    avg_loss = running_loss / len(data_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    # Log the metrics to TensorBoard\n",
    "    writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', accuracy, epoch)\n",
    "    \n",
    "    # Print loss and accuracy after every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'runs' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "folder_path = \"runs\"\n",
    "\n",
    "shutil.rmtree(folder_path)\n",
    "\n",
    "print(f\"Folder '{folder_path}' has been deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you make predictions using a trained neural network in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: What performance metrics can you use to evaluate a neural network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you visualize the performance of a neural network using `matplotlib`?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you save a trained neural network model in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you load a saved neural network model in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you perform hyperparameter tuning to improve the performance of a neural network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: What regularization techniques can you implement to prevent overfitting in neural networks?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you use learning rate scheduling to adjust the learning rate during training?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling real-world data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you preprocess a real-world dataset for training a neural network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you handle missing data in a real-world dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you encode categorical variables for use in a neural network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q27: How do you split a real-world dataset into training, validation, and test sets?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q28: How do you train a neural network on a real-world dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q29: How do you implement a neural network from scratch without using PyTorch's built-in functions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q30: How do you experiment with different neural network architectures to see their impact on performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q31: How do you apply a neural network to a new dataset and evaluate its performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q32: How do you improve neural network performance with advanced techniques like dropout or batch normalization?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q33: How do you interpret and visualize the decisions made by a neural network?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
