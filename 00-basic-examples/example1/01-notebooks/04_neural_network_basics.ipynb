{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network basics\n",
    "\n",
    "Welcome to the `04_neural_network_basics` notebook. This part of the portfolio is designed to introduce fundamental concepts and techniques in PyTorch, with a particular emphasis on building and understanding neural networks.\n",
    "\n",
    "Throughout this notebook, I'll explore essential topics such as setting up the environment, defining neural network architectures, and implementing both forward and backward propagation. I'll also cover training procedures, model evaluation, and techniques to save and load trained models.\n",
    "\n",
    "By working through these exercises, the aim is to gain practical experience in constructing and optimizing neural networks, forming a solid foundation for more advanced machine learning endeavors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Understanding neural networks](#understanding-neural-networks)\n",
    "3. [Setting up the environment](#setting-up-the-environment)\n",
    "4. [Building a neural network](#building-a-neural-network)\n",
    "5. [Forward propagation](#forward-propagation)\n",
    "6. [Loss function](#loss-function)\n",
    "7. [Backpropagation](#backpropagation)\n",
    "8. [Training the neural network](#training-the-neural-network)\n",
    "9. [Evaluating the model](#evaluating-the-model)\n",
    "10. [Saving and loading the model](#saving-and-loading-the-model)\n",
    "11. [Optimizations](#optimizations)\n",
    "12. [Handling real-world data](#handling-real-world-data)\n",
    "13. [Conclusion](#conclusion)\n",
    "14. [Further exercises](#further-exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding neural networks\n",
    "\n",
    "Neural networks are a foundational technique in machine learning and artificial intelligence, utilized for identifying patterns and relationships within data. Unlike traditional algorithms, which often rely on predefined rules, neural networks learn directly from examples, making them highly adaptable and powerful. These models can handle a variety of tasks, from classification to regression, by adjusting their parameters through training, thereby improving their performance with more data and experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key concepts\n",
    "\n",
    "#### 1. Neurons and layers\n",
    "Neural networks are inspired by the structure and function of the human brain, consisting of interconnected units called neurons. These neurons are organized into layers: the input layer, hidden layers, and the output layer.\n",
    "\n",
    "- **Input layer**: The first layer in the network that receives the input data. Each neuron in this layer represents a feature of the input data.\n",
    "- **Hidden layers**: Layers between the input and output layers. These layers perform various transformations on the inputs received, allowing the network to learn complex patterns. There can be multiple hidden layers, which is why deep neural networks are also known as deep learning models.\n",
    "- **Output layer**: The final layer that produces the output of the network. The number of neurons in this layer corresponds to the number of desired outputs.\n",
    "\n",
    "#### 2. Activation functions\n",
    "Activation functions introduce non-linearity into the network, enabling it to learn and model complex relationships in the data. Without activation functions, the network would only be able to model linear relationships.\n",
    "\n",
    "- **ReLU (Rectified Linear Unit)**: Outputs the input directly if it is positive; otherwise, it outputs zero. It is widely used due to its simplicity and effectiveness.\n",
    "- **Sigmoid**: Compresses the input to a range between 0 and 1. It is often used in binary classification problems.\n",
    "- **Tanh (Hyperbolic tangent)**: Compresses the input to a range between -1 and 1, centering the data. It is often used in practice but can lead to issues with gradient vanishing.\n",
    "\n",
    "#### 3. Forward propagation\n",
    "Forward propagation is the process of passing input data through the network to obtain an output. During this process, each neuron computes a weighted sum of its inputs, adds a bias term, and applies an activation function to produce its output. The output of one layer becomes the input to the next layer, and this process continues until the final output layer.\n",
    "\n",
    "#### 4. Loss function\n",
    "The loss function measures the difference between the predicted outputs and the actual targets. It quantifies how well the neural network is performing. The goal of training the network is to minimize this loss.\n",
    "\n",
    "- **Mean Squared Error (MSE)**: Commonly used for regression tasks, it calculates the average squared difference between predicted and actual values.\n",
    "- **Cross-entropy loss**: Used for classification tasks, it measures the difference between the predicted probability distribution and the actual distribution.\n",
    "\n",
    "#### 5. Backpropagation\n",
    "Backpropagation is the process of updating the network's weights to minimize the loss. It involves calculating the gradient of the loss function with respect to each weight and adjusting the weights in the opposite direction of the gradient. This ensures that the loss decreases with each iteration.\n",
    "\n",
    "#### 6. Optimizers\n",
    "Optimizers are algorithms that adjust the weights of the network to minimize the loss. They use the gradients calculated during backpropagation to update the weights.\n",
    "\n",
    "- **Stochastic Gradient Descent (SGD)**: Updates the weights using a small, randomly chosen subset of the data (a mini-batch). This makes the optimization process faster and allows the model to learn from a diverse set of examples.\n",
    "- **Adam (Adaptive Moment Estimation)**: An extension of SGD that adapts the learning rate for each parameter, making it more efficient and robust in practice.\n",
    "\n",
    "#### 7. Training and validation\n",
    "Training a neural network involves iteratively feeding data through the network, calculating the loss, and updating the weights. This process is repeated for a specified number of epochs (complete passes through the training dataset).\n",
    "\n",
    "- **Training set**: The subset of data used to train the model.\n",
    "- **Validation set**: A separate subset of data used to evaluate the model's performance during training, helping to tune hyperparameters and prevent overfitting.\n",
    "\n",
    "#### 8. Overfitting and underfitting\n",
    "- **Overfitting**: Occurs when the model learns the training data too well, capturing noise and specific patterns that do not generalize to new data. This results in poor performance on the validation set.\n",
    "- **Underfitting**: Occurs when the model is too simple to capture the underlying patterns in the data, leading to poor performance on both the training and validation sets.\n",
    "\n",
    "#### 9. Regularization techniques\n",
    "Regularization techniques are used to prevent overfitting by adding constraints or penalties to the model.\n",
    "\n",
    "- **Dropout**: Randomly drops a fraction of neurons during training, forcing the network to learn redundant representations and improving generalization.\n",
    "- **L2 regularization (Ridge)**: Adds a penalty proportional to the sum of the squares of the weights, discouraging large weights and promoting simpler models.\n",
    "\n",
    "#### 10. Model evaluation\n",
    "After training, the model's performance is evaluated on a test set that was not seen during training. This provides an unbiased estimate of how well the model generalizes to new data.\n",
    "\n",
    "- **Accuracy**: The proportion of correctly classified instances out of the total instances.\n",
    "- **Precision, recall, and F1 score**: Metrics that provide deeper insights into the model's performance, especially for imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maths\n",
    "\n",
    "#### 1. Structure of Neural Networks\n",
    "\n",
    "##### Layers\n",
    "- **Input layer**: This is the first layer of the network, which receives the raw input data. Each neuron in this layer corresponds to one feature of the input data.\n",
    "- **Hidden layers**: These layers are located between the input and output layers. They perform computations and extract features from the input data. A neural network can have multiple hidden layers, which allows it to learn complex patterns.\n",
    "- **Output layer**: This is the final layer of the network, which produces the output. The number of neurons in this layer depends on the type of problem (e.g., one neuron for binary classification, multiple neurons for multi-class classification).\n",
    "\n",
    "##### Neurons\n",
    "Each neuron in a neural network computes a weighted sum of its inputs, adds a bias term, and applies an activation function to produce its output.\n",
    "\n",
    "#### 2. Forward propagation\n",
    "\n",
    "Forward propagation is the process by which input data passes through the network to generate an output. It involves the following steps:\n",
    "\n",
    "##### Weighted sum\n",
    "For a given neuron $ j $ in layer $ l $, the input $ z_j^{(l)} $ is computed as:\n",
    "$$ z_j^{(l)} = \\sum_{i=1}^{n} w_{ij}^{(l-1)} a_i^{(l-1)} + b_j^{(l)} $$\n",
    "where:\n",
    "- $ w_{ij}^{(l-1)} $ is the weight connecting neuron $ i $ in layer $ l-1 $ to neuron $ j $ in layer $ l $.\n",
    "- $ a_i^{(l-1)} $ is the activation of neuron $ i $ in layer $ l-1 $.\n",
    "- $ b_j^{(l)} $ is the bias term for neuron $ j $ in layer $ l $.\n",
    "- $ n $ is the number of neurons in layer $ l-1 $.\n",
    "\n",
    "##### Activation function\n",
    "The output $ a_j^{(l)} $ of neuron $ j $ in layer $ l $ is obtained by applying an activation function $ f $ to the weighted sum:\n",
    "$$ a_j^{(l)} = f(z_j^{(l)}) $$\n",
    "\n",
    "Common activation functions include:\n",
    "- **Sigmoid**: $ f(z) = \\frac{1}{1 + e^{-z}} $\n",
    "- **Tanh**: $ f(z) = \\tanh(z) $\n",
    "- **ReLU (Rectified Linear Unit)**: $ f(z) = \\max(0, z) $\n",
    "\n",
    "#### 3. Loss function\n",
    "\n",
    "The loss function quantifies the difference between the predicted output and the actual target. The goal of training a neural network is to minimize this loss. Common loss functions include:\n",
    "\n",
    "- **Mean Squared Error (MSE)**: Used for regression tasks, defined as:\n",
    "  $$ \\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y_i})^2 $$\n",
    "  where $ y_i $ is the true value and $ \\hat{y_i} $ is the predicted value.\n",
    "\n",
    "- **Cross-entropy loss**: Used for classification tasks, defined as:\n",
    "  $$ \\text{Cross-Entropy} = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{k} y_{ij} \\log(\\hat{y_{ij}}) $$\n",
    "  where $ y_{ij} $ is the binary indicator (0 or 1) if class label $ j $ is the correct classification for input $ i $, and $ \\hat{y_{ij}} $ is the predicted probability of $ i $ being in class $ j $.\n",
    "\n",
    "#### 4. Backpropagation\n",
    "\n",
    "Backpropagation is the process of adjusting the network's weights to minimize the loss. It involves calculating the gradient of the loss function with respect to each weight and updating the weights using gradient descent.\n",
    "\n",
    "##### Gradient descent\n",
    "The weight update rule for gradient descent is:\n",
    "$$ w_{ij} = w_{ij} - \\eta \\frac{\\partial L}{\\partial w_{ij}} $$\n",
    "where:\n",
    "- $ \\eta $ is the learning rate, a hyperparameter that controls the step size of the update.\n",
    "- $ \\frac{\\partial L}{\\partial w_{ij}} $ is the partial derivative of the loss function with respect to the weight $ w_{ij} $.\n",
    "\n",
    "##### Calculating gradients\n",
    "The gradients are computed using the chain rule of calculus. For a given weight $ w_{ij} $, the gradient is calculated as:\n",
    "$$ \\frac{\\partial L}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial a_j} \\cdot \\frac{\\partial a_j}{\\partial z_j} \\cdot \\frac{\\partial z_j}{\\partial w_{ij}} $$\n",
    "\n",
    "The partial derivatives are:\n",
    "- $ \\frac{\\partial L}{\\partial a_j} $: The derivative of the loss with respect to the neuron's activation, which depends on the loss function.\n",
    "- $ \\frac{\\partial a_j}{\\partial z_j} $: The derivative of the activation function.\n",
    "- $ \\frac{\\partial z_j}{\\partial w_{ij}} $: The derivative of the weighted sum with respect to the weight, which is the input to the neuron.\n",
    "\n",
    "#### 5. Training the network\n",
    "\n",
    "Training a neural network involves the following steps:\n",
    "1. **Initialize weights**: Set the initial values of the weights, typically using small random values.\n",
    "2. **Forward propagation**: Compute the outputs of the network for a batch of input data.\n",
    "3. **Compute loss**: Calculate the loss using the predicted outputs and the true targets.\n",
    "4. **Backpropagation**: Compute the gradients of the loss with respect to the weights.\n",
    "5. **Update weights**: Adjust the weights using gradient descent.\n",
    "6. **Repeat**: Iterate over the training data for a specified number of epochs until the loss converges.\n",
    "\n",
    "#### 6. Regularization techniques\n",
    "\n",
    "To prevent overfitting, various regularization techniques can be applied:\n",
    "\n",
    "- **L2 regularization (Ridge)**: Adds a penalty proportional to the sum of the squares of the weights to the loss function.\n",
    "  $$ L_{\\text{ridge}} = L + \\lambda \\sum_{j} w_j^2 $$\n",
    "  where $ \\lambda $ is the regularization parameter.\n",
    "\n",
    "- **L1 regularization (Lasso)**: Adds a penalty proportional to the sum of the absolute values of the weights to the loss function.\n",
    "  $$ L_{\\text{lasso}} = L + \\lambda \\sum_{j} |w_j| $$\n",
    "\n",
    "- **Dropout**: Randomly sets a fraction of the neurons to zero during training, which helps prevent the network from becoming too reliant on any single neuron and improves generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling real-world data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further exercises"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
