{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and processing in PyTorch\n",
    "\n",
    "Welcome to the `06_data_loading_processing` notebook. This piece of code provides an overview of essential techniques for data handling in PyTorch, including environment setup, working with datasets and DataLoader, and implementing data transformations and augmentations. \n",
    "\n",
    "It also covers methods for managing different data formats, constructing preprocessing pipelines, and optimizing data loading for large datasets. Practical examples are included to demonstrate these concepts in action, making this notebook a valuable resource for efficient data management in machine learning workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding data loading and processing in PyTorch](#understanding-data-loading-and-processing-in-pytorch)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Working with datasets and DataLoader](#working-with-datasets-and-dataloader)\n",
    "4. [Data transformations and augmentations](#data-transformations-and-augmentations)\n",
    "5. [Handling different data formats](#handling-different-data-formats)\n",
    "6. [Preprocessing pipelines](#preprocessing-pipelines)\n",
    "7. [Advanced data loading techniques](#advanced-data-loading-techniques)\n",
    "8. [Practical examples and use cases](#practical-examples-and-use-cases)\n",
    "9. [Conclusion](#conclusion)\n",
    "10. [Further exercises](#further-exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding data loading and processing in PyTorch\n",
    "\n",
    "Data loading and processing are crucial steps in any machine learning workflow. In PyTorch, these steps are designed to be both flexible and efficient, allowing for seamless integration with various data types and formats. This section will provide an in-depth explanation of how data is handled in PyTorch, setting the foundation for the rest of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Why data loading and processing matter**\n",
    "\n",
    "The performance of a machine learning model heavily relies on the quality and format of the data it receives. Properly loading and preprocessing data ensures that the model can effectively learn from the input data, leading to better generalization and accuracy. Efficient data handling also reduces bottlenecks during training, particularly when working with large datasets or complex models.\n",
    "\n",
    "#### **Key concepts**\n",
    "\n",
    "- **Datasets**: PyTorch provides the `torch.utils.data.Dataset` class as an abstract class for handling datasets. Custom datasets can be created by subclassing `Dataset` and overriding two methods: `__len__()` to return the size of the dataset and `__getitem__()` to retrieve a data sample. PyTorch also offers built-in datasets like MNIST, CIFAR-10, and more, which can be easily loaded using `torchvision.datasets`.\n",
    "\n",
    "- **DataLoader**: The `torch.utils.data.DataLoader` class is responsible for loading data in batches, shuffling data, and handling multiprocessing for loading data in parallel. It is highly customizable, allowing for control over batch size, shuffling, and the number of worker threads used for loading.\n",
    "\n",
    "- **Transforms**: Data transformations are essential for normalizing, augmenting, and converting data into the appropriate format for model training. PyTorch’s `torchvision.transforms` module provides a wide range of predefined transformations that can be chained together using `transforms.Compose`. Custom transformations can also be created to fit specific needs.\n",
    "\n",
    "#### **Data loading workflow in PyTorch**\n",
    "\n",
    "The typical data loading workflow in PyTorch involves the following steps:\n",
    "\n",
    "- **Defining the dataset**: Whether using a built-in dataset or creating a custom one, the first step is to define the dataset by subclassing `Dataset`. This involves specifying how to access and return individual samples.\n",
    "\n",
    "- **Applying transforms**: Once the dataset is defined, transformations are applied to the data to ensure it is in the correct format for model training. This might include normalization, resizing, cropping, or more advanced augmentations like random rotations or color jitter.\n",
    "\n",
    "- **Creating DataLoader**: With the dataset and transformations in place, the DataLoader is created to handle the batching, shuffling, and parallel loading of data. This is where most of the heavy lifting in terms of data management happens.\n",
    "\n",
    "- **Iterating through data**: Finally, the DataLoader is used in the training loop to iterate through the dataset in batches, feeding data to the model for training or validation.\n",
    "\n",
    "#### **Handling large datasets**\n",
    "\n",
    "For large datasets that cannot fit into memory, PyTorch’s DataLoader supports lazy loading, where only a portion of the data is loaded into memory at a time. This is done through the use of custom datasets and careful management of batch sizes and worker threads. Techniques such as data streaming, where data is continuously fed from disk to memory, can also be employed.\n",
    "\n",
    "#### **Optimization techniques**\n",
    "\n",
    "Optimizing data loading and processing can have a significant impact on training speed and model performance. Some key techniques include:\n",
    "\n",
    "- **Using multiple workers**: Increasing the number of worker threads in the DataLoader can speed up data loading by parallelizing the process.\n",
    "\n",
    "- **Prefetching data**: Preloading the next batch while the model is training on the current batch can reduce the waiting time between epochs.\n",
    "\n",
    "- **Data augmentation**: Real-time data augmentation during training can increase the diversity of the dataset without the need to store augmented images on disk.\n",
    "\n",
    "#### **Common pitfalls and best practices**\n",
    "\n",
    "- **Shuffling data**: Always shuffle the training data to prevent the model from learning the order of the data, which can lead to overfitting.\n",
    "\n",
    "- **Normalizing data**: Proper normalization ensures that the data is on a similar scale, which is crucial for stable and efficient model training.\n",
    "\n",
    "- **Managing data formats**: Ensure that the data is in the correct format (e.g., tensors) before feeding it to the model. PyTorch expects data in the form of tensors, with specific shapes depending on the model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for data loading and processing in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio numpy pandas scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules for data handling in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with datasets and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you load built-in datasets using `torchvision`?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../00-src\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:06<00:00, 1561489.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../00-src\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../00-src\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../00-src\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 116348.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../00-src\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../00-src\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../00-src\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 2508841.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../00-src\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../00-src\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../00-src\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 4570664.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../00-src\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../00-src\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Using the MNIST dataset as an example:\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_dataset = datasets.MNIST(root='../00-src', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='../00-src', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you explore the properties of a dataset, such as size and classes, in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 60000\n",
      "Test dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Dataset size\n",
    "print(f'Train dataset size: {len(train_dataset)}')\n",
    "print(f'Test dataset size: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    }
   ],
   "source": [
    "# Classes\n",
    "print(f'Classes: {train_dataset.classes}')  # Only applicable for datasets that have 'classes' attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image shape: torch.Size([1, 28, 28])\n",
      "Sample label: 5\n"
     ]
    }
   ],
   "source": [
    "# Shape of a single sample\n",
    "sample_image, sample_label = train_dataset[0]\n",
    "print(f'Sample image shape: {sample_image.shape}')\n",
    "print(f'Sample label: {sample_label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you create a custom dataset class in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass torch.utils.data.Dataset and implement the __len__ and __getitem__ methods:\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):  # Initializes the dataset object with data, labels, and any optional transformations\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):  # Returns the total number of samples\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):  # Retrieves the sample and label at the given index\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you implement the `__len__` and `__getitem__` methods for a custom dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you use the DataLoader to batch data in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 64\n",
      "Batch size: 64\n",
      "Batch size: 64\n",
      "Batch size: 64\n",
      "Batch size: 64\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Create DataLoader for the custom dataset\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Convert the DataLoader to a list to randomly sample from it\n",
    "all_batches = list(train_loader)\n",
    "\n",
    "# Randomly select five batches\n",
    "sampled_batches = random.sample(all_batches, 5)\n",
    "\n",
    "# Iterate through the sampled batches and print the batch size\n",
    "for images, labels in sampled_batches:\n",
    "    print(f'Batch size: {images.size(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you shuffle data using DataLoader in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)  # set the shuffle parameter to True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you load data in parallel using multiple workers with DataLoader?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, num_workers=4)  # set the num_workers parameter in DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformations and augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you apply basic data transformations, such as normalization, in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the normalization transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalizes each channel to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Apply the transform when loading the dataset\n",
    "dataset = datasets.MNIST(root='../00-src', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you resize and crop images using PyTorch transformations?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the resize and crop transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize to 128x128\n",
    "    transforms.CenterCrop(112),     # Crop the center 112x112\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Apply the transform when loading the dataset\n",
    "dataset = datasets.MNIST(root='../00-src', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you compose multiple transformations using `transforms.Compose` in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose multiple transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize for grayscale images\n",
    "])\n",
    "\n",
    "# Apply the transform when loading the dataset\n",
    "dataset = datasets.MNIST(root='../00-src', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: What are some common data augmentation techniques like rotating, flipping, and color jittering in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentation transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=15),            # Rotate the image by up to 15 degrees\n",
    "    transforms.RandomHorizontalFlip(p=0.5),           # Randomly flip the image horizontally with a 50% probability\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Randomly change brightness, contrast, saturation, and hue\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Apply the transform when loading the dataset\n",
    "dataset = datasets.MNIST(root='../00-src', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ../00-src\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.1, 0.1))\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling different data formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you load image data from files and directories in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fellm\\AppData\\Local\\Temp\\ipykernel_2496\\4019146405.py:20: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar_ref.extractall(root_dir)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset extracted to ../00-src\\flower_photos\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "# Define the path to the root directory\n",
    "root_dir = '../00-src'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(root_dir):\n",
    "    os.makedirs(root_dir)\n",
    "\n",
    "# URL of a sample dataset (e.g., a small dataset of flowers)\n",
    "url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "tgz_path = os.path.join(root_dir, \"flower_photos.tgz\")\n",
    "\n",
    "# Download the dataset\n",
    "urllib.request.urlretrieve(url, tgz_path)\n",
    "\n",
    "# Extract the dataset\n",
    "with tarfile.open(tgz_path, 'r:gz') as tar_ref:\n",
    "    tar_ref.extractall(root_dir)\n",
    "\n",
    "# Define the directory containing the images\n",
    "data_dir = os.path.join(root_dir, \"flower_photos\")\n",
    "\n",
    "print(f\"Dataset extracted to {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for the images\n",
    "transform_imgs = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize images to 128x128\n",
    "    transforms.ToTensor(),          # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize images\n",
    "])\n",
    "\n",
    "# Load the image data from the directory\n",
    "dataset_imgs = datasets.ImageFolder(root=data_dir, transform=transform_imgs)\n",
    "\n",
    "# Create a DataLoader to batch and shuffle the data\n",
    "dataloader_imgs = DataLoader(dataset_imgs, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you load and preprocess CSV or tabular data using `pandas` and convert it to tensors?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to save the CSV file\n",
    "csv_path = os.path.join(root_dir, \"sample_data.csv\")\n",
    "\n",
    "# Create a sample CSV file\n",
    "df = pd.DataFrame({\n",
    "    'feature1': [1.0, 2.0, 3.0, 4.0, 5.0],\n",
    "    'feature2': [10.0, 20.0, 30.0, 40.0, 50.0],\n",
    "    'label': [0, 1, 0, 1, 0]\n",
    "})\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1., 10.],\n",
      "        [ 2., 20.],\n",
      "        [ 3., 30.],\n",
      "        [ 4., 40.],\n",
      "        [ 5., 50.]])\n",
      "tensor([0, 1, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file using pandas\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert features and labels to tensors\n",
    "features = torch.tensor(df[['feature1', 'feature2']].values, dtype=torch.float32)\n",
    "labels = torch.tensor(df['label'].values, dtype=torch.long)\n",
    "\n",
    "# Example of how to use the features and labels tensors\n",
    "print(features)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you load and preprocess text data in PyTorch, including tokenization and embedding creation?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../00-src\\\\sample_text.txt', <http.client.HTTPMessage at 0x1bc56d6d370>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to save the text file\n",
    "text_path = os.path.join(root_dir, \"sample_text.txt\")\n",
    "\n",
    "# Download a sample text file (e.g., from Project Gutenberg)\n",
    "url = \"https://www.gutenberg.org/files/11/11-0.txt\"  # Alice's Adventures in Wonderland\n",
    "urllib.request.urlretrieve(url, text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fellm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\fellm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2,  2,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# Download the necessary NLTK data\n",
    "nltk.download('punkt')  # This will download the punkt tokenizer\n",
    "nltk.download('punkt_tab')  # Add this line to download 'punkt_tab' if needed\n",
    "\n",
    "# Load the text data with the correct encoding\n",
    "with open(text_path, 'r', encoding='utf-8') as f:\n",
    "    text_data = f.readlines()\n",
    "\n",
    "# Tokenize the text data using nltk\n",
    "tokenized_data = [nltk.word_tokenize(line.lower()) for line in text_data]\n",
    "\n",
    "# Build a vocabulary\n",
    "counter = Counter([word for line in tokenized_data for word in line])\n",
    "vocab = {word: idx for idx, (word, _) in enumerate(counter.items(), start=1)}\n",
    "vocab['<unk>'] = 0  # Add an unknown token\n",
    "\n",
    "# Convert tokens to tensor indices\n",
    "text_as_tensor = [torch.tensor([vocab.get(word, 0) for word in line], dtype=torch.long) for line in tokenized_data]\n",
    "\n",
    "# Example of how to use the text tensors\n",
    "print(text_as_tensor[0])  # Print tensor for the first line of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: What strategies can you use to handle missing data when loading and preprocessing datasets?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values: 9836\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a large dataset with 10000 samples and 10 features\n",
    "num_samples = 10000\n",
    "num_features = 10\n",
    "\n",
    "# Generate random data\n",
    "data = torch.randn(num_samples, num_features)\n",
    "\n",
    "# Introduce missing values (NaNs) in 10% of the data\n",
    "mask = torch.rand(num_samples, num_features) < 0.1\n",
    "data[mask] = float('nan')\n",
    "\n",
    "# Print a summary of the missing data\n",
    "print(f\"Number of missing values: {torch.isnan(data).sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: torch.Size([10000, 10])\n",
      "Cleaned dataset size: torch.Size([3601, 10])\n"
     ]
    }
   ],
   "source": [
    "# Dropping rows with missing values:\n",
    "rows_with_nan = torch.isnan(data).any(dim=1)\n",
    "\n",
    "# Drop rows with any NaNs\n",
    "cleaned_data = data[~rows_with_nan]\n",
    "\n",
    "print(f\"Original dataset size: {data.size()}\")\n",
    "print(f\"Cleaned dataset size: {cleaned_data.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values after filling with zero: 0\n"
     ]
    }
   ],
   "source": [
    "# Fill NaNs with zero:\n",
    "data_filled_zero = torch.nan_to_num(data, nan=0.0)\n",
    "\n",
    "print(f\"Number of missing values after filling with zero: {torch.isnan(data_filled_zero).sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values after filling with column mean: 0\n"
     ]
    }
   ],
   "source": [
    "# Fill NaNs with the column mean:\n",
    "column_means = torch.nanmean(data, dim=0)\n",
    "\n",
    "# Expand the column means to match the data shape\n",
    "column_means_expanded = column_means.unsqueeze(0).expand_as(data)\n",
    "\n",
    "# Filling in the mean\n",
    "data_filled_mean = torch.where(torch.isnan(data), column_means_expanded, data)\n",
    "\n",
    "print(f\"Number of missing values after filling with column mean: {torch.isnan(data_filled_mean).sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values after interpolation: 0\n"
     ]
    }
   ],
   "source": [
    "# Interpolating missing data using pandas:\n",
    "data_df = pd.DataFrame(data.numpy())\n",
    "\n",
    "# Interpolate missing values with both forward and backward fill for edge cases\n",
    "data_interpolated = data_df.interpolate(method='linear', limit_direction='both', axis=0)\n",
    "\n",
    "# Convert back to PyTorch tensor\n",
    "data_interpolated_tensor = torch.tensor(data_interpolated.values)\n",
    "\n",
    "print(f\"Number of missing values after interpolation: {torch.isnan(data_interpolated_tensor).sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values after imputation: 0\n",
      "tensor([[ 1.9269e+00,  1.4873e+00,  9.0072e-01, -2.1055e+00, -4.9533e-04,\n",
      "         -1.2345e+00, -4.3067e-02, -1.6047e+00, -7.5214e-01,  1.6487e+00],\n",
      "        [-3.9248e-01, -1.4036e+00, -7.2788e-01, -5.5943e-01, -7.6884e-01,\n",
      "          7.6245e-01,  1.6423e+00, -1.5960e-01, -4.9740e-01,  4.3959e-01],\n",
      "        [-7.5813e-01,  1.0783e+00,  8.0080e-01,  1.6806e+00,  1.2791e+00,\n",
      "          1.2964e+00,  6.1047e-01,  1.3347e+00, -2.3162e-01,  4.1759e-02],\n",
      "        [-2.5158e-01,  8.5986e-01, -1.3847e+00, -8.7124e-01, -2.2337e-01,\n",
      "          1.7174e+00,  3.1888e-01, -4.2452e-01,  3.0572e-01, -7.7459e-01],\n",
      "        [-1.5576e+00,  9.9564e-01, -8.7979e-01,  1.3268e-02, -1.2742e+00,\n",
      "          2.1228e+00,  1.0641e-02, -4.8791e-01, -9.1382e-01,  1.4074e-02]])\n"
     ]
    }
   ],
   "source": [
    "# Imputation with sklearn:\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "data_np = data.numpy()\n",
    "\n",
    "# Initialize the SimpleImputer to fill missing values with the mean of each column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit the imputer on the data and transform it to fill in the missing values\n",
    "data_imputed_np = imputer.fit_transform(data_np)\n",
    "\n",
    "# Convert the imputed NumPy array back to a PyTorch tensor\n",
    "data_imputed_tensor = torch.tensor(data_imputed_np)\n",
    "\n",
    "# Print the number of missing values after imputation\n",
    "print(f\"Number of missing values after imputation: {torch.isnan(data_imputed_tensor).sum().item()}\")\n",
    "\n",
    "# Example: Checking the imputed data\n",
    "print(data_imputed_tensor[:5])  # Print the first 5 rows of the imputed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you build a preprocessing pipeline that integrates transformations and augmentations in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you manage data flow from raw input to a model-ready format in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you create and use custom collate functions in PyTorch to handle variable-length inputs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you manage different data structures in a preprocessing pipeline?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced data loading techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: What strategies can you use to work with large datasets that do not fit in memory in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you implement lazy loading to load data as needed in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How can you speed up data loading by caching preprocessed data in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical examples and use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you prepare image data for classification tasks using CNNs in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you preprocess text data for NLP tasks in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q27: How do you work with multi-modal data, combining image and text data, in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q28: How do you implement custom data transformations in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q29: How do you create and load a custom dataset in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q30: How do you build a data preprocessing pipeline for a specific machine learning task in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q31: How do you optimize data loading for large datasets in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q32: What are some advanced data augmentation techniques you can explore in PyTorch?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
