{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Classification\n",
    "\n",
    "Welcome to `05_mnist_classification` notebook. Here we will explore the techniques and principles necessary for categorizing handwritten digits from the well-known MNIST dataset, a benchmark dataset widely used in the field of machine learning.\n",
    "\n",
    "This piece will walk through the steps of loading and preprocessing the MNIST dataset, constructing a neural network with a softmax output layer, and training the model to accurately classify the digits. It will also delve into the evaluation metrics used to measure the model's performance, such as accuracy and confusion matrices, to provide a clear understanding of the classification results.\n",
    "\n",
    "Additionally, this notebook offers an in-depth analysis of various hyperparameters and their effects on model training and accuracy. Here we experiment with different learning rates, batch sizes, and network architectures to demonstrate how these factors influence the convergence and generalization of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the MNIST dataset\n",
    "\n",
    "The MNIST (Modified National Institute of Standards and Technology) dataset is a large collection of handwritten digits, commonly used for training and testing in the field of machine learning. It serves as a benchmark dataset for evaluating algorithms and models, particularly in the area of image classification. The dataset consists of 70,000 grayscale images of digits, split into 60,000 training images and 10,000 testing images, each of which is 28x28 pixels in size. The pixels are represented as integers in the range of 0 to 255, where 0 corresponds to a white pixel (background) and 255 corresponds to a black pixel (foreground).\n",
    "\n",
    "Some of its key features include:\n",
    "\n",
    "1. **Diversity and simplicity:** The images in the MNIST dataset cover a wide variety of handwriting styles, providing a comprehensive set of examples for each digit (0-9). Despite its simplicity, the dataset contains enough variability in the handwriting to pose a challenging problem for classification models. This variability makes it an excellent testbed for machine learning algorithms, allowing researchers to assess how well their models generalize across different handwriting styles.\n",
    "\n",
    "2. **Standardized format:** Each image in the dataset is normalized and centered in a fixed-size 28x28 pixel grid. This standardization facilitates uniformity, ensuring that the models trained on the dataset can focus on learning the underlying patterns rather than adjusting for size and position variations. The images are also grayscale, which reduces the computational complexity compared to colored images while retaining enough information for accurate classification.\n",
    "\n",
    "3. **Labels and class distribution:** The dataset is accompanied by labels for each image, indicating the correct digit (0-9) represented. This labeled aspect makes the MNIST dataset a supervised learning dataset, where models can be trained using the input images and their corresponding labels. The distribution of digits is approximately uniform, ensuring that each digit is well-represented in both the training and testing sets. This uniform distribution helps in training balanced models without bias toward any particular class.\n",
    "\n",
    "4. **Preprocessing and augmentation:** While the MNIST dataset comes preprocessed, researchers often apply additional preprocessing techniques, such as normalization, to scale pixel values between 0 and 1, and data augmentation to artificially increase the size and variability of the training set. Common augmentation techniques include random rotations, shifts, and scaling, which help models become more robust to variations in the input data.\n",
    "\n",
    "5. **Accessibility and historical context:** The MNIST dataset is widely accessible and has been extensively used since its introduction in 1998 by Yann LeCun and colleagues. It has become a standard benchmark in the field, allowing for the comparison of new algorithms and models against established results. The historical significance of MNIST lies in its role in the development and evaluation of early neural networks and continues to be a relevant dataset for testing modern deep learning architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for working with the MNIST dataset in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install numpy matplotlib  # extra libraries for other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules for MNIST digit classification?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you download the MNIST dataset using PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../00-src\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:01<00:00, 6674226.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../00-src\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../00-src\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../00-src\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 246585.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../00-src\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../00-src\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../00-src\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 2728848.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../00-src\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../00-src\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../00-src\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../00-src\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../00-src\\MNIST\\raw\n",
      "\n",
      "Training dataset succesfully downloaded\n",
      "Testing dataset succesfully downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define transformations: convert to tensor and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "train_dataset = datasets.MNIST(root='../00-src', train=True, download=True, transform=transform)\n",
    "print(\"Training dataset succesfully downloaded\")\n",
    "\n",
    "# Download and load the test data\n",
    "test_dataset = datasets.MNIST(root='../00-src', train=False, download=True, transform=transform)\n",
    "print(\"Testing dataset succesfully downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you normalize the MNIST data for neural network training?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above. i.e.,\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),  # Convert image to tensor\n",
    "#     transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you split the MNIST data into training and testing sets?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already done so in the downloading step. i.e.,\n",
    "# train_dataset = datasets.MNIST(root='../00-src', train=True, download=True, transform=transform)\n",
    "# print(\"Training dataset succesfully downloaded\")\n",
    "\n",
    "# test_dataset = datasets.MNIST(root='../00-src', train=False, download=True, transform=transform)\n",
    "# print(\"Testing dataset succesfully downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you create data loaders for the MNIST dataset in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the training set\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# For the test set\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the neural network model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you define the architecture of a neural network for MNIST digit classification using `nn.Module` in PyTorch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network class\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        # Define the layers\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)  # Input layer (784) to hidden layer (128)\n",
    "        self.fc2 = nn.Linear(128, 64)       # Hidden layer (128) to another hidden layer (64)\n",
    "        self.fc3 = nn.Linear(64, 10)        # Hidden layer (64) to output layer (10 for 10 classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the image to a vector of size 28*28\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        # Apply first fully connected layer with ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Apply second fully connected layer with ReLU activation\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Output layer with logits (raw scores)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the network\n",
    "model = MNISTClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you initialize the weights and biases of the neural network?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing so manually (i.e., directly access the weights and biases of the layers and set them manually)\n",
    "import torch.nn.init as init\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Initialize weights using Xavier uniform distribution\n",
    "        init.xavier_uniform_(self.fc1.weight)  # sets the weights by sampling from a uniform distribution with a specific range, ensuring that the variance of the inputs and outputs is maintained across layers\n",
    "        init.xavier_uniform_(self.fc2.weight)\n",
    "        init.xavier_uniform_(self.fc3.weight)\n",
    "        \n",
    "        # Initialize biases to zero\n",
    "        nn.init.constant_(self.fc1.bias, 0)  # ...as they do not have the same variance issues as weights\n",
    "        nn.init.constant_(self.fc2.bias, 0)\n",
    "        nn.init.constant_(self.fc3.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the network\n",
    "model = MNISTClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another predefined initialization option (aside from Xavier): Kaiming\n",
    "layer = \"e.g.\"\n",
    "\n",
    "kaiming = init.kaiming_uniform_(layer.weight, nonlinearity='relu')  # ensures that the variance of the outputs is maintained across layers, which helps in preventing exploding/vanishing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you choose activation functions for the layers in your neural network?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu: Commonly used for hidden layers to introduce non-linearity\n",
    "relu_layer = nn.ReLU()\n",
    "\n",
    "# Sigmoid: Often used in the output layer for binary classification to get probabilities\n",
    "sigmoid_layer = nn.Sigmoid()\n",
    "\n",
    "# Tanh: Sometimes used in hidden layers to center the data around zero\n",
    "tanh_layer = nn.Tanh()\n",
    "\n",
    "# Leaky ReLU: A variant of ReLU that allows a small, non-zero gradient when the unit is not active\n",
    "leaky_relu_layer = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "# Softmax: Used in the output layer for multi-class classification problems to get a probability distribution over classes\n",
    "softmax_layer = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the loss function and optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you select the appropriate loss function for MNIST digit classification?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you configure an optimizer for training the neural network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural network model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you set up the training loop for the MNIST neural network in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you train the neural network on the MNIST dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you monitor training progress during the training process?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you make predictions using the trained MNIST neural network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you calculate the accuracy of the MNIST neural network model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you visualize the performance of the MNIST neural network model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you create a confusion matrix to evaluate the performance of the MNIST digit classification model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you save the trained MNIST neural network model in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you load a saved MNIST neural network model in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning and optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you perform hyperparameter tuning to improve the performance of the MNIST neural network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: What regularization techniques can you implement to prevent overfitting in the MNIST neural network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you use learning rate scheduling to adjust the learning rate during training?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling model improvements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you apply data augmentation techniques to the MNIST dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you fine-tune the MNIST neural network model for better performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you evaluate the improvements made to the MNIST neural network model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q27: How do you experiment with different neural network architectures for MNIST digit classification?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q28: How do you apply data augmentation techniques to improve model robustness?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q29: How do you test the MNIST neural network model on different digit datasets?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q30: How do you integrate more advanced regularization methods into the MNIST neural network model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q31: How do you deploy the MNIST neural network model for real-time digit recognition?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
