{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention seq2seq in PyTorch\n",
    "\n",
    "The `18_attention_seq2seq` notebook explores the use of attention mechanisms in sequence-to-sequence (seq2seq) models, enhancing the model's ability to focus on relevant parts of the input sequence during translation. \n",
    "\n",
    "The notebook guides through preparing the dataset, building the Encoder model, implementing the attention mechanism, and integrating it into the Decoder model. It then covers training the attention-based seq2seq model, evaluating its performance, visualizing attention weights, and experimenting with hyperparameters for better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding Attention in seq2seq Models](#understanding-attention-in-seq2seq-models)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Preparing the dataset](#preparing-the-dataset)\n",
    "4. [Building the Encoder model](#building-the-encoder-model)\n",
    "5. [Building the Attention mechanism](#building-the-attention-mechanism)\n",
    "6. [Building the Decoder model with Attention](#building-the-decoder-model-with-attention)\n",
    "7. [Combining Encoder and Decoder into an Attention seq2seq model](#combining-encoder-and-decoder-into-an-attention-seq2seq-model)\n",
    "8. [Training the Attention seq2seq model](#training-the-attention-seq2seq-model)\n",
    "9. [Evaluating the Attention seq2seq model](#evaluating-the-attention-seq2seq-model)\n",
    "10. [Visualizing Attention weights](#visualizing-attention-weights)\n",
    "11. [Experimenting with hyperparameters](#experimenting-with-hyperparameters)\n",
    "12. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Attention in seq2seq Models\n",
    "\n",
    "Attention-based sequence-to-sequence (seq2seq) models are an extension of the traditional seq2seq architecture designed to address the limitations of encoding long input sequences into a fixed-length context vector. The primary innovation in these models is the **attention mechanism**, which allows the model to focus on specific parts of the input sequence at each step of the output generation. This significantly improves the model's ability to handle longer sequences and complex relationships between input and output elements.\n",
    "\n",
    "Attention mechanisms have become particularly important in tasks such as **machine translation**, **text summarization**, and **image captioning**, where the relationship between input and output tokens is not always direct or linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Key challenges with traditional seq2seq models**\n",
    "\n",
    "In the traditional seq2seq architecture, the encoder processes the entire input sequence and produces a fixed-length context vector that summarizes the information. The decoder then generates the output sequence based solely on this vector. While this approach works well for short sequences, it faces several problems with longer sequences:\n",
    "- **Information bottleneck**: Compressing all the information from the input sequence into a single context vector often leads to information loss, especially for long and complex inputs.\n",
    "- **Difficulty in long-term dependencies**: The fixed-length context vector struggles to capture long-term dependencies between distant elements in the sequence.\n",
    "\n",
    "### **How attention works in seq2seq models**\n",
    "\n",
    "The attention mechanism resolves these issues by allowing the decoder to focus on different parts of the input sequence dynamically, rather than relying on a single context vector. At each decoding step, the attention mechanism computes a set of attention weights, which determine how much focus should be given to each input token. This gives the model the flexibility to concentrate on the most relevant parts of the input sequence for generating the next output token.\n",
    "\n",
    "In essence, the decoder no longer uses a single, fixed-length context vector. Instead, it generates a new context vector at each time step, which is a weighted sum of the encoder’s hidden states. These weights are learned dynamically based on how relevant each input token is to the current output token being generated.\n",
    "\n",
    "### **Components of the attention mechanism**\n",
    "\n",
    "#### **Alignment scores**\n",
    "The first step in the attention mechanism is to compute **alignment scores** between the current decoder hidden state and each of the encoder’s hidden states. These scores indicate how well the current output token is aligned with each input token. The alignment scores can be computed in different ways, such as:\n",
    "- **Dot product**: Taking the dot product between the decoder’s hidden state and each encoder hidden state.\n",
    "- **Additive (Bahdanau) attention**: A more complex approach that uses a learned feedforward network to compute the alignment scores.\n",
    "- **Scaled dot product (Luong) attention**: A scaled version of the dot product attention to prevent very large values when working with high-dimensional hidden states.\n",
    "\n",
    "#### **Attention weights**\n",
    "Once the alignment scores are computed, they are normalized using a softmax function to produce **attention weights**. These weights represent the importance of each input token relative to the current decoding step. Higher attention weights indicate that the decoder should focus more on the corresponding input token.\n",
    "\n",
    "The attention weights sum to 1 and are used to compute a weighted average of the encoder’s hidden states.\n",
    "\n",
    "#### **Context vector**\n",
    "The context vector at each decoding step is a weighted sum of the encoder’s hidden states, where the weights are the attention scores. This context vector contains information about the most relevant parts of the input sequence for generating the current output token. The context vector is updated at every decoding step, providing the decoder with more focused and relevant information compared to using a single context vector for the entire sequence.\n",
    "\n",
    "The decoder combines this context vector with its own hidden state to generate the next output token.\n",
    "\n",
    "### **Training attention-based seq2seq models**\n",
    "\n",
    "Attention-based seq2seq models are trained in a manner similar to traditional seq2seq models, with the key difference being the introduction of the attention mechanism. The training process minimizes a loss function, such as cross-entropy, to match the predicted output sequence with the target sequence.\n",
    "\n",
    "During training, **teacher forcing** is commonly used, where the true output token from the previous time step is provided to the decoder as input. The model learns to generate accurate translations or outputs by adjusting the attention weights and improving the alignment between the input and output sequences.\n",
    "\n",
    "### **Benefits of attention mechanisms in seq2seq models**\n",
    "\n",
    "The attention mechanism provides several important benefits:\n",
    "- **Improved handling of long sequences**: By allowing the decoder to focus on different parts of the input sequence at each time step, attention mechanisms eliminate the information bottleneck, making it easier for the model to handle long sequences.\n",
    "- **Better alignment**: In tasks like machine translation, attention mechanisms help the model align words or phrases in the input and output sequences more effectively, capturing the relationships between corresponding tokens across languages.\n",
    "- **Interpretability**: The attention weights provide a form of interpretability, as they show which parts of the input sequence the model is focusing on while generating each output token. This can be useful for understanding how the model works and debugging its predictions.\n",
    "\n",
    "### **Applications of attention-based seq2seq models**\n",
    "\n",
    "Attention-based seq2seq models are widely used in tasks where the input and output are sequences and where the alignment between these sequences is important. Some common applications include:\n",
    "- **Machine translation**: In machine translation tasks, attention mechanisms help the model align words in the source language with their translations in the target language, improving the quality of translations, especially for long or complex sentences.\n",
    "- **Text summarization**: Attention mechanisms are used to focus on the most important parts of a document when generating a summary, making the output more concise and relevant.\n",
    "- **Speech recognition**: Attention-based models help align audio signals with their corresponding transcriptions, improving the performance of automatic speech recognition systems.\n",
    "- **Image captioning**: In tasks where images are translated into descriptive sentences, attention mechanisms can help the model focus on specific parts of the image while generating each word of the caption.\n",
    "\n",
    "### **Limitations of attention-based seq2seq models**\n",
    "\n",
    "Despite their success, attention-based seq2seq models have some limitations:\n",
    "- **Computation cost**: The attention mechanism introduces additional computational overhead, as it requires calculating alignment scores and attention weights for each input token at every decoding step.\n",
    "- **Long training times**: Due to the added complexity, attention-based models can take longer to train compared to vanilla seq2seq models, especially on large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Maths**\n",
    "\n",
    "#### **Encoder**\n",
    "\n",
    "In the attention-based seq2seq model, the encoder processes the input sequence $ X = (x_1, x_2, \\dots, x_T) $, where $ T $ is the length of the input sequence. Each input token $ x_t $ is passed through a recurrent neural network (RNN), such as an LSTM or GRU, to produce a sequence of hidden states $ h_t $:\n",
    "\n",
    "$$\n",
    "h_t = f(W_{hx} x_t + W_{hh} h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ h_t $ is the hidden state at time step $ t $,\n",
    "- $ W_{hx} $ is the weight matrix for the input $ x_t $,\n",
    "- $ W_{hh} $ is the weight matrix for the hidden state $ h_{t-1} $,\n",
    "- $ b_h $ is the bias,\n",
    "- $ f $ is the non-linear activation function (e.g., tanh or ReLU).\n",
    "\n",
    "The encoder generates hidden states for every input token, resulting in $ H = (h_1, h_2, \\dots, h_T) $, a sequence of hidden states that will be used in the attention mechanism.\n",
    "\n",
    "#### **Decoder**\n",
    "\n",
    "The decoder in the attention-based seq2seq model takes the context vector (a dynamic combination of encoder hidden states) and the previous decoder hidden state to generate the output sequence. The decoder’s hidden state $ s_t $ at each time step is computed using the previous output token $ y_{t-1} $, the context vector $ c_t $, and the previous hidden state $ s_{t-1} $:\n",
    "\n",
    "$$\n",
    "s_t = f(W_{sy} y_{t-1} + W_{sc} c_t + W_{ss} s_{t-1} + b_s)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ s_t $ is the hidden state of the decoder at time step $ t $,\n",
    "- $ y_{t-1} $ is the previous output token,\n",
    "- $ c_t $ is the context vector at time step $ t $,\n",
    "- $ W_{sy}, W_{sc}, W_{ss} $ are the weight matrices for the previous output token, the context vector, and the previous hidden state, respectively,\n",
    "- $ b_s $ is the bias term.\n",
    "\n",
    "The output at each time step is generated using the hidden state $ s_t $, typically passed through a softmax function to generate probabilities over the target vocabulary.\n",
    "\n",
    "#### **Attention mechanism**\n",
    "\n",
    "The core of the attention mechanism lies in generating the context vector $ c_t $ at each decoding step. Instead of using a single fixed-length context vector (as in traditional seq2seq models), the attention mechanism dynamically computes a weighted sum of all encoder hidden states $ h_1, h_2, \\dots, h_T $ at each time step of the decoder.\n",
    "\n",
    "##### **Step 1: Alignment scores**\n",
    "\n",
    "The attention mechanism computes an **alignment score** $ e_{t,i} $ between the current decoder hidden state $ s_t $ and each encoder hidden state $ h_i $. The alignment score represents the relevance of encoder hidden state $ h_i $ to the current decoding step $ t $. The alignment score can be computed in various ways, such as:\n",
    "\n",
    "- **Dot-product**: The dot product between $ s_t $ and $ h_i $:\n",
    "\n",
    "  $$\n",
    "  e_{t,i} = s_t^T h_i\n",
    "  $$\n",
    "\n",
    "- **Additive attention (Bahdanau)**: A learned feedforward network computes the alignment score:\n",
    "\n",
    "  $$\n",
    "  e_{t,i} = v_a^T \\tanh(W_s s_t + W_h h_i)\n",
    "  $$\n",
    "\n",
    "- **Scaled dot-product attention (Luong)**: A scaled version of the dot product:\n",
    "\n",
    "  $$\n",
    "  e_{t,i} = \\frac{s_t^T h_i}{\\sqrt{d_h}}\n",
    "  $$\n",
    "\n",
    "  Where $ d_h $ is the dimensionality of the hidden states, and the scaling factor prevents large dot-product values in high-dimensional spaces.\n",
    "\n",
    "##### **Step 2: Attention weights**\n",
    "\n",
    "The alignment scores $ e_{t,i} $ are then normalized using a softmax function to produce **attention weights** $ \\alpha_{t,i} $:\n",
    "\n",
    "$$\n",
    "\\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_{j=1}^{T} \\exp(e_{t,j})}\n",
    "$$\n",
    "\n",
    "These attention weights represent the importance of each encoder hidden state $ h_i $ for generating the next output token at time step $ t $. The weights $ \\alpha_{t,i} $ sum to 1.\n",
    "\n",
    "##### **Step 3: Context vector**\n",
    "\n",
    "The **context vector** $ c_t $ is computed as the weighted sum of the encoder hidden states $ h_i $, where the weights are the attention scores $ \\alpha_{t,i} $:\n",
    "\n",
    "$$\n",
    "c_t = \\sum_{i=1}^{T} \\alpha_{t,i} h_i\n",
    "$$\n",
    "\n",
    "The context vector $ c_t $ is updated dynamically at each time step $ t $, allowing the decoder to focus on different parts of the input sequence as it generates the output sequence.\n",
    "\n",
    "#### **Combining context vector with decoder**\n",
    "\n",
    "The context vector $ c_t $ is combined with the decoder’s hidden state $ s_t $ to generate the next output token. The final output at each time step is usually produced by passing the combination of the context vector and the decoder hidden state through a fully connected layer followed by a softmax function:\n",
    "\n",
    "$$\n",
    "\\hat{y_t} = \\text{softmax}(W_o [s_t; c_t] + b_o)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W_o $ is the output weight matrix,\n",
    "- $ b_o $ is the output bias,\n",
    "- $ [s_t; c_t] $ denotes the concatenation of the decoder hidden state $ s_t $ and the context vector $ c_t $.\n",
    "\n",
    "The softmax function produces a probability distribution over the target vocabulary, allowing the model to predict the next token in the sequence.\n",
    "\n",
    "#### **Loss function**\n",
    "\n",
    "The model is trained by minimizing the **cross-entropy loss** between the predicted output sequence $ \\hat{Y} $ and the true target sequence $ Y $:\n",
    "\n",
    "$$\n",
    "L = - \\sum_{t=1}^{T'} \\sum_{k=1}^{V} y_{t,k} \\log(\\hat{y_{t,k}})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ T' $ is the length of the output sequence,\n",
    "- $ V $ is the size of the target vocabulary,\n",
    "- $ y_{t,k} $ is the true one-hot encoded value for the $ k $-th word at time step $ t $,\n",
    "- $ \\hat{y_{t,k}} $ is the predicted probability of the $ k $-th word at time step $ t $.\n",
    "\n",
    "This loss is minimized using gradient descent, with the gradients flowing through the entire attention mechanism, updating both the encoder and decoder parameters.\n",
    "\n",
    "#### **Backpropagation through time (BPTT)**\n",
    "\n",
    "Training attention-based seq2seq models involves backpropagation through time (BPTT). The gradients of the loss with respect to the attention weights, context vectors, and hidden states are computed, allowing the model to learn how to align the input and output sequences.\n",
    "\n",
    "Since the attention weights and context vectors are computed at each decoding step, the gradients flow through both the encoder and decoder at every time step, ensuring that the entire model is updated based on the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for building and training an attention-based seq2seq model in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules for data loading, model building, and training in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you set up the environment to use a GPU for training the attention-based seq2seq model in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you set random seeds in PyTorch to ensure reproducibility when training the attention-based seq2seq model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you load a machine translation dataset (e.g., English to French) using PyTorch’s `torchtext.datasets`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you tokenize the dataset and convert sentences into sequences of indices for machine translation tasks?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you create vocabulary mappings for both source and target languages using PyTorch’s `Field` class?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you set up DataLoaders to handle batching of source-target sentence pairs for training the model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Encoder model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you define the architecture of the Encoder model using PyTorch’s `nn.Module`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you implement the forward pass of the Encoder to generate a sequence of hidden states instead of a single context vector?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you specify the number of hidden units and layers in the Encoder, and how does this affect performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Attention mechanism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you implement the attention mechanism to calculate attention scores between encoder hidden states and the decoder's current hidden state?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you define the attention scoring function (e.g., dot product, additive) to compute the relevance of each input token during decoding?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you apply the attention weights to compute a context vector for each decoding step?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Decoder model with Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you modify the Decoder model to include the attention mechanism in its architecture?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you implement the forward pass of the Decoder with attention, using the context vector and hidden state to generate each output token?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you use `nn.Linear` and `nn.Softmax` layers in the Decoder to convert the attention-weighted hidden state into predicted tokens?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Encoder and Decoder into an Attention seq2seq model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you combine the Encoder and Decoder models into a complete seq2seq model with attention?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you implement teacher forcing in the training loop to improve the performance of the attention-based seq2seq model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you implement the forward pass of the complete attention-based seq2seq model, using the context vector and attention weights at each decoding step?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Attention seq2seq model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you define the loss function (e.g., CrossEntropyLoss) to measure the difference between the predicted and actual target sequences?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you configure the optimizer (e.g., Adam) to update the parameters of both the Encoder and Decoder models during training?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you implement the training loop, including forward pass, loss calculation, backpropagation, and logging of the training loss?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you monitor and log the loss during training to ensure the model is converging and learning effectively?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Attention seq2seq model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you evaluate the attention-based seq2seq model on a validation set using metrics such as BLEU score?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you calculate the BLEU score to assess the quality of the translations produced by the model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q27: How do you compare the performance of the attention-based seq2seq model with a vanilla seq2seq model without attention?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Attention weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q28: How do you visualize the attention weights for specific input-output pairs using a heatmap?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q29: How do you interpret the attention heatmap to understand which parts of the input sequence the model focused on during translation?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q30: How do you extract the attention weights from the Decoder to analyze how the model's attention changes across decoding steps?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q31: How do you adjust the learning rate and observe its effect on the stability and performance of the attention-based seq2seq model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q32: How do you experiment with different hidden dimensions and evaluate how they impact the performance of the model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q33: How do you tune the teacher forcing ratio during training and analyze how it affects the model’s convergence?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q34: How do you experiment with different attention scoring functions (e.g., dot product vs. additive) and observe their impact on model performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
