{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence (seq2seq) models and machine translation\n",
    "\n",
    "The `17_seq2seq_translation` notebook focuses on sequence-to-sequence (seq2seq) models for machine translation, a key application of neural networks in natural language processing. It covers preparing a dataset for translation tasks, building both the Encoder and Decoder models, and combining them into a complete seq2seq architecture. \n",
    "\n",
    "The notebook further explores training the model, evaluating its performance, translating new sentences, and experimenting with hyperparameters to fine-tune the model’s accuracy and fluency in translation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding seq2seq models and machine translation](#understanding-seq2seq-models-and-machine-translation)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Preparing the dataset for machine translation](#preparing-the-dataset-for-machine-translation)\n",
    "4. [Building the Encoder model](#building-the-encoder-model)\n",
    "5. [Building the Decoder model](#building-the-decoder-model)\n",
    "6. [Combining Encoder and Decoder into a seq2seq model](#combining-encoder-and-decoder-into-a-seq2seq-model)\n",
    "7. [Training the seq2seq model](#training-the-seq2seq-model)\n",
    "8. [Evaluating the seq2seq model](#evaluating-the-seq2seq-model)\n",
    "9. [Translating new sentences](#translating-new-sentences)\n",
    "10. [Experimenting with hyperparameters](#experimenting-with-hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding seq2seq models and machine translation\n",
    "\n",
    "Sequence-to-sequence (seq2seq) models are a class of neural networks designed to transform one sequence into another, making them particularly effective for tasks where input and output are sequences of varying lengths. These models are widely used in tasks like **machine translation**, where the input is a sentence in one language and the output is the translation in another language. Other applications include text summarization, speech recognition, and image captioning.\n",
    "\n",
    "The seq2seq model is based on a **recurrent neural network (RNN)** architecture and is typically composed of two main parts: an **encoder** and a **decoder**. The encoder processes the input sequence and compresses it into a fixed-size context vector, and the decoder takes this context vector to generate the output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Key concepts**\n",
    "Sequence-to-sequence (seq2seq) models are a class of neural networks designed to map input sequences to output sequences of variable lengths, making them ideal for tasks like machine translation, text summarization, and conversational AI. These models typically consist of two main components: an **encoder** and a **decoder**.\n",
    "\n",
    "- **Encoder**: Processes the input sequence and encodes it into a fixed-length context vector that captures the semantic information.\n",
    "- **Context Vector**: Acts as a bridge between the encoder and decoder, summarizing the input sequence.\n",
    "- **Decoder**: Generates the output sequence one token at a time, conditioned on the context vector and its own previous outputs.\n",
    "\n",
    "Training seq2seq models involves teacher forcing, where the true output tokens are fed into the decoder during training to improve convergence. These models often use Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, or Gated Recurrent Units (GRUs) as their core architecture.\n",
    "\n",
    "### **Applications**\n",
    "Seq2seq models are widely used in sequence transformation tasks, such as:\n",
    "- **Machine translation**: Translating text from one language to another (e.g., English to French).\n",
    "- **Text summarization**: Condensing long documents into shorter summaries.\n",
    "- **Conversational AI**: Powering chatbots and virtual assistants for natural dialogue generation.\n",
    "- **Speech recognition**: Converting audio signals into text sequences.\n",
    "- **Code generation**: Translating natural language descriptions into code snippets.\n",
    "\n",
    "### **Advantages**\n",
    "- **Variable-length mapping**: Handles sequences of differing lengths for input and output.\n",
    "- **General-purpose framework**: Adaptable to a wide range of sequence transformation tasks.\n",
    "- **Sequential dependency capture**: Models relationships across input and output tokens effectively.\n",
    "- **Scalability**: Can be extended with attention mechanisms or transformer architectures for improved performance.\n",
    "\n",
    "### **Challenges**\n",
    "- **Information bottleneck**: Compressing an entire input sequence into a single context vector can lead to loss of information, especially for long sequences.\n",
    "- **Long-term dependency modeling**: Traditional RNN-based seq2seq models struggle with capturing dependencies in lengthy sequences.\n",
    "- **Data requirements**: Requires large and high-quality parallel datasets for effective training.\n",
    "- **Training instability**: Susceptible to issues like vanishing gradients, especially with deeper architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for building and training seq2seq models in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install numpy matplotlib scikit-learn pandas\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules for model building, training, and data loading in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you set up the environment to use a GPU for training seq2seq models, and how do you fallback to CPU in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you set random seeds in PyTorch to ensure reproducibility when training seq2seq models?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset for machine translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you load a machine translation dataset to use in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/fellmir/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "url_train_en = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.en.gz'\n",
    "url_train_de = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.de.gz'\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "def download_and_extract(url, filename):\n",
    "    filepath = os.path.join('data', filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        urllib.request.urlretrieve(url, filepath + '.gz')\n",
    "        with gzip.open(filepath + '.gz', 'rb') as f_in:\n",
    "            with open(filepath, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        os.remove(filepath + '.gz')\n",
    "\n",
    "download_and_extract(url_train_en, 'train.en')\n",
    "download_and_extract(url_train_de, 'train.de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you preprocess the dataset by tokenizing the sentences and converting them into sequences of indices?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the tokenizer functions:\n",
    "tokenizer_src = simple_tokenizer  # For English\n",
    "tokenizer_trg = simple_tokenizer  # For German\n",
    "\n",
    "with open('data/train.en', 'r', encoding='utf-8') as f:\n",
    "    sentences_en = f.readlines()\n",
    "\n",
    "with open('data/train.de', 'r', encoding='utf-8') as f:\n",
    "    sentences_de = f.readlines()\n",
    "\n",
    "assert len(sentences_en) == len(sentences_de)  # Ensure both files have the same number of sentences\n",
    "\n",
    "tokenized_en = [tokenizer_src(sentence) for sentence in sentences_en]\n",
    "tokenized_de = [tokenizer_trg(sentence) for sentence in sentences_de]  # Tokenize the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative using NLTK's wordpunct_tokenize:\n",
    "# from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "# tokenizer_src = wordpunct_tokenize  # For English\n",
    "# tokenizer_trg = wordpunct_tokenize  # For German\n",
    "\n",
    "# Tokenize the sentences\n",
    "# tokenized_en = [tokenizer_src(sentence.lower()) for sentence in sentences_en]\n",
    "# tokenized_de = [tokenizer_trg(sentence.lower()) for sentence in sentences_de]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you build vocabulary for both the source and target languages?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(tokenized_sentences, min_freq):\n",
    "    counter = Counter()\n",
    "    for tokens in tokenized_sentences:\n",
    "        counter.update(tokens)\n",
    "    vocab = {'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3}\n",
    "    idx = 4\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    return vocab\n",
    "\n",
    "MIN_FREQ = 2\n",
    "vocab_src = build_vocab(tokenized_en, MIN_FREQ)\n",
    "vocab_trg = build_vocab(tokenized_de, MIN_FREQ)\n",
    "\n",
    "inv_vocab_src = {idx: word for word, idx in vocab_src.items()}\n",
    "inv_vocab_trg = {idx: word for word, idx in vocab_trg.items()}  # Inverse vocabularies for decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you create DataLoaders for batching the source-target sentence pairs during training?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, tokenized_src, tokenized_trg, vocab_src, vocab_trg):\n",
    "        self.data = []\n",
    "        for src_tokens, trg_tokens in zip(tokenized_src, tokenized_trg):\n",
    "            src_indices = [vocab_src.get('<bos>')] + [vocab_src.get(token, vocab_src['<unk>']) for token in src_tokens] + [vocab_src.get('<eos>')]\n",
    "            trg_indices = [vocab_trg.get('<bos>')] + [vocab_trg.get(token, vocab_trg['<unk>']) for token in trg_tokens] + [vocab_trg.get('<eos>')]\n",
    "            self.data.append((torch.tensor(src_indices), torch.tensor(trg_indices)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_padded = nn.utils.rnn.pad_sequence(src_batch, padding_value=vocab_src['<pad>'], batch_first=True)\n",
    "    trg_padded = nn.utils.rnn.pad_sequence(trg_batch, padding_value=vocab_trg['<pad>'], batch_first=True)\n",
    "    return src_padded, trg_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_src, valid_src, train_trg, valid_trg = train_test_split(tokenized_en, tokenized_de, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = TranslationDataset(train_src, train_trg, vocab_src, vocab_trg)\n",
    "valid_dataset = TranslationDataset(valid_src, valid_trg, vocab_src, vocab_trg)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Encoder model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you define the architecture of the Encoder model using PyTorch’s `nn.Module`?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you implement the forward pass of the Encoder to process input sequences and generate the context vector?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The forward pass of the Encoder processes input sequences and generates the context vector (hidden state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you specify the number of layers and hidden units in the Encoder?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_src)\n",
    "ENC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "ENC_N_LAYERS = 2\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_N_LAYERS).to(device)  # Initialize the Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Decoder model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you define the Decoder architecture using PyTorch’s `nn.Module`?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you implement the forward pass of the Decoder to generate translated sequences from the context vector?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The forward pass of the Decoder generates translated sequences from the context vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you use the `nn.Linear` and `nn.Softmax` layers to convert the Decoder's output into predicted tokens?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The nn.Linear and nn.Softmax layers convert the Decoder's output into predicted tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Encoder and Decoder into a seq2seq model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you combine the Encoder and Decoder models into a complete seq2seq model for machine translation?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.embedding.num_embeddings\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        hidden = self.encoder(src)\n",
    "        \n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = len(vocab_trg)\n",
    "DEC_EMB_DIM = 256\n",
    "DEC_N_LAYERS = 2\n",
    "\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_N_LAYERS).to(device)\n",
    "seq2seq_model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you implement teacher forcing in the training loop to improve the Decoder’s performance during training?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher forcing is implemented in the Seq2Seq model's forward method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you implement the forward pass for the combined seq2seq model, using the context vector from the Encoder to initialize the Decoder?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The forward pass for the combined seq2seq model uses the context vector from the Encoder to initialize the Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the seq2seq model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you define the loss function for training the seq2seq model on sequence data?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = vocab_trg['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)  # Define the loss function with padding index ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you configure an optimizer to update the parameters of both the Encoder and Decoder models during training?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(seq2seq_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you implement the training loop for the seq2seq model, including the forward pass, loss calculation, and backpropagation?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Loss: 0.7457\n",
      "Epoch 20/30, Loss: 0.6495\n",
      "Epoch 30/30, Loss: 0.6860\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    seq2seq_model.train()\n",
    "    epoch_loss = 0\n",
    "    for src_batch, trg_batch in train_loader:\n",
    "        src_batch = src_batch.to(device)\n",
    "        trg_batch = trg_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = seq2seq_model(src_batch, trg_batch)\n",
    "        \n",
    "        # output: (batch_size, trg_len, trg_vocab_size)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "        trg = trg_batch[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you monitor and log the training loss over epochs to ensure the seq2seq model is learning effectively?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss is monitored and logged in the training loop above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the seq2seq model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you evaluate the seq2seq model on a validation dataset using metrics such as the BLEU score?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src_batch, trg_batch in data_loader:\n",
    "            src_batch = src_batch.to(device)\n",
    "            trg_batch = trg_batch.to(device)\n",
    "            output = model(src_batch, trg_batch, teacher_forcing_ratio=0)\n",
    "            output_tokens = output.argmax(2)\n",
    "            for i in range(src_batch.size(0)):\n",
    "                trg_indices = trg_batch[i].cpu().numpy()\n",
    "                output_indices = output_tokens[i].cpu().numpy()\n",
    "                trg_tokens = [inv_vocab_trg[idx] for idx in trg_indices if idx != PAD_IDX and idx != vocab_trg['<bos>']]\n",
    "                output_tokens_list = [inv_vocab_trg[idx] for idx in output_indices if idx != PAD_IDX and idx != vocab_trg['<bos>']]\n",
    "                references.append([trg_tokens])\n",
    "                hypotheses.append(output_tokens_list)\n",
    "    bleu = corpus_bleu(references, hypotheses)\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU score: 7.99\n"
     ]
    }
   ],
   "source": [
    "bleu_score = evaluate(seq2seq_model, valid_loader)\n",
    "print(f'Validation BLEU score: {bleu_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you implement a function to calculate the BLEU score to assess the quality of the machine-translated sequences?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"evaluate\" function calculates the BLEU score to assess translation quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you compare the model's predictions to the target translations during evaluation to measure performance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model's predictions are compared to the target translations during evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating new sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you implement a function to translate new sentences using the trained seq2seq model?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, vocab_src, vocab_trg, model, tokenizer_src, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = ['<bos>'] + tokenizer_src(sentence.lower()) + ['<eos>']\n",
    "    src_indices = [vocab_src.get(token, vocab_src['<unk>']) for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden = model.encoder(src_tensor)\n",
    "    \n",
    "    input_token = torch.LongTensor([vocab_trg['<bos>']]).to(device)\n",
    "    outputs = []\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        output, hidden = model.decoder(input_token, hidden)\n",
    "        top1 = output.argmax(1)\n",
    "        outputs.append(top1.item())\n",
    "        if top1.item() == vocab_trg['<eos>']:\n",
    "            break\n",
    "        input_token = top1\n",
    "    translated_tokens = [inv_vocab_trg.get(idx, '<unk>') for idx in outputs]\n",
    "    return translated_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you handle sentences of varying lengths when translating new sentences with the seq2seq model?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The translate_sentence function handles sentences of varying lengths using a max_len parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q27: How do you visualize the original, translated, and reference (ground truth) sentences to evaluate the model’s translation performance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_translation(sentence):\n",
    "    print(f'Original: {sentence}')\n",
    "    translation = translate_sentence(sentence, vocab_src, vocab_trg, seq2seq_model, tokenizer_src)\n",
    "    print(f'Translated: {\" \".join(translation)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: A man is playing a guitar.\n",
      "Translated: ein mann spielt gitarre einer trommel <eos>\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"A man is playing a guitar.\"\n",
    "display_translation(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: the book is on the table\n",
      "Translated: die am tisch beim essen <eos>\n"
     ]
    }
   ],
   "source": [
    "display_translation('the book is on the table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: words\n",
      "Translated: straßenkünstler <unk> die arbeit <eos>\n"
     ]
    }
   ],
   "source": [
    "display_translation('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q28: How do you adjust the learning rate and observe its effect on the seq2seq model’s training stability and performance?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.0005, 0.0001]\n",
    "\n",
    "def train_with_learning_rate(lr):\n",
    "    print(f\"\\nTraining with learning rate: {lr}\")\n",
    "    # Re-initialize the model:\n",
    "    encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_N_LAYERS).to(device)\n",
    "    decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_N_LAYERS).to(device)\n",
    "    seq2seq_model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(seq2seq_model.parameters(), lr=lr)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        seq2seq_model.train()\n",
    "        epoch_loss = 0\n",
    "        for src_batch, trg_batch in train_loader:\n",
    "            src_batch = src_batch.to(device)\n",
    "            trg_batch = trg_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = seq2seq_model(src_batch, trg_batch)\n",
    "            \n",
    "            # output: (batch_size, trg_len, trg_vocab_size)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "            trg = trg_batch[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    bleu_score = evaluate(seq2seq_model, valid_loader)\n",
    "    print(f'Validation BLEU score: {bleu_score*100:.2f}')\n",
    "    return avg_loss, bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with learning rate: 0.001\n",
      "Epoch 1/10, Loss: 4.6847\n",
      "Epoch 2/10, Loss: 3.6639\n",
      "Epoch 3/10, Loss: 3.1361\n",
      "Epoch 4/10, Loss: 2.7249\n",
      "Epoch 5/10, Loss: 2.3774\n",
      "Epoch 6/10, Loss: 2.0795\n",
      "Epoch 7/10, Loss: 1.8155\n",
      "Epoch 8/10, Loss: 1.6143\n",
      "Epoch 9/10, Loss: 1.4452\n",
      "Epoch 10/10, Loss: 1.2949\n",
      "Validation BLEU score: 9.00\n",
      "\n",
      "Training with learning rate: 0.0005\n",
      "Epoch 1/10, Loss: 4.8586\n",
      "Epoch 2/10, Loss: 3.7968\n",
      "Epoch 3/10, Loss: 3.2709\n",
      "Epoch 4/10, Loss: 2.8791\n",
      "Epoch 5/10, Loss: 2.5498\n",
      "Epoch 6/10, Loss: 2.2679\n",
      "Epoch 7/10, Loss: 1.9957\n",
      "Epoch 8/10, Loss: 1.7537\n",
      "Epoch 9/10, Loss: 1.5253\n",
      "Epoch 10/10, Loss: 1.3123\n",
      "Validation BLEU score: 10.03\n",
      "\n",
      "Training with learning rate: 0.0001\n",
      "Epoch 1/10, Loss: 5.4526\n",
      "Epoch 2/10, Loss: 4.8022\n",
      "Epoch 3/10, Loss: 4.4589\n",
      "Epoch 4/10, Loss: 4.1844\n",
      "Epoch 5/10, Loss: 3.9739\n",
      "Epoch 6/10, Loss: 3.7985\n",
      "Epoch 7/10, Loss: 3.6333\n",
      "Epoch 8/10, Loss: 3.4942\n",
      "Epoch 9/10, Loss: 3.3708\n",
      "Epoch 10/10, Loss: 3.2538\n",
      "Validation BLEU score: 6.85\n",
      "\n",
      "Learning Rate Experiment Results:\n",
      "Learning Rate: 0.001, Final Loss: 1.2949, BLEU Score: 9.00\n",
      "Learning Rate: 0.0005, Final Loss: 1.3123, BLEU Score: 10.03\n",
      "Learning Rate: 0.0001, Final Loss: 3.2538, BLEU Score: 6.85\n"
     ]
    }
   ],
   "source": [
    "results_lr = []\n",
    "for lr in learning_rates:\n",
    "    avg_loss, bleu = train_with_learning_rate(lr)\n",
    "    results_lr.append({'learning_rate': lr, 'loss': avg_loss, 'bleu_score': bleu})\n",
    "\n",
    "print(\"\\nLearning Rate Experiment Results:\")\n",
    "for res in results_lr:\n",
    "    print(f\"Learning Rate: {res['learning_rate']}, Final Loss: {res['loss']:.4f}, BLEU Score: {res['bleu_score']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q29: How do you experiment with different batch sizes to observe how they impact training speed and memory usage?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [16, 32, 64]\n",
    "\n",
    "def train_with_batch_size(batch_size):\n",
    "    print(f\"\\nTraining with batch size: {batch_size}\")\n",
    "    # Re-create DataLoaders with specified batch size:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_N_LAYERS).to(device)\n",
    "    decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_N_LAYERS).to(device)\n",
    "    seq2seq_model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(seq2seq_model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        seq2seq_model.train()\n",
    "        epoch_loss = 0\n",
    "        for src_batch, trg_batch in train_loader:\n",
    "            src_batch = src_batch.to(device)\n",
    "            trg_batch = trg_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = seq2seq_model(src_batch, trg_batch)\n",
    "            \n",
    "            # output: (batch_size, trg_len, trg_vocab_size)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "            trg = trg_batch[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    bleu_score = evaluate(seq2seq_model, valid_loader)\n",
    "    print(f'Validation BLEU score: {bleu_score*100:.2f}')\n",
    "    print(f\"Training time: {total_time:.2f} seconds\")\n",
    "    return avg_loss, bleu_score, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with batch size: 16\n",
      "Epoch 1/10, Loss: 4.4521\n",
      "Epoch 2/10, Loss: 3.3737\n",
      "Epoch 3/10, Loss: 2.8145\n",
      "Epoch 4/10, Loss: 2.3840\n",
      "Epoch 5/10, Loss: 2.0419\n",
      "Epoch 6/10, Loss: 1.8051\n",
      "Epoch 7/10, Loss: 1.6383\n",
      "Epoch 8/10, Loss: 1.5304\n",
      "Epoch 9/10, Loss: 1.4372\n",
      "Epoch 10/10, Loss: 1.3601\n",
      "Validation BLEU score: 9.88\n",
      "Training time: 829.96 seconds\n",
      "\n",
      "Training with batch size: 32\n",
      "Epoch 1/10, Loss: 4.6968\n",
      "Epoch 2/10, Loss: 3.6638\n",
      "Epoch 3/10, Loss: 3.1340\n",
      "Epoch 4/10, Loss: 2.7402\n",
      "Epoch 5/10, Loss: 2.3981\n",
      "Epoch 6/10, Loss: 2.0969\n",
      "Epoch 7/10, Loss: 1.8206\n",
      "Epoch 8/10, Loss: 1.6418\n",
      "Epoch 9/10, Loss: 1.4646\n",
      "Epoch 10/10, Loss: 1.3230\n",
      "Validation BLEU score: 8.94\n",
      "Training time: 537.01 seconds\n",
      "\n",
      "Training with batch size: 64\n",
      "Epoch 1/10, Loss: 5.1268\n",
      "Epoch 2/10, Loss: 4.1523\n",
      "Epoch 3/10, Loss: 3.6510\n",
      "Epoch 4/10, Loss: 3.2843\n",
      "Epoch 5/10, Loss: 2.9597\n",
      "Epoch 6/10, Loss: 2.7052\n",
      "Epoch 7/10, Loss: 2.4615\n",
      "Epoch 8/10, Loss: 2.2732\n",
      "Epoch 9/10, Loss: 2.0692\n",
      "Epoch 10/10, Loss: 1.8595\n",
      "Validation BLEU score: 7.67\n",
      "Training time: 400.06 seconds\n",
      "\n",
      "Batch Size Experiment Results:\n",
      "Batch Size: 16, Final Loss: 1.3601, BLEU Score: 9.88, Training Time: 829.96 seconds\n",
      "Batch Size: 32, Final Loss: 1.3230, BLEU Score: 8.94, Training Time: 537.01 seconds\n",
      "Batch Size: 64, Final Loss: 1.8595, BLEU Score: 7.67, Training Time: 400.06 seconds\n"
     ]
    }
   ],
   "source": [
    "results_bs = []\n",
    "for batch_size in batch_sizes:\n",
    "    avg_loss, bleu, total_time = train_with_batch_size(batch_size)\n",
    "    results_bs.append({'batch_size': batch_size, 'loss': avg_loss, 'bleu_score': bleu, 'training_time': total_time})\n",
    "\n",
    "print(\"\\nBatch Size Experiment Results:\")\n",
    "for res in results_bs:\n",
    "    print(f\"Batch Size: {res['batch_size']}, Final Loss: {res['loss']:.4f}, BLEU Score: {res['bleu_score']*100:.2f}, Training Time: {res['training_time']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q30: How do you modify the number of training epochs and analyze how it affects the model’s convergence and translation accuracy?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_numbers = [5, 15, 20]\n",
    "\n",
    "def train_with_epochs(num_epochs):\n",
    "    print(f\"\\nTraining with number of epochs: {num_epochs}\")\n",
    "    encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_N_LAYERS).to(device)\n",
    "    decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_N_LAYERS).to(device)\n",
    "    seq2seq_model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(seq2seq_model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        seq2seq_model.train()\n",
    "        epoch_loss = 0\n",
    "        for src_batch, trg_batch in train_loader:\n",
    "            src_batch = src_batch.to(device)\n",
    "            trg_batch = trg_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = seq2seq_model(src_batch, trg_batch)\n",
    "            \n",
    "            # output: (batch_size, trg_len, trg_vocab_size)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "            trg = trg_batch[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    bleu_score = evaluate(seq2seq_model, valid_loader)\n",
    "    print(f'Validation BLEU score after {num_epochs} epochs: {bleu_score*100:.2f}')\n",
    "    return avg_loss, bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with number of epochs: 5\n",
      "Epoch 1/5, Loss: 4.6984\n",
      "Epoch 2/5, Loss: 3.6317\n",
      "Epoch 3/5, Loss: 3.0833\n",
      "Epoch 4/5, Loss: 2.6554\n",
      "Epoch 5/5, Loss: 2.2912\n",
      "Validation BLEU score after 5 epochs: 9.15\n",
      "\n",
      "Training with number of epochs: 15\n",
      "Epoch 1/15, Loss: 4.7550\n",
      "Epoch 2/15, Loss: 3.7759\n",
      "Epoch 3/15, Loss: 3.2685\n",
      "Epoch 4/15, Loss: 2.8572\n",
      "Epoch 5/15, Loss: 2.5082\n",
      "Epoch 6/15, Loss: 2.1977\n",
      "Epoch 7/15, Loss: 1.9555\n",
      "Epoch 8/15, Loss: 1.7371\n",
      "Epoch 9/15, Loss: 1.5629\n",
      "Epoch 10/15, Loss: 1.4165\n",
      "Epoch 11/15, Loss: 1.2908\n",
      "Epoch 12/15, Loss: 1.1703\n",
      "Epoch 13/15, Loss: 1.0776\n",
      "Epoch 14/15, Loss: 1.0027\n",
      "Epoch 15/15, Loss: 0.9374\n",
      "Validation BLEU score after 15 epochs: 8.78\n",
      "\n",
      "Training with number of epochs: 20\n",
      "Epoch 1/20, Loss: 4.7713\n",
      "Epoch 2/20, Loss: 3.7686\n",
      "Epoch 3/20, Loss: 3.2649\n",
      "Epoch 4/20, Loss: 2.8748\n",
      "Epoch 5/20, Loss: 2.5597\n",
      "Epoch 6/20, Loss: 2.2812\n",
      "Epoch 7/20, Loss: 2.0288\n",
      "Epoch 8/20, Loss: 1.8197\n",
      "Epoch 9/20, Loss: 1.6428\n",
      "Epoch 10/20, Loss: 1.5024\n",
      "Epoch 11/20, Loss: 1.3940\n",
      "Epoch 12/20, Loss: 1.2498\n",
      "Epoch 13/20, Loss: 1.1637\n",
      "Epoch 14/20, Loss: 1.0731\n",
      "Epoch 15/20, Loss: 1.0077\n",
      "Epoch 16/20, Loss: 0.9365\n",
      "Epoch 17/20, Loss: 0.9030\n",
      "Epoch 18/20, Loss: 0.8329\n",
      "Epoch 19/20, Loss: 0.8156\n",
      "Epoch 20/20, Loss: 0.7812\n",
      "Validation BLEU score after 20 epochs: 8.13\n",
      "\n",
      "Epoch Number Experiment Results:\n",
      "Number of Epochs: 5, Final Loss: 2.2912, BLEU Score: 9.15\n",
      "Number of Epochs: 15, Final Loss: 0.9374, BLEU Score: 8.78\n",
      "Number of Epochs: 20, Final Loss: 0.7812, BLEU Score: 8.13\n"
     ]
    }
   ],
   "source": [
    "results_epochs = []\n",
    "for num_epochs in epoch_numbers:\n",
    "    avg_loss, bleu = train_with_epochs(num_epochs)\n",
    "    results_epochs.append({'num_epochs': num_epochs, 'loss': avg_loss, 'bleu_score': bleu})\n",
    "\n",
    "print(\"\\nEpoch Number Experiment Results:\")\n",
    "for res in results_epochs:\n",
    "    print(f\"Number of Epochs: {res['num_epochs']}, Final Loss: {res['loss']:.4f}, BLEU Score: {res['bleu_score']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q31: How do you experiment with different recurrent layers to evaluate their impact on translation quality?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_type='GRU'):\n",
    "    print(f\"\\nTraining with {model_type} model\")\n",
    "    if model_type == 'GRU':\n",
    "        # Define GRU-based model:\n",
    "        encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_N_LAYERS).to(device)\n",
    "        decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_N_LAYERS).to(device)\n",
    "        seq2seq_model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "    elif model_type == 'LSTM':\n",
    "        # Define LSTM-based model:\n",
    "        class EncoderLSTM(nn.Module):\n",
    "            def __init__(self, input_dim, emb_dim, hid_dim, n_layers):\n",
    "                super(EncoderLSTM, self).__init__()\n",
    "                self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "                self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, batch_first=True)\n",
    "                \n",
    "            def forward(self, src):\n",
    "                embedded = self.embedding(src)\n",
    "                outputs, (hidden, cell) = self.rnn(embedded)\n",
    "                return hidden, cell\n",
    "        \n",
    "        class DecoderLSTM(nn.Module):\n",
    "            def __init__(self, output_dim, emb_dim, hid_dim, n_layers):\n",
    "                super(DecoderLSTM, self).__init__()\n",
    "                self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "                self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, batch_first=True)\n",
    "                self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "                \n",
    "            def forward(self, input, hidden, cell):\n",
    "                input = input.unsqueeze(1)\n",
    "                embedded = self.embedding(input)\n",
    "                output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "                prediction = self.fc_out(output.squeeze(1))\n",
    "                return prediction, hidden, cell\n",
    "        \n",
    "        class Seq2SeqLSTM(nn.Module):\n",
    "            def __init__(self, encoder, decoder, device):\n",
    "                super(Seq2SeqLSTM, self).__init__()\n",
    "                self.encoder = encoder\n",
    "                self.decoder = decoder\n",
    "                self.device = device\n",
    "                \n",
    "            def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "                batch_size = src.shape[0]\n",
    "                trg_len = trg.shape[1]\n",
    "                trg_vocab_size = self.decoder.embedding.num_embeddings\n",
    "                \n",
    "                outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "                \n",
    "                hidden, cell = self.encoder(src)\n",
    "                \n",
    "                input = trg[:, 0]\n",
    "                \n",
    "                for t in range(1, trg_len):\n",
    "                    output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "                    outputs[:, t, :] = output\n",
    "                    teacher_force = random.random() < teacher_forcing_ratio\n",
    "                    top1 = output.argmax(1)\n",
    "                    input = trg[:, t] if teacher_force else top1\n",
    "                return outputs\n",
    "        \n",
    "        encoder = EncoderLSTM(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_N_LAYERS).to(device)\n",
    "        decoder = DecoderLSTM(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_N_LAYERS).to(device)\n",
    "        seq2seq_model = Seq2SeqLSTM(encoder, decoder, device).to(device)\n",
    "    else:\n",
    "        raise ValueError(\"model_type must be 'GRU' or 'LSTM'\")\n",
    "    \n",
    "    optimizer = optim.Adam(seq2seq_model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        seq2seq_model.train()\n",
    "        epoch_loss = 0\n",
    "        for src_batch, trg_batch in train_loader:\n",
    "            src_batch = src_batch.to(device)\n",
    "            trg_batch = trg_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = seq2seq_model(src_batch, trg_batch)\n",
    "            \n",
    "            # output: (batch_size, trg_len, trg_vocab_size)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "            trg = trg_batch[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    bleu_score = evaluate(seq2seq_model, valid_loader)\n",
    "    print(f'Validation BLEU score: {bleu_score*100:.2f}')\n",
    "    return avg_loss, bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with GRU model\n",
      "Epoch 1/10, Loss: 4.6459\n",
      "Epoch 2/10, Loss: 3.5893\n",
      "Epoch 3/10, Loss: 3.0096\n",
      "Epoch 4/10, Loss: 2.5689\n",
      "Epoch 5/10, Loss: 2.2054\n",
      "Epoch 6/10, Loss: 1.8771\n",
      "Epoch 7/10, Loss: 1.6052\n",
      "Epoch 8/10, Loss: 1.4044\n",
      "Epoch 9/10, Loss: 1.2310\n",
      "Epoch 10/10, Loss: 1.0996\n",
      "Validation BLEU score: 9.33\n",
      "\n",
      "Training with LSTM model\n",
      "Epoch 1/10, Loss: 4.9880\n",
      "Epoch 2/10, Loss: 4.1204\n",
      "Epoch 3/10, Loss: 3.7079\n",
      "Epoch 4/10, Loss: 3.3919\n",
      "Epoch 5/10, Loss: 3.1239\n",
      "Epoch 6/10, Loss: 2.8745\n",
      "Epoch 7/10, Loss: 2.6470\n",
      "Epoch 8/10, Loss: 2.4340\n",
      "Epoch 9/10, Loss: 2.2234\n",
      "Epoch 10/10, Loss: 2.0312\n",
      "Validation BLEU score: 8.62\n",
      "\n",
      "Recurrent Layer Experiment Results:\n",
      "GRU Model - Final Loss: 1.0996, BLEU Score: 9.33\n",
      "LSTM Model - Final Loss: 2.0312, BLEU Score: 8.62\n"
     ]
    }
   ],
   "source": [
    "gru_loss, gru_bleu = train_model(model_type='GRU')  # Train and evaluate GRU-based model\n",
    "\n",
    "lstm_loss, lstm_bleu = train_model(model_type='LSTM')  # Train and evaluate LSTM-based model\n",
    "\n",
    "print(\"\\nRecurrent Layer Experiment Results:\")\n",
    "print(f\"GRU Model - Final Loss: {gru_loss:.4f}, BLEU Score: {gru_bleu*100:.2f}\")\n",
    "print(f\"LSTM Model - Final Loss: {lstm_loss:.4f}, BLEU Score: {lstm_bleu*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'data' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "if os.path.exists('data'):\n",
    "    shutil.rmtree('data')\n",
    "    print(\"Folder 'data' has been deleted.\")\n",
    "else:\n",
    "    print(\"Folder 'data' does not exist.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
