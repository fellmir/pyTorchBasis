{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence (seq2seq) models and machine translation\n",
    "\n",
    "The `17_seq2seq_translation` notebook focuses on sequence-to-sequence (seq2seq) models for machine translation, a key application of neural networks in natural language processing. It covers preparing a dataset for translation tasks, building both the Encoder and Decoder models, and combining them into a complete seq2seq architecture. \n",
    "\n",
    "The notebook further explores training the model, evaluating its performance, translating new sentences, and experimenting with hyperparameters to fine-tune the model’s accuracy and fluency in translation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding seq2seq models and machine translation](#understanding-seq2seq-models-and-machine-translation)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Preparing the dataset for machine translation](#preparing-the-dataset-for-machine-translation)\n",
    "4. [Building the Encoder model](#building-the-encoder-model)\n",
    "5. [Building the Decoder model](#building-the-decoder-model)\n",
    "6. [Combining Encoder and Decoder into a seq2seq model](#combining-encoder-and-decoder-into-a-seq2seq-model)\n",
    "7. [Training the seq2seq model](#training-the-seq2seq-model)\n",
    "8. [Evaluating the seq2seq model](#evaluating-the-seq2seq-model)\n",
    "9. [Translating new sentences](#translating-new-sentences)\n",
    "10. [Experimenting with hyperparameters](#experimenting-with-hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding seq2seq models and machine translation\n",
    "\n",
    "Sequence-to-sequence (seq2seq) models are a class of neural networks designed to transform one sequence into another, making them particularly effective for tasks where input and output are sequences of varying lengths. These models are widely used in tasks like **machine translation**, where the input is a sentence in one language and the output is the translation in another language. Other applications include text summarization, speech recognition, and image captioning.\n",
    "\n",
    "The seq2seq model is based on a **recurrent neural network (RNN)** architecture and is typically composed of two main parts: an **encoder** and a **decoder**. The encoder processes the input sequence and compresses it into a fixed-size context vector, and the decoder takes this context vector to generate the output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How seq2seq models work**\n",
    "\n",
    "Seq2seq models are designed to handle input and output sequences of different lengths, which makes them ideal for translation tasks where sentences in different languages vary in length and structure. The core idea is to read the input sequence, encode it into a compact representation, and then use this representation to generate the output sequence.\n",
    "\n",
    "#### **Encoder**\n",
    "\n",
    "The encoder processes the input sequence, which can be a sequence of words (such as a sentence) or other time-ordered data. Each input token (word) is passed one by one through an RNN, which updates its hidden state at each step. The hidden state at the final time step is a summary of the entire input sequence and serves as the **context vector** that the decoder will use to generate the output sequence.\n",
    "\n",
    "In traditional seq2seq models, this context vector is the only information passed to the decoder, making it a crucial component of the model. It needs to encode all relevant information from the input sequence.\n",
    "\n",
    "#### **Decoder**\n",
    "\n",
    "The decoder is another RNN that takes the context vector generated by the encoder and produces the output sequence. At each time step, the decoder generates one token of the output sequence, conditioned on the context vector and the tokens generated so far.\n",
    "\n",
    "The decoder also maintains its own hidden state, which evolves as it generates the output sequence token by token. It typically uses a **teacher forcing** strategy during training, where the ground truth output token from the previous step is provided as input for the next step rather than the token predicted by the model.\n",
    "\n",
    "In tasks like machine translation, the decoder will generate words in the target language until it outputs a special **end-of-sequence** token, signaling the end of the translation.\n",
    "\n",
    "### **Training seq2seq models**\n",
    "\n",
    "Seq2seq models are trained by minimizing the difference between the predicted output sequence and the actual target sequence. This is typically done using a loss function like **cross-entropy**, which compares the predicted probabilities for each output token to the actual token in the target sequence.\n",
    "\n",
    "During training, the model learns to map the input sequence to the output sequence, improving its ability to capture long-range dependencies and handle variable-length inputs and outputs. However, the reliance on the context vector alone (as in traditional seq2seq models) can lead to information loss, especially in long sequences, which is why extensions like **attention mechanisms** have been developed to address this issue.\n",
    "\n",
    "### **Limitations of vanilla seq2seq models**\n",
    "\n",
    "While seq2seq models have been highly successful, they come with limitations, particularly when handling long input sequences. In a vanilla seq2seq model, the entire input sequence is compressed into a single context vector. This vector must carry all the necessary information for the decoder to generate the entire output sequence. For short sentences, this works relatively well, but for longer or more complex sequences, important details can be lost.\n",
    "\n",
    "Some key limitations include:\n",
    "- **Information bottleneck**: The encoder must compress all the information from the input sequence into a single fixed-length vector, which can lead to an information bottleneck, especially for long sequences.\n",
    "- **Difficulty with long-term dependencies**: Seq2seq models, especially when based on traditional RNNs or GRUs, struggle to capture long-term dependencies in the data. While LSTMs help mitigate this issue, they still face challenges when handling very long sequences.\n",
    "\n",
    "### **Machine translation with seq2seq models**\n",
    "\n",
    "Seq2seq models are particularly well-suited for machine translation tasks. In this setting, the input sequence is a sentence in the source language, and the output sequence is its translation in the target language. The seq2seq model learns to map the structure and meaning of the source sentence into a context vector, which the decoder then uses to generate the translated sentence.\n",
    "\n",
    "The process of machine translation with seq2seq models typically follows these steps:\n",
    "1. **Input processing**: The encoder reads the input sentence (in the source language) one word at a time, updating its hidden state at each time step.\n",
    "2. **Context vector creation**: Once the entire sentence has been processed, the encoder produces a final hidden state, known as the context vector, which summarizes the input sentence.\n",
    "3. **Decoding**: The decoder takes the context vector and starts generating the translated sentence word by word, based on the context and the previous words generated in the target language.\n",
    "4. **End of sequence**: The decoding process continues until an end-of-sequence token is generated, signaling that the translation is complete.\n",
    "\n",
    "Seq2seq models for machine translation can be trained on large parallel corpora, where sentences in the source language are paired with their translations in the target language. The model learns to align and map the structure of sentences across different languages.\n",
    "\n",
    "### **Variants and improvements to seq2seq models**\n",
    "\n",
    "To address the limitations of vanilla seq2seq models, several improvements have been introduced over the years. One of the most important innovations is the **attention mechanism**, which allows the decoder to focus on different parts of the input sequence at each decoding step, rather than relying solely on the context vector.\n",
    "\n",
    "#### **Attention mechanisms**\n",
    "\n",
    "Attention mechanisms allow the model to selectively focus on different parts of the input sequence while generating the output. Instead of compressing the entire input into a single fixed-length vector, the attention mechanism provides the decoder with a dynamic weighted combination of the encoder's hidden states, allowing the model to \"attend\" to specific words in the input sequence during each step of the decoding process.\n",
    "\n",
    "By doing so, attention helps the model overcome the information bottleneck issue and improves its ability to handle long input sequences. This is particularly useful in machine translation, where the correspondence between words in the source and target languages can vary significantly.\n",
    "\n",
    "#### **Bidirectional RNNs**\n",
    "\n",
    "Another enhancement to seq2seq models is the use of **bidirectional RNNs** in the encoder. A bidirectional RNN processes the input sequence in both forward and backward directions, allowing the model to capture context from both the past and the future at each time step. This helps improve the quality of the context vector by giving the encoder access to information about the entire sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Maths**\n",
    "\n",
    "#### **Encoder**\n",
    "\n",
    "In a seq2seq model, the encoder processes the input sequence one element (token) at a time, updating its hidden state at each step. Let the input sequence be $ X = (x_1, x_2, \\dots, x_T) $, where $ T $ is the length of the input sequence. The encoder uses a recurrent neural network (RNN), such as a vanilla RNN, a GRU (Gated Recurrent Unit), or an LSTM (Long Short-Term Memory), to produce a sequence of hidden states $ h_t $ at each time step $ t $.\n",
    "\n",
    "For an RNN, the hidden state update can be represented as:\n",
    "\n",
    "$$\n",
    "h_t = f(W_{hx} x_t + W_{hh} h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ x_t $ is the input at time step $ t $,\n",
    "- $ h_{t-1} $ is the hidden state from the previous time step,\n",
    "- $ W_{hx} $ and $ W_{hh} $ are the weight matrices for the input and the previous hidden state, respectively,\n",
    "- $ b_h $ is the bias term,\n",
    "- $ f $ is a non-linear activation function (e.g., tanh or ReLU).\n",
    "\n",
    "The final hidden state of the encoder $ h_T $ is used as the context vector, which summarizes the entire input sequence:\n",
    "\n",
    "$$\n",
    "c = h_T\n",
    "$$\n",
    "\n",
    "This context vector $ c $ is then passed to the decoder.\n",
    "\n",
    "#### **Decoder**\n",
    "\n",
    "The decoder generates the output sequence $ Y = (y_1, y_2, \\dots, y_{T'}) $, where $ T' $ is the length of the output sequence. The decoder is also an RNN, and it generates the output one token at a time, conditioned on the context vector $ c $ from the encoder and its own previous hidden state.\n",
    "\n",
    "At each time step $ t $ in the decoder, the hidden state is updated as follows:\n",
    "\n",
    "$$\n",
    "s_t = f(W_{sy} y_{t-1} + W_{sc} c + W_{ss} s_{t-1} + b_s)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ y_{t-1} $ is the previous output token (used as input during training in the teacher forcing setup),\n",
    "- $ c $ is the context vector from the encoder,\n",
    "- $ s_{t-1} $ is the previous hidden state of the decoder,\n",
    "- $ W_{sy}, W_{sc}, W_{ss} $ are the weight matrices for the previous output token, the context vector, and the previous hidden state, respectively,\n",
    "- $ b_s $ is the bias term.\n",
    "\n",
    "At each step, the decoder produces an output $ \\hat{y_t} $, which is the probability distribution over the possible output tokens. This is usually done by applying a softmax function to the decoder’s output at each time step:\n",
    "\n",
    "$$\n",
    "\\hat{y_t} = \\text{softmax}(W_o s_t + b_o)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ W_o $ is the output weight matrix,\n",
    "- $ b_o $ is the output bias.\n",
    "\n",
    "The softmax function normalizes the output into a probability distribution over the vocabulary, allowing the model to predict the next token in the sequence.\n",
    "\n",
    "#### **Sequence generation**\n",
    "\n",
    "During training, the seq2seq model uses **teacher forcing**, where the true output token from the previous time step is provided as input to the decoder for the next time step. During inference (or testing), the model uses its own predictions as input for the next time step, generating the output sequence token by token.\n",
    "\n",
    "The output sequence is generated until the model produces an **end-of-sequence** (EOS) token, signaling that the sequence is complete.\n",
    "\n",
    "#### **Loss function**\n",
    "\n",
    "The seq2seq model is trained to minimize the difference between the predicted sequence $ \\hat{Y} $ and the true sequence $ Y $. A common loss function for this is the **cross-entropy loss**, which measures the difference between the predicted probability distribution $ \\hat{y_t} $ and the true one-hot encoded output $ y_t $ at each time step:\n",
    "\n",
    "$$\n",
    "L = - \\sum_{t=1}^{T'} \\sum_{k=1}^{V} y_{t,k} \\log(\\hat{y_{t,k}})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ T' $ is the length of the output sequence,\n",
    "- $ V $ is the size of the output vocabulary,\n",
    "- $ y_{t,k} $ is the true one-hot encoded value for the $ k $-th word in the vocabulary at time step $ t $,\n",
    "- $ \\hat{y_{t,k}} $ is the predicted probability for the $ k $-th word at time step $ t $.\n",
    "\n",
    "The goal is to minimize this loss over the entire training dataset, adjusting the model's parameters using gradient descent or another optimization algorithm.\n",
    "\n",
    "#### **Gradient flow and backpropagation through time (BPTT)**\n",
    "\n",
    "Training seq2seq models involves **backpropagation through time (BPTT)**, which is a form of backpropagation applied to sequences. In BPTT, the gradients of the loss with respect to the model’s parameters are computed over the entire sequence, and the parameters are updated accordingly.\n",
    "\n",
    "For each time step $ t $, the gradients are calculated for both the encoder and decoder. The weights in both networks are updated based on the error signals from the output sequence, which are propagated backward through the decoder and then through the encoder.\n",
    "\n",
    "Since seq2seq models involve both an encoder and a decoder, BPTT is applied to the entire architecture, ensuring that the gradients flow from the output sequence back through the decoder and encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for building and training seq2seq models in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!conda install -y pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules for model building, training, and data loading in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you set up the environment to use a GPU for training seq2seq models, and how do you fallback to CPU in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you set random seeds in PyTorch to ensure reproducibility when training seq2seq models?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset for machine translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you load a machine translation dataset (e.g., English to German) to use in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/fellmir/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "url_train_en = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.en.gz'\n",
    "url_train_de = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/train.de.gz'\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "def download_and_extract(url, filename):\n",
    "    filepath = os.path.join('data', filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        urllib.request.urlretrieve(url, filepath + '.gz')\n",
    "        with gzip.open(filepath + '.gz', 'rb') as f_in:\n",
    "            with open(filepath, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        os.remove(filepath + '.gz')\n",
    "\n",
    "download_and_extract(url_train_en, 'train.en')\n",
    "download_and_extract(url_train_de, 'train.de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you preprocess the dataset by tokenizing the sentences and converting them into sequences of indices?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the tokenizer functions:\n",
    "tokenizer_src = simple_tokenizer  # For English\n",
    "tokenizer_trg = simple_tokenizer  # For German\n",
    "\n",
    "with open('data/train.en', 'r', encoding='utf-8') as f:\n",
    "    sentences_en = f.readlines()\n",
    "\n",
    "with open('data/train.de', 'r', encoding='utf-8') as f:\n",
    "    sentences_de = f.readlines()\n",
    "\n",
    "assert len(sentences_en) == len(sentences_de)  # Ensure both files have the same number of sentences\n",
    "\n",
    "tokenized_en = [tokenizer_src(sentence) for sentence in sentences_en]\n",
    "tokenized_de = [tokenizer_trg(sentence) for sentence in sentences_de]  # Tokenize the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative using NLTK's wordpunct_tokenize:\n",
    "# from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "# tokenizer_src = wordpunct_tokenize  # For English\n",
    "# tokenizer_trg = wordpunct_tokenize  # For German\n",
    "\n",
    "# Tokenize the sentences\n",
    "# tokenized_en = [tokenizer_src(sentence.lower()) for sentence in sentences_en]\n",
    "# tokenized_de = [tokenizer_trg(sentence.lower()) for sentence in sentences_de]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you build vocabulary for both the source and target languages?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(tokenized_sentences, min_freq):\n",
    "    counter = Counter()\n",
    "    for tokens in tokenized_sentences:\n",
    "        counter.update(tokens)\n",
    "    vocab = {'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3}\n",
    "    idx = 4\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    return vocab\n",
    "\n",
    "MIN_FREQ = 2\n",
    "vocab_src = build_vocab(tokenized_en, MIN_FREQ)\n",
    "vocab_trg = build_vocab(tokenized_de, MIN_FREQ)\n",
    "\n",
    "inv_vocab_src = {idx: word for word, idx in vocab_src.items()}\n",
    "inv_vocab_trg = {idx: word for word, idx in vocab_trg.items()}  # Inverse vocabularies for decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you create DataLoaders for batching the source-target sentence pairs during training?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, tokenized_src, tokenized_trg, vocab_src, vocab_trg):\n",
    "        self.data = []\n",
    "        for src_tokens, trg_tokens in zip(tokenized_src, tokenized_trg):\n",
    "            src_indices = [vocab_src.get('<bos>')] + [vocab_src.get(token, vocab_src['<unk>']) for token in src_tokens] + [vocab_src.get('<eos>')]\n",
    "            trg_indices = [vocab_trg.get('<bos>')] + [vocab_trg.get(token, vocab_trg['<unk>']) for token in trg_tokens] + [vocab_trg.get('<eos>')]\n",
    "            self.data.append((torch.tensor(src_indices), torch.tensor(trg_indices)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_padded = nn.utils.rnn.pad_sequence(src_batch, padding_value=vocab_src['<pad>'], batch_first=True)\n",
    "    trg_padded = nn.utils.rnn.pad_sequence(trg_batch, padding_value=vocab_trg['<pad>'], batch_first=True)\n",
    "    return src_padded, trg_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_src, valid_src, train_trg, valid_trg = train_test_split(tokenized_en, tokenized_de, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = TranslationDataset(train_src, train_trg, vocab_src, vocab_trg)\n",
    "valid_dataset = TranslationDataset(valid_src, valid_trg, vocab_src, vocab_trg)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Encoder model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you define the architecture of the Encoder model using PyTorch’s `nn.Module`?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you implement the forward pass of the Encoder to process input sequences and generate the context vector?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The forward pass of the Encoder processes input sequences and generates the context vector (hidden state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you specify the number of layers and hidden units in the Encoder?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_src)\n",
    "ENC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "ENC_N_LAYERS = 2\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_N_LAYERS).to(device)  # Initialize the Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Decoder model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you define the Decoder architecture using PyTorch’s `nn.Module`?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you implement the forward pass of the Decoder to generate translated sequences from the context vector?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The forward pass of the Decoder generates translated sequences from the context vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you use the `nn.Linear` and `nn.Softmax` layers to convert the Decoder's output into predicted tokens?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The nn.Linear and nn.Softmax layers convert the Decoder's output into predicted tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Encoder and Decoder into a seq2seq model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you combine the Encoder and Decoder models into a complete seq2seq model for machine translation?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.embedding.num_embeddings\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        hidden = self.encoder(src)\n",
    "        \n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = len(vocab_trg)\n",
    "DEC_EMB_DIM = 256\n",
    "DEC_N_LAYERS = 2\n",
    "\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_N_LAYERS).to(device)\n",
    "seq2seq_model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you implement teacher forcing in the training loop to improve the Decoder’s performance during training?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher forcing is implemented in the Seq2Seq model's forward method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you implement the forward pass for the combined seq2seq model, using the context vector from the Encoder to initialize the Decoder?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The forward pass for the combined seq2seq model uses the context vector from the Encoder to initialize the Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the seq2seq model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you define the loss function (e.g., CrossEntropyLoss) for training the seq2seq model on sequence data?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = vocab_trg['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)  # Define the loss function with padding index ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you configure an optimizer (e.g., Adam) to update the parameters of both the Encoder and Decoder models during training?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(seq2seq_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you implement the training loop for the seq2seq model, including the forward pass, loss calculation, and backpropagation?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Loss: 0.7457\n",
      "Epoch 20/30, Loss: 0.6495\n",
      "Epoch 30/30, Loss: 0.6860\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    seq2seq_model.train()\n",
    "    epoch_loss = 0\n",
    "    for src_batch, trg_batch in train_loader:\n",
    "        src_batch = src_batch.to(device)\n",
    "        trg_batch = trg_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = seq2seq_model(src_batch, trg_batch)\n",
    "        \n",
    "        # output: (batch_size, trg_len, trg_vocab_size)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "        trg = trg_batch[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you monitor and log the training loss over epochs to ensure the seq2seq model is learning effectively?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss is monitored and logged in the training loop above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the seq2seq model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you evaluate the seq2seq model on a validation dataset using metrics such as the BLEU score?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src_batch, trg_batch in data_loader:\n",
    "            src_batch = src_batch.to(device)\n",
    "            trg_batch = trg_batch.to(device)\n",
    "            output = model(src_batch, trg_batch, teacher_forcing_ratio=0)\n",
    "            output_tokens = output.argmax(2)\n",
    "            for i in range(src_batch.size(0)):\n",
    "                trg_indices = trg_batch[i].cpu().numpy()\n",
    "                output_indices = output_tokens[i].cpu().numpy()\n",
    "                trg_tokens = [inv_vocab_trg[idx] for idx in trg_indices if idx != PAD_IDX and idx != vocab_trg['<bos>']]\n",
    "                output_tokens_list = [inv_vocab_trg[idx] for idx in output_indices if idx != PAD_IDX and idx != vocab_trg['<bos>']]\n",
    "                references.append([trg_tokens])\n",
    "                hypotheses.append(output_tokens_list)\n",
    "    bleu = corpus_bleu(references, hypotheses)\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation BLEU score: 7.99\n"
     ]
    }
   ],
   "source": [
    "bleu_score = evaluate(seq2seq_model, valid_loader)\n",
    "print(f'Validation BLEU score: {bleu_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you implement a function to calculate the BLEU score to assess the quality of the machine-translated sequences?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"evaluate\" function calculates the BLEU score to assess translation quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you compare the model's predictions to the target translations during evaluation to measure performance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model's predictions are compared to the target translations during evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating new sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you implement a function to translate new sentences using the trained seq2seq model?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, vocab_src, vocab_trg, model, tokenizer_src, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = ['<bos>'] + tokenizer_src(sentence.lower()) + ['<eos>']\n",
    "    src_indices = [vocab_src.get(token, vocab_src['<unk>']) for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden = model.encoder(src_tensor)\n",
    "    \n",
    "    input_token = torch.LongTensor([vocab_trg['<bos>']]).to(device)\n",
    "    outputs = []\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        output, hidden = model.decoder(input_token, hidden)\n",
    "        top1 = output.argmax(1)\n",
    "        outputs.append(top1.item())\n",
    "        if top1.item() == vocab_trg['<eos>']:\n",
    "            break\n",
    "        input_token = top1\n",
    "    translated_tokens = [inv_vocab_trg.get(idx, '<unk>') for idx in outputs]\n",
    "    return translated_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you handle sentences of varying lengths when translating new sentences with the seq2seq model?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The translate_sentence function handles sentences of varying lengths using a max_len parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q27: How do you visualize the original, translated, and reference (ground truth) sentences to evaluate the model’s translation performance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_translation(sentence):\n",
    "    print(f'Original: {sentence}')\n",
    "    translation = translate_sentence(sentence, vocab_src, vocab_trg, seq2seq_model, tokenizer_src)\n",
    "    print(f'Translated: {\" \".join(translation)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: A man is playing a guitar.\n",
      "Translated: ein mann spielt gitarre einer trommel <eos>\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"A man is playing a guitar.\"\n",
    "display_translation(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: the book is on the table\n",
      "Translated: die am tisch beim essen <eos>\n"
     ]
    }
   ],
   "source": [
    "display_translation('the book is on the table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: words\n",
      "Translated: straßenkünstler <unk> die arbeit <eos>\n"
     ]
    }
   ],
   "source": [
    "display_translation('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q28: How do you adjust the learning rate and observe its effect on the seq2seq model’s training stability and performance?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.0005, 0.0001]\n",
    "\n",
    "def train_with_learning_rate(lr):\n",
    "    print(f\"\\nTraining with learning rate: {lr}\")\n",
    "    # Re-initialize the model:\n",
    "    encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_N_LAYERS).to(device)\n",
    "    decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_N_LAYERS).to(device)\n",
    "    seq2seq_model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(seq2seq_model.parameters(), lr=lr)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        seq2seq_model.train()\n",
    "        epoch_loss = 0\n",
    "        for src_batch, trg_batch in train_loader:\n",
    "            src_batch = src_batch.to(device)\n",
    "            trg_batch = trg_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = seq2seq_model(src_batch, trg_batch)\n",
    "            \n",
    "            # output: (batch_size, trg_len, trg_vocab_size)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "            trg = trg_batch[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    bleu_score = evaluate(seq2seq_model, valid_loader)\n",
    "    print(f'Validation BLEU score: {bleu_score*100:.2f}')\n",
    "    return avg_loss, bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with learning rate: 0.001\n",
      "Epoch 1/10, Loss: 4.6847\n",
      "Epoch 2/10, Loss: 3.6639\n",
      "Epoch 3/10, Loss: 3.1361\n",
      "Epoch 4/10, Loss: 2.7249\n",
      "Epoch 5/10, Loss: 2.3774\n",
      "Epoch 6/10, Loss: 2.0795\n",
      "Epoch 7/10, Loss: 1.8155\n",
      "Epoch 8/10, Loss: 1.6143\n",
      "Epoch 9/10, Loss: 1.4452\n",
      "Epoch 10/10, Loss: 1.2949\n",
      "Validation BLEU score: 9.00\n",
      "\n",
      "Training with learning rate: 0.0005\n",
      "Epoch 1/10, Loss: 4.8586\n",
      "Epoch 2/10, Loss: 3.7968\n",
      "Epoch 3/10, Loss: 3.2709\n",
      "Epoch 4/10, Loss: 2.8791\n",
      "Epoch 5/10, Loss: 2.5498\n",
      "Epoch 6/10, Loss: 2.2679\n",
      "Epoch 7/10, Loss: 1.9957\n",
      "Epoch 8/10, Loss: 1.7537\n",
      "Epoch 9/10, Loss: 1.5253\n",
      "Epoch 10/10, Loss: 1.3123\n",
      "Validation BLEU score: 10.03\n",
      "\n",
      "Training with learning rate: 0.0001\n",
      "Epoch 1/10, Loss: 5.4526\n",
      "Epoch 2/10, Loss: 4.8022\n",
      "Epoch 3/10, Loss: 4.4589\n",
      "Epoch 4/10, Loss: 4.1844\n",
      "Epoch 5/10, Loss: 3.9739\n",
      "Epoch 6/10, Loss: 3.7985\n",
      "Epoch 7/10, Loss: 3.6333\n",
      "Epoch 8/10, Loss: 3.4942\n",
      "Epoch 9/10, Loss: 3.3708\n",
      "Epoch 10/10, Loss: 3.2538\n",
      "Validation BLEU score: 6.85\n",
      "\n",
      "Learning Rate Experiment Results:\n",
      "Learning Rate: 0.001, Final Loss: 1.2949, BLEU Score: 9.00\n",
      "Learning Rate: 0.0005, Final Loss: 1.3123, BLEU Score: 10.03\n",
      "Learning Rate: 0.0001, Final Loss: 3.2538, BLEU Score: 6.85\n"
     ]
    }
   ],
   "source": [
    "results_lr = []\n",
    "for lr in learning_rates:\n",
    "    avg_loss, bleu = train_with_learning_rate(lr)\n",
    "    results_lr.append({'learning_rate': lr, 'loss': avg_loss, 'bleu_score': bleu})\n",
    "\n",
    "print(\"\\nLearning Rate Experiment Results:\")\n",
    "for res in results_lr:\n",
    "    print(f\"Learning Rate: {res['learning_rate']}, Final Loss: {res['loss']:.4f}, BLEU Score: {res['bleu_score']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q29: How do you experiment with different batch sizes to observe how they impact training speed and memory usage?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [16, 32, 64]\n",
    "\n",
    "def train_with_batch_size(batch_size):\n",
    "    print(f\"\\nTraining with batch size: {batch_size}\")\n",
    "    # Re-create DataLoaders with specified batch size:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_N_LAYERS).to(device)\n",
    "    decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_N_LAYERS).to(device)\n",
    "    seq2seq_model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(seq2seq_model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        seq2seq_model.train()\n",
    "        epoch_loss = 0\n",
    "        for src_batch, trg_batch in train_loader:\n",
    "            src_batch = src_batch.to(device)\n",
    "            trg_batch = trg_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = seq2seq_model(src_batch, trg_batch)\n",
    "            \n",
    "            # output: (batch_size, trg_len, trg_vocab_size)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "            trg = trg_batch[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    bleu_score = evaluate(seq2seq_model, valid_loader)\n",
    "    print(f'Validation BLEU score: {bleu_score*100:.2f}')\n",
    "    print(f\"Training time: {total_time:.2f} seconds\")\n",
    "    return avg_loss, bleu_score, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with batch size: 16\n",
      "Epoch 1/10, Loss: 4.4521\n",
      "Epoch 2/10, Loss: 3.3737\n",
      "Epoch 3/10, Loss: 2.8145\n",
      "Epoch 4/10, Loss: 2.3840\n",
      "Epoch 5/10, Loss: 2.0419\n",
      "Epoch 6/10, Loss: 1.8051\n",
      "Epoch 7/10, Loss: 1.6383\n",
      "Epoch 8/10, Loss: 1.5304\n",
      "Epoch 9/10, Loss: 1.4372\n",
      "Epoch 10/10, Loss: 1.3601\n",
      "Validation BLEU score: 9.88\n",
      "Training time: 829.96 seconds\n",
      "\n",
      "Training with batch size: 32\n",
      "Epoch 1/10, Loss: 4.6968\n",
      "Epoch 2/10, Loss: 3.6638\n",
      "Epoch 3/10, Loss: 3.1340\n",
      "Epoch 4/10, Loss: 2.7402\n",
      "Epoch 5/10, Loss: 2.3981\n",
      "Epoch 6/10, Loss: 2.0969\n",
      "Epoch 7/10, Loss: 1.8206\n",
      "Epoch 8/10, Loss: 1.6418\n",
      "Epoch 9/10, Loss: 1.4646\n",
      "Epoch 10/10, Loss: 1.3230\n",
      "Validation BLEU score: 8.94\n",
      "Training time: 537.01 seconds\n",
      "\n",
      "Training with batch size: 64\n",
      "Epoch 1/10, Loss: 5.1268\n",
      "Epoch 2/10, Loss: 4.1523\n",
      "Epoch 3/10, Loss: 3.6510\n",
      "Epoch 4/10, Loss: 3.2843\n",
      "Epoch 5/10, Loss: 2.9597\n",
      "Epoch 6/10, Loss: 2.7052\n",
      "Epoch 7/10, Loss: 2.4615\n",
      "Epoch 8/10, Loss: 2.2732\n",
      "Epoch 9/10, Loss: 2.0692\n",
      "Epoch 10/10, Loss: 1.8595\n",
      "Validation BLEU score: 7.67\n",
      "Training time: 400.06 seconds\n",
      "\n",
      "Batch Size Experiment Results:\n",
      "Batch Size: 16, Final Loss: 1.3601, BLEU Score: 9.88, Training Time: 829.96 seconds\n",
      "Batch Size: 32, Final Loss: 1.3230, BLEU Score: 8.94, Training Time: 537.01 seconds\n",
      "Batch Size: 64, Final Loss: 1.8595, BLEU Score: 7.67, Training Time: 400.06 seconds\n"
     ]
    }
   ],
   "source": [
    "results_bs = []\n",
    "for batch_size in batch_sizes:\n",
    "    avg_loss, bleu, total_time = train_with_batch_size(batch_size)\n",
    "    results_bs.append({'batch_size': batch_size, 'loss': avg_loss, 'bleu_score': bleu, 'training_time': total_time})\n",
    "\n",
    "print(\"\\nBatch Size Experiment Results:\")\n",
    "for res in results_bs:\n",
    "    print(f\"Batch Size: {res['batch_size']}, Final Loss: {res['loss']:.4f}, BLEU Score: {res['bleu_score']*100:.2f}, Training Time: {res['training_time']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q30: How do you modify the number of training epochs and analyze how it affects the model’s convergence and translation accuracy?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_numbers = [5, 15, 20]\n",
    "\n",
    "def train_with_epochs(num_epochs):\n",
    "    print(f\"\\nTraining with number of epochs: {num_epochs}\")\n",
    "    encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_N_LAYERS).to(device)\n",
    "    decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_N_LAYERS).to(device)\n",
    "    seq2seq_model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(seq2seq_model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        seq2seq_model.train()\n",
    "        epoch_loss = 0\n",
    "        for src_batch, trg_batch in train_loader:\n",
    "            src_batch = src_batch.to(device)\n",
    "            trg_batch = trg_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = seq2seq_model(src_batch, trg_batch)\n",
    "            \n",
    "            # output: (batch_size, trg_len, trg_vocab_size)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "            trg = trg_batch[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    bleu_score = evaluate(seq2seq_model, valid_loader)\n",
    "    print(f'Validation BLEU score after {num_epochs} epochs: {bleu_score*100:.2f}')\n",
    "    return avg_loss, bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with number of epochs: 5\n",
      "Epoch 1/5, Loss: 4.6984\n",
      "Epoch 2/5, Loss: 3.6317\n",
      "Epoch 3/5, Loss: 3.0833\n",
      "Epoch 4/5, Loss: 2.6554\n",
      "Epoch 5/5, Loss: 2.2912\n",
      "Validation BLEU score after 5 epochs: 9.15\n",
      "\n",
      "Training with number of epochs: 15\n",
      "Epoch 1/15, Loss: 4.7550\n",
      "Epoch 2/15, Loss: 3.7759\n",
      "Epoch 3/15, Loss: 3.2685\n",
      "Epoch 4/15, Loss: 2.8572\n",
      "Epoch 5/15, Loss: 2.5082\n",
      "Epoch 6/15, Loss: 2.1977\n",
      "Epoch 7/15, Loss: 1.9555\n",
      "Epoch 8/15, Loss: 1.7371\n",
      "Epoch 9/15, Loss: 1.5629\n",
      "Epoch 10/15, Loss: 1.4165\n",
      "Epoch 11/15, Loss: 1.2908\n",
      "Epoch 12/15, Loss: 1.1703\n",
      "Epoch 13/15, Loss: 1.0776\n",
      "Epoch 14/15, Loss: 1.0027\n",
      "Epoch 15/15, Loss: 0.9374\n",
      "Validation BLEU score after 15 epochs: 8.78\n",
      "\n",
      "Training with number of epochs: 20\n",
      "Epoch 1/20, Loss: 4.7713\n",
      "Epoch 2/20, Loss: 3.7686\n",
      "Epoch 3/20, Loss: 3.2649\n",
      "Epoch 4/20, Loss: 2.8748\n",
      "Epoch 5/20, Loss: 2.5597\n",
      "Epoch 6/20, Loss: 2.2812\n",
      "Epoch 7/20, Loss: 2.0288\n",
      "Epoch 8/20, Loss: 1.8197\n",
      "Epoch 9/20, Loss: 1.6428\n",
      "Epoch 10/20, Loss: 1.5024\n",
      "Epoch 11/20, Loss: 1.3940\n",
      "Epoch 12/20, Loss: 1.2498\n",
      "Epoch 13/20, Loss: 1.1637\n",
      "Epoch 14/20, Loss: 1.0731\n",
      "Epoch 15/20, Loss: 1.0077\n",
      "Epoch 16/20, Loss: 0.9365\n",
      "Epoch 17/20, Loss: 0.9030\n",
      "Epoch 18/20, Loss: 0.8329\n",
      "Epoch 19/20, Loss: 0.8156\n",
      "Epoch 20/20, Loss: 0.7812\n",
      "Validation BLEU score after 20 epochs: 8.13\n",
      "\n",
      "Epoch Number Experiment Results:\n",
      "Number of Epochs: 5, Final Loss: 2.2912, BLEU Score: 9.15\n",
      "Number of Epochs: 15, Final Loss: 0.9374, BLEU Score: 8.78\n",
      "Number of Epochs: 20, Final Loss: 0.7812, BLEU Score: 8.13\n"
     ]
    }
   ],
   "source": [
    "results_epochs = []\n",
    "for num_epochs in epoch_numbers:\n",
    "    avg_loss, bleu = train_with_epochs(num_epochs)\n",
    "    results_epochs.append({'num_epochs': num_epochs, 'loss': avg_loss, 'bleu_score': bleu})\n",
    "\n",
    "print(\"\\nEpoch Number Experiment Results:\")\n",
    "for res in results_epochs:\n",
    "    print(f\"Number of Epochs: {res['num_epochs']}, Final Loss: {res['loss']:.4f}, BLEU Score: {res['bleu_score']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q31: How do you experiment with different recurrent layers (e.g., LSTM vs. GRU) to evaluate their impact on translation quality?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_type='GRU'):\n",
    "    print(f\"\\nTraining with {model_type} model\")\n",
    "    if model_type == 'GRU':\n",
    "        # Define GRU-based model:\n",
    "        encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_N_LAYERS).to(device)\n",
    "        decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_N_LAYERS).to(device)\n",
    "        seq2seq_model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "    elif model_type == 'LSTM':\n",
    "        # Define LSTM-based model:\n",
    "        class EncoderLSTM(nn.Module):\n",
    "            def __init__(self, input_dim, emb_dim, hid_dim, n_layers):\n",
    "                super(EncoderLSTM, self).__init__()\n",
    "                self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "                self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, batch_first=True)\n",
    "                \n",
    "            def forward(self, src):\n",
    "                embedded = self.embedding(src)\n",
    "                outputs, (hidden, cell) = self.rnn(embedded)\n",
    "                return hidden, cell\n",
    "        \n",
    "        class DecoderLSTM(nn.Module):\n",
    "            def __init__(self, output_dim, emb_dim, hid_dim, n_layers):\n",
    "                super(DecoderLSTM, self).__init__()\n",
    "                self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "                self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, batch_first=True)\n",
    "                self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "                \n",
    "            def forward(self, input, hidden, cell):\n",
    "                input = input.unsqueeze(1)\n",
    "                embedded = self.embedding(input)\n",
    "                output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "                prediction = self.fc_out(output.squeeze(1))\n",
    "                return prediction, hidden, cell\n",
    "        \n",
    "        class Seq2SeqLSTM(nn.Module):\n",
    "            def __init__(self, encoder, decoder, device):\n",
    "                super(Seq2SeqLSTM, self).__init__()\n",
    "                self.encoder = encoder\n",
    "                self.decoder = decoder\n",
    "                self.device = device\n",
    "                \n",
    "            def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "                batch_size = src.shape[0]\n",
    "                trg_len = trg.shape[1]\n",
    "                trg_vocab_size = self.decoder.embedding.num_embeddings\n",
    "                \n",
    "                outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "                \n",
    "                hidden, cell = self.encoder(src)\n",
    "                \n",
    "                input = trg[:, 0]\n",
    "                \n",
    "                for t in range(1, trg_len):\n",
    "                    output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "                    outputs[:, t, :] = output\n",
    "                    teacher_force = random.random() < teacher_forcing_ratio\n",
    "                    top1 = output.argmax(1)\n",
    "                    input = trg[:, t] if teacher_force else top1\n",
    "                return outputs\n",
    "        \n",
    "        encoder = EncoderLSTM(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_N_LAYERS).to(device)\n",
    "        decoder = DecoderLSTM(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_N_LAYERS).to(device)\n",
    "        seq2seq_model = Seq2SeqLSTM(encoder, decoder, device).to(device)\n",
    "    else:\n",
    "        raise ValueError(\"model_type must be 'GRU' or 'LSTM'\")\n",
    "    \n",
    "    optimizer = optim.Adam(seq2seq_model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        seq2seq_model.train()\n",
    "        epoch_loss = 0\n",
    "        for src_batch, trg_batch in train_loader:\n",
    "            src_batch = src_batch.to(device)\n",
    "            trg_batch = trg_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = seq2seq_model(src_batch, trg_batch)\n",
    "            \n",
    "            # output: (batch_size, trg_len, trg_vocab_size)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "            trg = trg_batch[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    bleu_score = evaluate(seq2seq_model, valid_loader)\n",
    "    print(f'Validation BLEU score: {bleu_score*100:.2f}')\n",
    "    return avg_loss, bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with GRU model\n",
      "Epoch 1/10, Loss: 4.6459\n",
      "Epoch 2/10, Loss: 3.5893\n",
      "Epoch 3/10, Loss: 3.0096\n",
      "Epoch 4/10, Loss: 2.5689\n",
      "Epoch 5/10, Loss: 2.2054\n",
      "Epoch 6/10, Loss: 1.8771\n",
      "Epoch 7/10, Loss: 1.6052\n",
      "Epoch 8/10, Loss: 1.4044\n",
      "Epoch 9/10, Loss: 1.2310\n",
      "Epoch 10/10, Loss: 1.0996\n",
      "Validation BLEU score: 9.33\n",
      "\n",
      "Training with LSTM model\n",
      "Epoch 1/10, Loss: 4.9880\n",
      "Epoch 2/10, Loss: 4.1204\n",
      "Epoch 3/10, Loss: 3.7079\n",
      "Epoch 4/10, Loss: 3.3919\n",
      "Epoch 5/10, Loss: 3.1239\n",
      "Epoch 6/10, Loss: 2.8745\n",
      "Epoch 7/10, Loss: 2.6470\n",
      "Epoch 8/10, Loss: 2.4340\n",
      "Epoch 9/10, Loss: 2.2234\n",
      "Epoch 10/10, Loss: 2.0312\n",
      "Validation BLEU score: 8.62\n",
      "\n",
      "Recurrent Layer Experiment Results:\n",
      "GRU Model - Final Loss: 1.0996, BLEU Score: 9.33\n",
      "LSTM Model - Final Loss: 2.0312, BLEU Score: 8.62\n"
     ]
    }
   ],
   "source": [
    "gru_loss, gru_bleu = train_model(model_type='GRU')  # Train and evaluate GRU-based model\n",
    "\n",
    "lstm_loss, lstm_bleu = train_model(model_type='LSTM')  # Train and evaluate LSTM-based model\n",
    "\n",
    "print(\"\\nRecurrent Layer Experiment Results:\")\n",
    "print(f\"GRU Model - Final Loss: {gru_loss:.4f}, BLEU Score: {gru_bleu*100:.2f}\")\n",
    "print(f\"LSTM Model - Final Loss: {lstm_loss:.4f}, BLEU Score: {lstm_bleu*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'data' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "if os.path.exists('data'):\n",
    "    shutil.rmtree('data')\n",
    "    print(\"Folder 'data' has been deleted.\")\n",
    "else:\n",
    "    print(\"Folder 'data' does not exist.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
