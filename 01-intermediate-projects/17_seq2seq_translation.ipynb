{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence (seq2seq) models and machine translation\n",
    "\n",
    "The `17_seq2seq_translation` notebook focuses on sequence-to-sequence (seq2seq) models for machine translation, a key application of neural networks in natural language processing. It covers preparing a dataset for translation tasks, building both the Encoder and Decoder models, and combining them into a complete seq2seq architecture. \n",
    "\n",
    "The notebook further explores training the model, evaluating its performance, translating new sentences, and experimenting with hyperparameters to fine-tune the modelâ€™s accuracy and fluency in translation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding seq2seq models and machine translation](#understanding-seq2seq-models-and-machine-translation)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Preparing the dataset for machine translation](#preparing-the-dataset-for-machine-translation)\n",
    "4. [Building the Encoder model](#building-the-encoder-model)\n",
    "5. [Building the Decoder model](#building-the-decoder-model)\n",
    "6. [Combining Encoder and Decoder into a seq2seq model](#combining-encoder-and-decoder-into-a-seq2seq-model)\n",
    "7. [Training the seq2seq model](#training-the-seq2seq-model)\n",
    "8. [Evaluating the seq2seq model](#evaluating-the-seq2seq-model)\n",
    "9. [Translating new sentences](#translating-new-sentences)\n",
    "10. [Experimenting with hyperparameters](#experimenting-with-hyperparameters)\n",
    "11. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding seq2seq models and machine translation\n",
    "\n",
    "Sequence-to-sequence (seq2seq) models are a class of neural networks designed to transform one sequence into another, making them particularly effective for tasks where input and output are sequences of varying lengths. These models are widely used in tasks like **machine translation**, where the input is a sentence in one language and the output is the translation in another language. Other applications include text summarization, speech recognition, and image captioning.\n",
    "\n",
    "The seq2seq model is based on a **recurrent neural network (RNN)** architecture and is typically composed of two main parts: an **encoder** and a **decoder**. The encoder processes the input sequence and compresses it into a fixed-size context vector, and the decoder takes this context vector to generate the output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How seq2seq models work**\n",
    "\n",
    "Seq2seq models are designed to handle input and output sequences of different lengths, which makes them ideal for translation tasks where sentences in different languages vary in length and structure. The core idea is to read the input sequence, encode it into a compact representation, and then use this representation to generate the output sequence.\n",
    "\n",
    "#### **Encoder**\n",
    "\n",
    "The encoder processes the input sequence, which can be a sequence of words (such as a sentence) or other time-ordered data. Each input token (word) is passed one by one through an RNN, which updates its hidden state at each step. The hidden state at the final time step is a summary of the entire input sequence and serves as the **context vector** that the decoder will use to generate the output sequence.\n",
    "\n",
    "In traditional seq2seq models, this context vector is the only information passed to the decoder, making it a crucial component of the model. It needs to encode all relevant information from the input sequence.\n",
    "\n",
    "#### **Decoder**\n",
    "\n",
    "The decoder is another RNN that takes the context vector generated by the encoder and produces the output sequence. At each time step, the decoder generates one token of the output sequence, conditioned on the context vector and the tokens generated so far.\n",
    "\n",
    "The decoder also maintains its own hidden state, which evolves as it generates the output sequence token by token. It typically uses a **teacher forcing** strategy during training, where the ground truth output token from the previous step is provided as input for the next step rather than the token predicted by the model.\n",
    "\n",
    "In tasks like machine translation, the decoder will generate words in the target language until it outputs a special **end-of-sequence** token, signaling the end of the translation.\n",
    "\n",
    "### **Training seq2seq models**\n",
    "\n",
    "Seq2seq models are trained by minimizing the difference between the predicted output sequence and the actual target sequence. This is typically done using a loss function like **cross-entropy**, which compares the predicted probabilities for each output token to the actual token in the target sequence.\n",
    "\n",
    "During training, the model learns to map the input sequence to the output sequence, improving its ability to capture long-range dependencies and handle variable-length inputs and outputs. However, the reliance on the context vector alone (as in traditional seq2seq models) can lead to information loss, especially in long sequences, which is why extensions like **attention mechanisms** have been developed to address this issue.\n",
    "\n",
    "### **Limitations of vanilla seq2seq models**\n",
    "\n",
    "While seq2seq models have been highly successful, they come with limitations, particularly when handling long input sequences. In a vanilla seq2seq model, the entire input sequence is compressed into a single context vector. This vector must carry all the necessary information for the decoder to generate the entire output sequence. For short sentences, this works relatively well, but for longer or more complex sequences, important details can be lost.\n",
    "\n",
    "Some key limitations include:\n",
    "- **Information bottleneck**: The encoder must compress all the information from the input sequence into a single fixed-length vector, which can lead to an information bottleneck, especially for long sequences.\n",
    "- **Difficulty with long-term dependencies**: Seq2seq models, especially when based on traditional RNNs or GRUs, struggle to capture long-term dependencies in the data. While LSTMs help mitigate this issue, they still face challenges when handling very long sequences.\n",
    "\n",
    "### **Machine translation with seq2seq models**\n",
    "\n",
    "Seq2seq models are particularly well-suited for machine translation tasks. In this setting, the input sequence is a sentence in the source language, and the output sequence is its translation in the target language. The seq2seq model learns to map the structure and meaning of the source sentence into a context vector, which the decoder then uses to generate the translated sentence.\n",
    "\n",
    "The process of machine translation with seq2seq models typically follows these steps:\n",
    "1. **Input processing**: The encoder reads the input sentence (in the source language) one word at a time, updating its hidden state at each time step.\n",
    "2. **Context vector creation**: Once the entire sentence has been processed, the encoder produces a final hidden state, known as the context vector, which summarizes the input sentence.\n",
    "3. **Decoding**: The decoder takes the context vector and starts generating the translated sentence word by word, based on the context and the previous words generated in the target language.\n",
    "4. **End of sequence**: The decoding process continues until an end-of-sequence token is generated, signaling that the translation is complete.\n",
    "\n",
    "Seq2seq models for machine translation can be trained on large parallel corpora, where sentences in the source language are paired with their translations in the target language. The model learns to align and map the structure of sentences across different languages.\n",
    "\n",
    "### **Variants and improvements to seq2seq models**\n",
    "\n",
    "To address the limitations of vanilla seq2seq models, several improvements have been introduced over the years. One of the most important innovations is the **attention mechanism**, which allows the decoder to focus on different parts of the input sequence at each decoding step, rather than relying solely on the context vector.\n",
    "\n",
    "#### **Attention mechanisms**\n",
    "\n",
    "Attention mechanisms allow the model to selectively focus on different parts of the input sequence while generating the output. Instead of compressing the entire input into a single fixed-length vector, the attention mechanism provides the decoder with a dynamic weighted combination of the encoder's hidden states, allowing the model to \"attend\" to specific words in the input sequence during each step of the decoding process.\n",
    "\n",
    "By doing so, attention helps the model overcome the information bottleneck issue and improves its ability to handle long input sequences. This is particularly useful in machine translation, where the correspondence between words in the source and target languages can vary significantly.\n",
    "\n",
    "#### **Bidirectional RNNs**\n",
    "\n",
    "Another enhancement to seq2seq models is the use of **bidirectional RNNs** in the encoder. A bidirectional RNN processes the input sequence in both forward and backward directions, allowing the model to capture context from both the past and the future at each time step. This helps improve the quality of the context vector by giving the encoder access to information about the entire sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Maths**\n",
    "\n",
    "#### **Encoder**\n",
    "\n",
    "In a seq2seq model, the encoder processes the input sequence one element (token) at a time, updating its hidden state at each step. Let the input sequence be $ X = (x_1, x_2, \\dots, x_T) $, where $ T $ is the length of the input sequence. The encoder uses a recurrent neural network (RNN), such as a vanilla RNN, a GRU (Gated Recurrent Unit), or an LSTM (Long Short-Term Memory), to produce a sequence of hidden states $ h_t $ at each time step $ t $.\n",
    "\n",
    "For an RNN, the hidden state update can be represented as:\n",
    "\n",
    "$$\n",
    "h_t = f(W_{hx} x_t + W_{hh} h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ x_t $ is the input at time step $ t $,\n",
    "- $ h_{t-1} $ is the hidden state from the previous time step,\n",
    "- $ W_{hx} $ and $ W_{hh} $ are the weight matrices for the input and the previous hidden state, respectively,\n",
    "- $ b_h $ is the bias term,\n",
    "- $ f $ is a non-linear activation function (e.g., tanh or ReLU).\n",
    "\n",
    "The final hidden state of the encoder $ h_T $ is used as the context vector, which summarizes the entire input sequence:\n",
    "\n",
    "$$\n",
    "c = h_T\n",
    "$$\n",
    "\n",
    "This context vector $ c $ is then passed to the decoder.\n",
    "\n",
    "#### **Decoder**\n",
    "\n",
    "The decoder generates the output sequence $ Y = (y_1, y_2, \\dots, y_{T'}) $, where $ T' $ is the length of the output sequence. The decoder is also an RNN, and it generates the output one token at a time, conditioned on the context vector $ c $ from the encoder and its own previous hidden state.\n",
    "\n",
    "At each time step $ t $ in the decoder, the hidden state is updated as follows:\n",
    "\n",
    "$$\n",
    "s_t = f(W_{sy} y_{t-1} + W_{sc} c + W_{ss} s_{t-1} + b_s)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ y_{t-1} $ is the previous output token (used as input during training in the teacher forcing setup),\n",
    "- $ c $ is the context vector from the encoder,\n",
    "- $ s_{t-1} $ is the previous hidden state of the decoder,\n",
    "- $ W_{sy}, W_{sc}, W_{ss} $ are the weight matrices for the previous output token, the context vector, and the previous hidden state, respectively,\n",
    "- $ b_s $ is the bias term.\n",
    "\n",
    "At each step, the decoder produces an output $ \\hat{y_t} $, which is the probability distribution over the possible output tokens. This is usually done by applying a softmax function to the decoderâ€™s output at each time step:\n",
    "\n",
    "$$\n",
    "\\hat{y_t} = \\text{softmax}(W_o s_t + b_o)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W_o $ is the output weight matrix,\n",
    "- $ b_o $ is the output bias.\n",
    "\n",
    "The softmax function normalizes the output into a probability distribution over the vocabulary, allowing the model to predict the next token in the sequence.\n",
    "\n",
    "#### **Sequence generation**\n",
    "\n",
    "During training, the seq2seq model uses **teacher forcing**, where the true output token from the previous time step is provided as input to the decoder for the next time step. During inference (or testing), the model uses its own predictions as input for the next time step, generating the output sequence token by token.\n",
    "\n",
    "The output sequence is generated until the model produces an **end-of-sequence** (EOS) token, signaling that the sequence is complete.\n",
    "\n",
    "#### **Loss function**\n",
    "\n",
    "The seq2seq model is trained to minimize the difference between the predicted sequence $ \\hat{Y} $ and the true sequence $ Y $. A common loss function for this is the **cross-entropy loss**, which measures the difference between the predicted probability distribution $ \\hat{y_t} $ and the true one-hot encoded output $ y_t $ at each time step:\n",
    "\n",
    "$$\n",
    "L = - \\sum_{t=1}^{T'} \\sum_{k=1}^{V} y_{t,k} \\log(\\hat{y_{t,k}})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ T' $ is the length of the output sequence,\n",
    "- $ V $ is the size of the output vocabulary,\n",
    "- $ y_{t,k} $ is the true one-hot encoded value for the $ k $-th word in the vocabulary at time step $ t $,\n",
    "- $ \\hat{y_{t,k}} $ is the predicted probability for the $ k $-th word at time step $ t $.\n",
    "\n",
    "The goal is to minimize this loss over the entire training dataset, adjusting the model's parameters using gradient descent or another optimization algorithm.\n",
    "\n",
    "#### **Gradient flow and backpropagation through time (BPTT)**\n",
    "\n",
    "Training seq2seq models involves **backpropagation through time (BPTT)**, which is a form of backpropagation applied to sequences. In BPTT, the gradients of the loss with respect to the modelâ€™s parameters are computed over the entire sequence, and the parameters are updated accordingly.\n",
    "\n",
    "For each time step $ t $, the gradients are calculated for both the encoder and decoder. The weights in both networks are updated based on the error signals from the output sequence, which are propagated backward through the decoder and then through the encoder.\n",
    "\n",
    "Since seq2seq models involve both an encoder and a decoder, BPTT is applied to the entire architecture, ensuring that the gradients flow from the output sequence back through the decoder and encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for building and training seq2seq models in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules for model building, training, and data loading in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you set up the environment to use a GPU for training seq2seq models, and how do you fallback to CPU in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you set random seeds in PyTorch to ensure reproducibility when training seq2seq models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset for machine translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you load a machine translation dataset (e.g., English to German) using `torchtext.datasets` in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you preprocess the dataset by tokenizing the sentences and converting them into sequences of indices?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you build vocabulary for both the source and target languages using PyTorch's `Field` or `Vocab`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you create DataLoaders for batching the source-target sentence pairs during training?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Encoder model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you define the architecture of the Encoder model using PyTorchâ€™s `nn.Module`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you implement the forward pass of the Encoder to process input sequences and generate the context vector?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you specify the number of layers and hidden units in the Encoder, and how do they impact the modelâ€™s performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Decoder model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you define the Decoder architecture using PyTorchâ€™s `nn.Module`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you implement the forward pass of the Decoder to generate translated sequences from the context vector?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you use the `nn.Linear` and `nn.Softmax` layers to convert the Decoder's output into predicted tokens?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Encoder and Decoder into a seq2seq model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you combine the Encoder and Decoder models into a complete seq2seq model for machine translation?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you implement teacher forcing in the training loop to improve the Decoderâ€™s performance during training?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you implement the forward pass for the combined seq2seq model, using the context vector from the Encoder to initialize the Decoder?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the seq2seq model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you define the loss function (e.g., CrossEntropyLoss) for training the seq2seq model on sequence data?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you configure an optimizer (e.g., Adam) to update the parameters of both the Encoder and Decoder models during training?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you implement the training loop for the seq2seq model, including the forward pass, loss calculation, and backpropagation?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you monitor and log the training loss over epochs to ensure the seq2seq model is learning effectively?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the seq2seq model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you evaluate the seq2seq model on a validation dataset using metrics such as the BLEU score?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you implement a function to calculate the BLEU score to assess the quality of the machine-translated sequences?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you compare the model's predictions to the target translations during evaluation to measure performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating new sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you implement a function to translate new sentences using the trained seq2seq model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you handle sentences of varying lengths when translating new sentences with the seq2seq model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q27: How do you visualize the original, translated, and reference (ground truth) sentences to evaluate the modelâ€™s translation performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q28: How do you adjust the learning rate and observe its effect on the seq2seq modelâ€™s training stability and performance?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q29: How do you experiment with different batch sizes to observe how they impact training speed and memory usage?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q30: How do you modify the number of training epochs and analyze how it affects the modelâ€™s convergence and translation accuracy?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q31: How do you experiment with different recurrent layers (e.g., LSTM vs. GRU) to evaluate their impact on translation quality?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
