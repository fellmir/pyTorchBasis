{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning in PyTorch\n",
    "\n",
    "The `11_transfer_learning` notebook focuses on applying transfer learning techniques, which involve leveraging pretrained models to accelerate training on new tasks. This approach is particularly useful when working with smaller datasets or specialized tasks. \n",
    "\n",
    "In this notebook, key topics include preparing the dataset, loading a pretrained model, freezing and modifying layers to adapt it for the new task, and training the modified model. It also covers evaluating model performance and experimenting with hyperparameters to optimize results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding transfer learning](#understanding-transfer-learning)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Preparing the dataset](#preparing-the-dataset)\n",
    "4. [Loading a pretrained model](#loading-a-pretrained-model)\n",
    "5. [Freezing and modifying layers](#freezing-and-modifying-layers)\n",
    "6. [Training the model](#training-the-model)\n",
    "7. [Evaluating model performance](#evaluating-model-performance)\n",
    "8. [Experimenting with hyperparameters](#experimenting-with-hyperparameters)\n",
    "9. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding transfer learning\n",
    "\n",
    "Transfer learning is a machine learning technique where a model developed for a specific task is reused as the starting point for a model on a second, related task. Instead of training a model from scratch on a new task, transfer learning leverages the patterns and features that a model has already learned from a previous task. This is particularly effective when the new task has limited data, or when training from scratch would be too computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Why use transfer learning?**\n",
    "\n",
    "There are several reasons why transfer learning is widely used in deep learning, especially in tasks involving large models like CNNs:\n",
    "\n",
    "- **Efficiency**: Training a large neural network from scratch can be computationally expensive and time-consuming. Transfer learning allows you to leverage the knowledge of a pretrained model, significantly reducing the time required for training.\n",
    "- **Limited data**: In many real-world applications, collecting large amounts of labeled data is difficult. Transfer learning helps overcome this limitation by using a model that has already been trained on a large dataset, such as ImageNet, and fine-tuning it on a smaller, domain-specific dataset.\n",
    "- **Generalization**: Models trained on large datasets like ImageNet learn powerful, general features that can be transferred to other tasks. For instance, in a pretrained CNN, the earlier layers tend to capture low-level features like edges and textures, while the later layers capture more task-specific features. By reusing these general features, the model can generalize well to new tasks.\n",
    "\n",
    "### **Key concepts in transfer learning**\n",
    "\n",
    "#### **Pretrained models**\n",
    "\n",
    "Pretrained models are the foundation of transfer learning. A model is first trained on a large dataset for a specific task (like image classification on ImageNet). Once trained, the model's weights and architecture can be transferred and adapted to a new task. Commonly used pretrained models include **ResNet**, **VGG**, **AlexNet**, **Inception**, and **DenseNet**.\n",
    "\n",
    "Pretrained models have learned a variety of patterns, which are stored in the form of weights. These learned patterns, especially in the early layers of convolutional neural networks (CNNs), often represent general features that are useful across a wide range of tasks.\n",
    "\n",
    "#### **Two main strategies in transfer learning**\n",
    "\n",
    "1. **Feature extraction**:\n",
    "   - In this approach, the pretrained model is used as a feature extractor. The convolutional base (i.e., the layers that extract features) of the pretrained model is kept frozen, and only the final classification layer is replaced and trained on the new task.\n",
    "   - This is especially useful when the new dataset is small, as the pretrained model has already learned powerful, general features. By only training the final layer, you reduce the risk of overfitting.\n",
    "\n",
    "2. **Fine-tuning**:\n",
    "   - In fine-tuning, you not only replace the final classification layer but also unfreeze part (or all) of the convolutional base and retrain it on the new task. This allows the model to adapt its features to the specific characteristics of the new dataset.\n",
    "   - Fine-tuning is more computationally expensive than feature extraction, and there is a higher risk of overfitting, especially if the new dataset is small. However, it often leads to better performance when the new task is significantly different from the original task.\n",
    "\n",
    "### **How transfer learning works in CNNs**\n",
    "\n",
    "Transfer learning in convolutional neural networks (CNNs) is especially effective because of the hierarchical nature of the features learned by CNNs:\n",
    "\n",
    "- **Early layers**: The initial layers of a CNN learn low-level features like edges, gradients, and textures. These features are often useful across many different image-related tasks.\n",
    "- **Mid-level layers**: Middle layers capture more complex patterns, such as shapes and objects.\n",
    "- **Late layers**: The final layers of the CNN tend to be more task-specific, focusing on high-level features related to the specific dataset the model was trained on (e.g., specific object categories).\n",
    "\n",
    "In transfer learning, the goal is to reuse the early and mid-level features of a pretrained model, which are generalizable, while adapting or retraining the later, task-specific layers to suit the new task.\n",
    "\n",
    "#### **Feature extraction process**\n",
    "\n",
    "In the feature extraction approach, the pretrained model’s convolutional layers act as a fixed feature extractor. The process works as follows:\n",
    "1. **Freeze the convolutional layers**: During training, the weights of the convolutional layers remain fixed, and only the classifier (fully connected layers) is trained.\n",
    "2. **Replace the final layer**: The final classification layer is replaced with a new one that matches the number of classes in the new dataset.\n",
    "3. **Train the new classifier**: Only the new classification layer is trained, typically with a much lower learning rate, to fine-tune the final model.\n",
    "\n",
    "This approach works well when the new dataset is small, as the convolutional layers have already been trained to capture general patterns.\n",
    "\n",
    "#### **Fine-tuning process**\n",
    "\n",
    "In fine-tuning, the goal is to allow part of the pretrained model to be retrained to better adapt to the new task:\n",
    "1. **Unfreeze some layers**: A portion of the pretrained layers, often the later layers, is unfrozen so that their weights can be updated during training.\n",
    "2. **Replace and train the classifier**: The final layer is replaced with a task-specific classifier, and both the classifier and the unfrozen layers are trained.\n",
    "3. **Lower learning rate**: Fine-tuning is typically done with a much lower learning rate to avoid making drastic changes to the pretrained weights.\n",
    "\n",
    "Fine-tuning can be particularly beneficial when the new task is very different from the original task (e.g., transferring from natural images to medical images), as it allows the model to adapt to the unique characteristics of the new dataset.\n",
    "\n",
    "### **Transfer learning strategies**\n",
    "\n",
    "The specific transfer learning strategy to use depends on several factors, such as the size of the new dataset and the similarity between the original and new tasks:\n",
    "\n",
    "- **Small dataset, similar task**: Feature extraction is typically sufficient. The general features learned by the pretrained model should be applicable to the new task, and only the classifier needs to be trained.\n",
    "- **Large dataset, similar task**: Fine-tuning can be beneficial. With enough data, the pretrained model’s weights can be adjusted to better suit the new task, improving performance.\n",
    "- **Small dataset, dissimilar task**: Feature extraction is usually the best option. Since the dataset is small, fine-tuning the model could lead to overfitting, so freezing most of the layers and training a new classifier reduces this risk.\n",
    "- **Large dataset, dissimilar task**: Fine-tuning is preferred. Since the task is different, the model needs to learn task-specific features from the new data, and a large dataset provides enough information for fine-tuning without overfitting.\n",
    "\n",
    "### **Advantages of transfer learning**\n",
    "\n",
    "Transfer learning offers several key benefits:\n",
    "- **Faster training**: Since the model has already been trained on a large dataset, the training process for the new task is significantly faster.\n",
    "- **Improved performance with limited data**: Transfer learning can lead to better performance on tasks where only a small dataset is available, as the pretrained model brings valuable knowledge from a large dataset.\n",
    "- **Effective use of computational resources**: Transfer learning allows you to leverage pretrained models that required extensive computational resources, avoiding the need to retrain large models from scratch.\n",
    "\n",
    "### **Limitations of transfer learning**\n",
    "\n",
    "Despite its advantages, transfer learning has some limitations:\n",
    "- **Task dissimilarity**: If the original task (used for pretraining) is very different from the new task, the features learned by the pretrained model may not be as useful. In such cases, the model may need to be fine-tuned extensively or even trained from scratch.\n",
    "- **Overfitting**: Fine-tuning a pretrained model on a small dataset can lead to overfitting, as the model may become overly specific to the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Maths**\n",
    "\n",
    "Transfer learning reuses a model that has already been trained on a source task, adjusting it for a new target task. The underlying mathematics is similar to that of training neural networks but introduces specific considerations for how weights, gradients, and backpropagation are managed in the context of using pretrained models.\n",
    "\n",
    "#### **Weight initialization in transfer learning**\n",
    "\n",
    "When training a model from scratch, weights are initialized randomly (e.g., using Xavier or He initialization). However, in transfer learning, the pretrained model’s weights, denoted $ W_{\\text{pretrained}} $, are used as the starting point. This initialization allows the model to start from a position of learned knowledge rather than a random state. The weights in a fine-tuning setup are represented as:\n",
    "\n",
    "$$\n",
    "W_{\\text{fine-tuned}} = W_{\\text{pretrained}} + \\Delta W\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W_{\\text{pretrained}} $ are the weights learned from the original dataset,\n",
    "- $ \\Delta W $ are the adjustments to the weights during fine-tuning on the new dataset.\n",
    "\n",
    "Since the weights are initialized with values that have already captured useful patterns, the training process starts from a more optimized position, allowing for faster convergence compared to random initialization.\n",
    "\n",
    "#### **Frozen and unfrozen layers**\n",
    "\n",
    "In transfer learning, some layers of the pretrained model are frozen, meaning their weights are not updated during backpropagation. The frozen layers are typically earlier layers that capture low-level features such as edges or textures, which are generalizable across tasks. In contrast, later layers are often unfrozen, allowing them to adapt to the specific patterns of the new task.\n",
    "\n",
    "Mathematically, the gradients for frozen layers are set to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{\\text{frozen}}} = 0\n",
    "$$\n",
    "\n",
    "Where $ L $ is the loss function and $ W_{\\text{frozen}} $ are the weights of the frozen layers. This ensures that these weights remain unchanged during the optimization process. For unfrozen layers, the standard backpropagation applies:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{\\text{unfrozen}}} = \\frac{\\partial L}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial W_{\\text{unfrozen}}}\n",
    "$$\n",
    "\n",
    "Where $ Z $ represents the output from the layer.\n",
    "\n",
    "#### **Backpropagation and gradient flow**\n",
    "\n",
    "When fine-tuning, gradients are only computed for the unfrozen layers. These gradients propagate through the network based on the chain rule. The overall goal is to minimize the loss function $ L $, which could be cross-entropy for classification tasks or mean squared error for regression tasks.\n",
    "\n",
    "For each weight $ W $ in the unfrozen layers, the update is computed using the gradient descent rule:\n",
    "\n",
    "$$\n",
    "W_{\\text{new}} = W_{\\text{old}} - \\eta \\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\eta $ is the learning rate, which is typically lower during fine-tuning to prevent large updates to the pretrained weights.\n",
    "- $ \\frac{\\partial L}{\\partial W} $ is the gradient of the loss with respect to the weight $ W $.\n",
    "\n",
    "This lower learning rate ensures that the adjustments to the pretrained weights are subtle, retaining the knowledge from the original task while adapting to the new one.\n",
    "\n",
    "#### **Feature extraction vs fine-tuning mathematically**\n",
    "\n",
    "In **feature extraction**, the pretrained model’s convolutional layers are used purely as feature extractors, and the weights in these layers remain fixed. Only the final classification layer is trained. The feature extraction process can be described mathematically as:\n",
    "\n",
    "$$\n",
    "Z = f(X; W_{\\text{pretrained}})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ X $ is the input data,\n",
    "- $ f $ is the function representing the forward pass through the frozen layers,\n",
    "- $ W_{\\text{pretrained}} $ are the frozen weights.\n",
    "\n",
    "The extracted features $ Z $ are then passed through a new classification layer $ g $, whose weights $ W_{\\text{new}} $ are updated during training:\n",
    "\n",
    "$$\n",
    "y_{\\text{pred}} = g(Z; W_{\\text{new}})\n",
    "$$\n",
    "\n",
    "In **fine-tuning**, the weights of some or all of the layers are updated. The forward pass through the network is similar, but both $ W_{\\text{pretrained}} $ and $ W_{\\text{new}} $ can be updated, depending on which layers are unfrozen.\n",
    "\n",
    "#### **Learning rate and catastrophic forgetting**\n",
    "\n",
    "One of the key challenges in fine-tuning is choosing an appropriate learning rate. A high learning rate can cause the model to forget the useful features learned during pretraining, a problem known as **catastrophic forgetting**. To avoid this, a lower learning rate is used when updating the pretrained weights:\n",
    "\n",
    "$$\n",
    "W_{\\text{new}} = W_{\\text{old}} - \\eta_{\\text{low}} \\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "\n",
    "Where $ \\eta_{\\text{low}} $ is a small learning rate. This ensures that the updates to the weights are gradual, allowing the model to fine-tune its features without losing the general knowledge learned during pretraining.\n",
    "\n",
    "#### **Regularization and overfitting**\n",
    "\n",
    "Fine-tuning on a small dataset can lead to overfitting, where the model becomes too specialized to the new task. Regularization techniques, such as **dropout** and **L2 regularization**, can help mitigate this by preventing the model from relying too heavily on any single feature or weight.\n",
    "\n",
    "- **Dropout** randomly sets a fraction of the neurons to zero during each forward pass, preventing the model from becoming too reliant on specific neurons.\n",
    "- **L2 regularization** adds a penalty to the loss function based on the magnitude of the weights:\n",
    "\n",
    "$$\n",
    "L_{\\text{regularized}} = L + \\lambda \\sum W^2\n",
    "$$\n",
    "\n",
    "Where $ \\lambda $ is the regularization parameter. This term discourages large weight values, which can lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for loading pretrained models and training in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules for model loading, training, and dataset handling in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you set up your environment to use a GPU if available, or fallback to CPU in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you set a random seed in PyTorch to ensure reproducibility of results?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you load a dataset using PyTorch’s `torchvision.datasets` module?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you apply image transformations such as resizing and normalization to match the input requirements of a pretrained model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you split a dataset into training, validation, and test sets using PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you create DataLoaders for efficient batch loading of the dataset in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you load a pretrained model, such as ResNet, from PyTorch’s `torchvision.models`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you inspect the architecture of a pretrained model to understand its layers and components?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you print the number of parameters in the pretrained model and identify which layers will be updated during training?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you modify the final fully connected layer of a pretrained model to match the number of output classes in your dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing and modifying layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you freeze all layers of the pretrained model to prevent them from being updated during training?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you selectively unfreeze certain layers of the pretrained model for fine-tuning?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you verify which layers of the model are frozen and which are trainable after modifying the layers?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you define the loss function for a classification task when using a pretrained model in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you configure an optimizer (e.g., Adam, SGD) to update only the unfrozen layers of the pretrained model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you implement a training loop to update the model’s weights using the training dataset in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you implement gradient clipping to prevent exploding gradients during training with a pretrained model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you monitor and plot the training loss and accuracy over epochs during the model training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you evaluate the trained model on the validation or test dataset using PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you calculate and print the classification accuracy of the pretrained model on the test set?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you visualize the confusion matrix to assess the model's classification performance on the test data?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you implement model evaluation in a batch-wise manner to handle large datasets more efficiently in PyTorch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you experiment with different learning rates for the frozen and unfrozen layers of the pretrained model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you modify the batch size in the DataLoader and analyze its impact on training performance?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q27: How do you adjust the number of training epochs and observe its effect on overfitting or underfitting the model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q28: How do you experiment with different optimizers (e.g., Adam vs. SGD) and their parameters to improve model performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
