{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence (seq2seq) models and machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Understanding seq2seq models and machine translation](#understanding-seq2seq-models-and-machine-translation)\n",
    "2. [Setting up the environment](#setting-up-the-environment)\n",
    "3. [Preparing the dataset for machine translation](#preparing-the-dataset-for-machine-translation)\n",
    "4. [Building the Encoder model](#building-the-encoder-model)\n",
    "5. [Building the Decoder model](#building-the-decoder-model)\n",
    "6. [Combining Encoder and Decoder into a seq2seq model](#combining-encoder-and-decoder-into-a-seq2seq-model)\n",
    "7. [Training the seq2seq model](#training-the-seq2seq-model)\n",
    "8. [Evaluating the seq2seq model](#evaluating-the-seq2seq-model)\n",
    "9. [Translating new sentences](#translating-new-sentences)\n",
    "10. [Experimenting with hyperparameters](#experimenting-with-hyperparameters)\n",
    "11. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding seq2seq models and machine translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q1: How do you install the necessary libraries for building and training seq2seq models in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q2: How do you import the required modules for model building, training, and data loading in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q3: How do you set up the environment to use a GPU for training seq2seq models, and how do you fallback to CPU in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q4: How do you set random seeds in PyTorch to ensure reproducibility when training seq2seq models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset for machine translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q5: How do you load a machine translation dataset (e.g., English to German) using `torchtext.datasets` in PyTorch?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q6: How do you preprocess the dataset by tokenizing the sentences and converting them into sequences of indices?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q7: How do you build vocabulary for both the source and target languages using PyTorch's `Field` or `Vocab`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q8: How do you create DataLoaders for batching the source-target sentence pairs during training?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Encoder model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q9: How do you define the architecture of the Encoder model using PyTorch’s `nn.Module`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q10: How do you implement the forward pass of the Encoder to process input sequences and generate the context vector?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q11: How do you specify the number of layers and hidden units in the Encoder, and how do they impact the model’s performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Decoder model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q12: How do you define the Decoder architecture using PyTorch’s `nn.Module`?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q13: How do you implement the forward pass of the Decoder to generate translated sequences from the context vector?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q14: How do you use the `nn.Linear` and `nn.Softmax` layers to convert the Decoder's output into predicted tokens?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Encoder and Decoder into a seq2seq model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q15: How do you combine the Encoder and Decoder models into a complete seq2seq model for machine translation?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q16: How do you implement teacher forcing in the training loop to improve the Decoder’s performance during training?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q17: How do you implement the forward pass for the combined seq2seq model, using the context vector from the Encoder to initialize the Decoder?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the seq2seq model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q18: How do you define the loss function (e.g., CrossEntropyLoss) for training the seq2seq model on sequence data?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q19: How do you configure an optimizer (e.g., Adam) to update the parameters of both the Encoder and Decoder models during training?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q20: How do you implement the training loop for the seq2seq model, including the forward pass, loss calculation, and backpropagation?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q21: How do you monitor and log the training loss over epochs to ensure the seq2seq model is learning effectively?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the seq2seq model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q22: How do you evaluate the seq2seq model on a validation dataset using metrics such as the BLEU score?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q23: How do you implement a function to calculate the BLEU score to assess the quality of the machine-translated sequences?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q24: How do you compare the model's predictions to the target translations during evaluation to measure performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating new sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q25: How do you implement a function to translate new sentences using the trained seq2seq model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q26: How do you handle sentences of varying lengths when translating new sentences with the seq2seq model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q27: How do you visualize the original, translated, and reference (ground truth) sentences to evaluate the model’s translation performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q28: How do you adjust the learning rate and observe its effect on the seq2seq model’s training stability and performance?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q29: How do you experiment with different batch sizes to observe how they impact training speed and memory usage?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q30: How do you modify the number of training epochs and analyze how it affects the model’s convergence and translation accuracy?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Q31: How do you experiment with different recurrent layers (e.g., LSTM vs. GRU) to evaluate their impact on translation quality?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
